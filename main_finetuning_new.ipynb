{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_user_reviews\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mretrieval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initialize_chromadb, collect_results_per_product\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize, compute_similarity, recall_at_k, ndcg_at_k\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecommenderModel  \u001b[38;5;66;03m# Import the RecommenderModel class\u001b[39;00m\n",
      "File \u001b[1;32md:\\Master_Thesis\\final_pipeline\\retrieval.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhyp1231/blair-roberta-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhyp1231/blair-roberta-large\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Load model in float16\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set model to evaluation mode\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_embedding\u001b[39m(text):\n",
      "File \u001b[1;32mc:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:543\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    540\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs_orig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    542\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map\n\u001b[1;32m--> 543\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_mapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    544\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m resolve_trust_remote_code(\n\u001b[0;32m    545\u001b[0m     trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n\u001b[0;32m    546\u001b[0m )\n\u001b[0;32m    548\u001b[0m \u001b[38;5;66;03m# Set the adapter kwargs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:780\u001b[0m, in \u001b[0;36m_LazyAutoMapping.keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkeys\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 780\u001b[0m     mapping_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    781\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_attr_from_module(key, name)\n\u001b[0;32m    782\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m    784\u001b[0m     ]\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping_keys \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32mc:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:781\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkeys\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    780\u001b[0m     mapping_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 781\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m    784\u001b[0m     ]\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping_keys \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32mc:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:777\u001b[0m, in \u001b[0;36m_LazyAutoMapping._load_attr_from_module\u001b[1;34m(self, model_type, attr)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m    776\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[module_name] \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 777\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:693\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[1;34m(module, attr)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attr, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(getattribute_from_module(module, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attr)\n\u001b[1;32m--> 693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr)\n\u001b[0;32m    695\u001b[0m \u001b[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;66;03m# object at the top level.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\utils\\import_utils.py:1766\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1764\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1766\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1767\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32mc:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\utils\\import_utils.py:1778\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1777\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1778\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1779\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1780\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1781\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1782\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1783\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main.ipynb\n",
    "\n",
    "# Import necessary packages\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append('./')  # Adjust the path if necessary\n",
    "from datetime import date\n",
    "import random\n",
    "current_date = date.today()\n",
    "# Import modules\n",
    "from data_loader import load_user_reviews\n",
    "from utils import *\n",
    "from retrieval import initialize_chromadb, collect_results_per_product\n",
    "from evaluation import normalize, compute_similarity, recall_at_k, ndcg_at_k\n",
    "from model_pipeline import RecommenderModel  # Import the RecommenderModel class\n",
    "from config import PIPELINE_PARAMS, USER_PROFILE_PROMPT, MODEL_PATH, TOKENIZER_PATH\n",
    "# Additional imports\n",
    "import torch\n",
    "\n",
    "# Function to run the experiment for a given sample size\n",
    "def run_experiment(sample_size = None, num_run = 0, adapter = False):\n",
    "    # Set the MODEL_PATH and TOKENIZER_PATH dynamically in config\n",
    "    # Assuming config.py defines TOKENIZER_PATH and MODEL_PATH as format strings\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    # Adjust paths for each sample size\n",
    "    \n",
    "    # Override config paths for this experiment\n",
    "\n",
    "    # Initialize the RecommenderModel with the adjusted paths\n",
    "    recommender_model = RecommenderModel(sample_size, adapter= adapter)    \n",
    "    # Initialize ChromaDB\n",
    "    db_path = \"./chroma_db_mpnet\"\n",
    "    db = initialize_chromadb(db_path)\n",
    "    collection_name = 'product_embeddings_filtered'\n",
    "    collection = db.get_collection(name=collection_name)\n",
    "    \n",
    "    # Load product data and user reviews\n",
    "    df = pd.read_csv(\"data/meta_all_beauty_filtered_simple.csv\")\n",
    "    train_file = 'new_data/new_train_val_output.json'\n",
    "    input_set = load_user_reviews(train_file)\n",
    "    test_file = 'new_data/test_output.json'\n",
    "    input_set_test = load_user_reviews(test_file)\n",
    "\n",
    "    # Experiment parameters\n",
    "    n_latest_reviews = 10\n",
    "    SIMILARITY_THRESHOLD = 90.0\n",
    "    K_VALUES = [1, 5, 10, 20]\n",
    "    MAX_RETRIES = 3\n",
    "    all_similarity_scores, all_matches, all_recalls, all_ndcgs, skipped_users = [], [], [], [], []\n",
    "\n",
    "    # Open a result file specific to this sample size\n",
    "    result_file_path = f'results_product_name_train_val_set_adapter_{adapter}_numOfRev={n_latest_reviews}_{sample_size}_{current_date}_{num_run}_samples.txt'\n",
    "    with open(result_file_path, 'w', encoding='utf-8') as result_file:\n",
    "        num_users = len(input_set)        \n",
    "        for userIndex in range(num_users):\n",
    "            print(f\"\\nProcessing user {userIndex + 1}/{num_users}\")\n",
    "            retries, success = 0, False\n",
    "            while not success and retries < MAX_RETRIES:\n",
    "                try:\n",
    "                    example_user = [input_set[userIndex]]\n",
    "                    latest_reviews = extract_latest_n_reviews(example_user, n_latest_reviews)\n",
    "                    # Randomize the order of the latest reviews\n",
    "                    print(f\"number of reviews {len(latest_reviews)}\")\n",
    "                    review_text = \"\\n\".join([\n",
    "                        f\"Product: {rev['product_name']}\\nReview: {rev['text']}\"\n",
    "                        for rev in latest_reviews\n",
    "                    ])\n",
    "                    if not latest_reviews:\n",
    "                        result_file.write(f\"User {userIndex + 1} skipped due to no latest reviews.\\n\\n\")\n",
    "                        skipped_users.append(userIndex + 1)\n",
    "                        break\n",
    "                    print(f\"generating profile for {review_text}\")\n",
    "                    # Generate user profile\n",
    "                    profile = recommender_model.create_user_profile_alpaca_adapter(review_text)\n",
    "                    print(f\"profile {profile}\")\n",
    "                    if not profile or not any(keyword in profile.lower() for keyword in [\"short-term\", \"long-term\", \"user profile\"]):\n",
    "                        result_file.write(f\"User {userIndex + 1} skipped due to empty or invalid profile.\\n\\n\")\n",
    "                        skipped_users.append(userIndex + 1)\n",
    "                        retries += 1  # Increment retries if you want to retry generating the profile\n",
    "                        print(f\"Profile invalid for user {userIndex + 1}. Retrying ({retries}/{MAX_RETRIES})...\")\n",
    "                        continue  # Retry or break, depending on your desired behavior\n",
    "            # If you don't want to retry, use 'break' instead of 'continue'\n",
    "        \n",
    "        # Write profile to result file\n",
    "                    \n",
    "                    # Write profile to result file\n",
    "                    result_file.write(f\"User {userIndex + 1} Profile:\\n{profile}\\n\\n\")\n",
    "                    \n",
    "                    # Generate preliminary recommendations and collect results\n",
    "                    retries_item = 0\n",
    "                    while retries_item < MAX_RETRIES:\n",
    "                        try:\n",
    "                            print(f\"Generating items for user {userIndex + 1}\")\n",
    "                            preliminary_rec = recommender_model.create_preliminary_recommendations_alpaca_adapter(profile)\n",
    "                            print(f\"preliminary_rec {preliminary_rec}\")\n",
    "                            # Write preliminary recommendations to result file\n",
    "\n",
    "\n",
    "                            product_names = extract_product_names_adapter(preliminary_rec)\n",
    "                            if not product_names:\n",
    "                                result_file.write(f\"User {userIndex + 1} skipped due to empty product names.\\n\\n\")\n",
    "                                skipped_users.append(userIndex + 1)\n",
    "                                raise Exception\n",
    "                            result_file.write(f\"User {userIndex + 1} Preliminary Recommendations:\\n{preliminary_rec}\\n\\n\")\n",
    "                            result_file.write(f\"User {userIndex + 1} Extracted products :\\n{product_names}\\n\\n\")\n",
    "                            print(f\"extracted product names {product_names}\")\n",
    "                            break  # Success, exit retry loop\n",
    "                        except Exception as e:\n",
    "                            retries_item += 1\n",
    "                            print(f\"Error generating items for user {userIndex + 1}: {e}\")\n",
    "                            print(f\"Retrying ({retries_item}/{MAX_RETRIES})...\")\n",
    "                    else:\n",
    "                        result_file.write(f\"User {userIndex + 1} skipped after {MAX_RETRIES} retries in generating profile.\\n\\n\")\n",
    "                        skipped_users.append(userIndex + 1)\n",
    "                        user_skipped = True\n",
    "                        continue  # Skip to next user\n",
    "\n",
    "                    \n",
    "                    #extract user history \n",
    "                    user_history = [rev['parent_asin'] for rev in latest_reviews if 'parent_asin' in rev]\n",
    "                    print(f\"user history id: {user_history}\")  # Output: ['ASIN123', 'ASIN456']\n",
    "                    final_results = collect_results_per_product(product_names, collection,user_history, max_products=20)\n",
    "                    if final_results == -1:\n",
    "                        result_file.write(f\"User {userIndex + 1} skipped due to no recommendations.\\n\\n\")\n",
    "                        skipped_users.append(userIndex + 1)\n",
    "                        raise Exception\n",
    "\n",
    "                    example_user_test = [input_set_test[userIndex]]\n",
    "                    test_review = extract_latest_n_reviews(example_user_test, 1)\n",
    "                    test_product = test_review[0]['parent_asin']\n",
    "\n",
    "\n",
    "                    recommended_products = []\n",
    "                    for doc,distance, metadata in final_results:\n",
    "                        asin = metadata['metadata']\n",
    "                        filt = df['parent_asin'] == asin\n",
    "                        title = asin\n",
    "                        print(asin)\n",
    "                        if len(title) > 0:\n",
    "                            recommended_products.append(title)\n",
    "                        else:\n",
    "                            recommended_products.append(doc) \n",
    "\n",
    "                    # Evaluate recommendations\n",
    "                    normalized_test_product = normalize(test_product)\n",
    "                    normalized_ranked_products = [normalize(name) for name in recommended_products]\n",
    "                    similarity_scores = []\n",
    "                    matches = []\n",
    "                    for rec_product in normalized_ranked_products:\n",
    "                        sim_score = compute_similarity(rec_product, normalized_test_product)\n",
    "                        similarity_scores.append(sim_score)\n",
    "                        matches.append(sim_score >= SIMILARITY_THRESHOLD)\n",
    "\n",
    "                    # Display similarity scores and matches\n",
    "                    print(\"\\nSimilarity Scores and Matches:\")\n",
    "                    for idx, (asin, score, match) in enumerate(zip(recommended_products, similarity_scores, matches), 1):\n",
    "                        print(f\"{idx}. {asin}\")\n",
    "                        print(f\"   Similarity Score: {score:.2f}%\")\n",
    "                        print(f\"   Match: {'Yes' if match else 'No'}\")\n",
    "                    \n",
    "                    # Write results for each user\n",
    "                    result_file.write(f\"User {userIndex + 1}:\\n\")\n",
    "                    result_file.write(f\"Test Product: {test_product}\\n\")\n",
    "                    result_file.write(f\"Recommended Products:\\n\")\n",
    "                    for i, (asin, score, match) in enumerate(zip(recommended_products, similarity_scores, matches)):\n",
    "                        result_file.write(f\"  {i+1}. {asin} - Similarity: {score:.2f}% - {'Match' if match else 'No Match'}\\n\")\n",
    "                    result_file.write(\"\\n\")\n",
    "\n",
    "                    # Collect similarity scores and matches\n",
    "                    all_similarity_scores.extend(similarity_scores)\n",
    "                    all_matches.extend(matches)\n",
    "\n",
    "                    # Calculate Recall@K and NDCG@K for this user for all K values\n",
    "                    for k in K_VALUES:\n",
    "                        recall = recall_at_k(matches, k)\n",
    "                        ndcg = ndcg_at_k(matches, k)\n",
    "                        all_recalls.append((k, recall))\n",
    "                        all_ndcgs.append((k, ndcg))\n",
    "                    \n",
    "                    success = True\n",
    "                except Exception as e:\n",
    "                    retries += 1\n",
    "                    if retries >= MAX_RETRIES:\n",
    "                        result_file.write(f\"User {userIndex + 1} skipped after {MAX_RETRIES} retries.\\n\\n\")\n",
    "                        skipped_users.append(userIndex + 1)\n",
    "\n",
    "        # Calculate and write overall metrics for each K\n",
    "        for k in K_VALUES:\n",
    "            recalls_at_k = [rec for k_val, rec in all_recalls if k_val == k]\n",
    "            ndcgs_at_k = [ndcg for k_val, ndcg in all_ndcgs if k_val == k]\n",
    "            mean_recall = np.mean(recalls_at_k) if recalls_at_k else 0.0\n",
    "            mean_ndcg = np.mean(ndcgs_at_k) if ndcgs_at_k else 0.0\n",
    "            result_file.write(f\"Overall Mean Recall@{k}: {mean_recall}\\nOverall Mean NDCG@{k}: {mean_ndcg}\\n\")\n",
    "\n",
    "        if skipped_users:\n",
    "            result_file.write(\"Skipped Users:\\n\" + \", \".join(map(str, skipped_users)) + \"\\n\")\n",
    "\n",
    "        print(f\"Experiment completed for sample size {sample_size}. Results saved to {result_file_path}.\")\n",
    "\n",
    "# Run experiments for each sample size\n",
    "\n",
    "for sample_size in [32]:\n",
    "    for i in range(4):\n",
    "        run_experiment(sample_size= sample_size,num_run= i, adapter=True)\n",
    "        print(f\"\\nStarting experiment for sample size {sample_size} run number {i} and adapters {True}...\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eco-Friendly Skincare Sets', 'Natural Moisturizing Creams/Lotions', 'Gentle Exfoliants/Peels', 'Vegan/Cruelty-Free Facial Care Kits']\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "test = \"\"\"Here's your output:\n",
    "\n",
    "The recommended product category set aligns closely with the user's expressed long-term preferences and short-term interests. These categories synergize to create an overarching theme emphasizing sustainability, eco-friendliness, natural ingredients, gentle formulations, and targeted benefits for mature skin. The collective assortment aims to satisfy both immediate needs (hydrated and nourished complexion) and long-standing aspirations (environmentally responsible living). Specifically, I recommend considering products from the categories listed below:\n",
    "\n",
    "1. Eco-Friendly Skincare Sets\n",
    "2. Natural Moisturizing Creams/Lotions\n",
    "3. Gentle Exfoliants/Peels\n",
    "4. Vegan/Cruelty-Free Facial Care Kits\"\"\"\n",
    "\n",
    "print(extract_product_names_adapter(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
