{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined JSON file with target field saved as 'new_data/combined_output.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def combine_with_target(train_file, val_file, output_file):\n",
    "    # Load sorted training data\n",
    "    with open(train_file, 'r', encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "    \n",
    "    # Load validation data\n",
    "    with open(val_file, 'r', encoding='utf-8') as f:\n",
    "        val_data = json.load(f)\n",
    "    \n",
    "    # Create a dictionary to map user_id to product_name (target) from the validation data\n",
    "    target_map = {user['user_id']: user['reviews'][0]['product_name'] for user in val_data if user['reviews']}\n",
    "    \n",
    "    # Add target field to each user in the training data based on user_id\n",
    "    for user in train_data:\n",
    "        user_id = user['user_id']\n",
    "        # Find the target product_name if available in the validation data\n",
    "        target_product = target_map.get(user_id)\n",
    "        if target_product:\n",
    "            user['target'] = target_product  # Add the target field to the user data\n",
    "    \n",
    "    # Save the combined data with target field to a new JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Paths to the input files and output file\n",
    "combine_with_target('new_data/sorted_train_output.json', 'new_data/new_val_output.json', 'new_data/combined_output_for_training_chatGPT.json')\n",
    "\n",
    "print(\"Combined JSON file with target field saved as 'new_data/combined_output_for_training_chatGPT.json'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the data from the chatGPT output to alpaca format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def transform_user_profiles(input_file_path, output_file_path, output_format='json'):\n",
    "    # Initialize list to hold user profiles\n",
    "    user_profiles = []\n",
    "\n",
    "    # Read file content\n",
    "    with open(input_file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Split content by each user profile\n",
    "    users = re.split(r\"(User ID: [A-Z0-9]+)\", content)[1:]  # Split while keeping 'User ID' lines\n",
    "\n",
    "    # Group each User ID with their profile\n",
    "    users = [(users[i], users[i + 1].strip()) for i in range(0, len(users), 2)]\n",
    "\n",
    "    # Process each user entry\n",
    "    for user_id, profile in users:\n",
    "        # Remove line breaks and unnecessary white spaces in main profile section\n",
    "        profile_text = re.sub(r\"(\\n\\s*)+\", \" \", profile).strip()\n",
    "        \n",
    "        # Remove unwanted strings from the profile text\n",
    "        profile_text = re.sub(r\"\\(similar to target product\\)\", \"\", profile_text, flags=re.IGNORECASE).strip()\n",
    "        profile_text = re.sub(r\"\\(target product\\)\", \"\", profile_text, flags=re.IGNORECASE).strip()\n",
    "\n",
    "        \n",
    "        # Extract Candidate Items separately, line by line, and number them\n",
    "        candidate_items_match = re.search(r\"Candidate Items:\\s*((?:\\n\\s{4}.*)+)\", profile)\n",
    "        if candidate_items_match:\n",
    "            # Split each item by new line and 4-space indentation, and strip them\n",
    "            items = [\n",
    "                re.sub(r\"\\(target product\\)|\\(similar to target product\\)\", \"\", item, flags=re.IGNORECASE).strip()\n",
    "                for item in candidate_items_match.group(1).split('\\n') if item.strip()\n",
    "            ]\n",
    "            numbered_items = {str(i + 1): items[i] for i in range(len(items))}\n",
    "            profile_text = re.sub(r\"Candidate Items:\\s*((?:\\n\\s{4}.*)+)\", \"\", profile_text).strip()  # Remove items from main profile text\n",
    "        else:\n",
    "            numbered_items = {}\n",
    "\n",
    "        # Prepare dictionary for each user\n",
    "        user_dict = {\n",
    "            \"User_ID\": user_id.split(\": \")[1].strip(),\n",
    "            \"User_Profile\": profile_text,\n",
    "            \"Candidate_Items\": numbered_items\n",
    "        }\n",
    "\n",
    "        user_profiles.append(user_dict)\n",
    "\n",
    "    # Output based on the selected format\n",
    "    if output_format == 'json':\n",
    "        with open(output_file_path, 'w') as json_file:\n",
    "            json.dump(user_profiles, json_file, indent=4)\n",
    "    elif output_format == 'csv':\n",
    "        # Convert to DataFrame and expand Candidate Items\n",
    "        df = pd.json_normalize(user_profiles)\n",
    "        df.to_csv(output_file_path, index=False)\n",
    "    else:\n",
    "        print(\"Unsupported output format. Choose either 'json' or 'csv'.\")\n",
    "\n",
    "# Example usage:\n",
    "input_file_path = 'chatGPT_output_UP.txt'\n",
    "output_file_path = 'chatGPT_UP_output.json'  # or 'output_file.csv'\n",
    "transform_user_profiles(input_file_path, output_file_path, output_format='json')  # Change 'json' to 'csv' for CSV output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All user profiles contain the required sections.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def validate_user_profiles_from_json(file_path, required_keywords):\n",
    "    \"\"\"\n",
    "    Loads user profiles from a JSON file and validates that each profile contains all required keywords.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the JSON file containing user profiles.\n",
    "        required_keywords (list): List of keywords that each profile must contain.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with user IDs as keys and a list of missing keywords as values.\n",
    "    \"\"\"\n",
    "    # Load profiles from JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        profiles = json.load(file)\n",
    "    \n",
    "    missing_keywords_report = {}\n",
    "\n",
    "    # Validate each profile\n",
    "    for profile in profiles:\n",
    "        user_id = profile[\"User_ID\"]\n",
    "        profile_text = profile[\"User_Profile\"]\n",
    "        \n",
    "        # Check for missing keywords in profile text\n",
    "        missing_keywords = [keyword for keyword in required_keywords if keyword not in profile_text]\n",
    "        \n",
    "        # Only record if there are missing keywords\n",
    "        if missing_keywords:\n",
    "            missing_keywords_report[user_id] = missing_keywords\n",
    "\n",
    "    return missing_keywords_report\n",
    "\n",
    "# Example usage:\n",
    "required_keywords = [\n",
    "    \"Short-term Intentions\", \"Long-term Preferences\", \"Item Descriptions\",\n",
    "    \"Preferences and Dislikes\", \"Explicit and Implicit Signals\", \"Contextual Features\", \"Candidate Items\"\n",
    "]\n",
    "\n",
    "# Path to the JSON file containing user profiles\n",
    "file_path = 'path_to_your_user_profiles.json'\n",
    "missing_report = validate_user_profiles_from_json(output_file_path, required_keywords)\n",
    "\n",
    "# Output the results\n",
    "if missing_report:\n",
    "    for user_id, missing in missing_report.items():\n",
    "        print(f\"User ID {user_id} is missing the following sections: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"All user profiles contain the required sections.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
