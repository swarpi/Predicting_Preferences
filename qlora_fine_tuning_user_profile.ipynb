{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import necessary modules and load configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Loaded:\n",
      "Tokenizer Path: models/hf-frompretrained-download/meta-llama/Meta-Llama-3-8B-Instruct\n",
      "Model Path: models/hf-frompretrained-downloadmeta-llama/Meta-Llama-3-8B-Instruct\n",
      "Pipeline Parameters: {'max_length': 2048, 'num_return_sequences': 1, 'temperature': 0.7, 'top_k': 50, 'top_p': 0.95, 'repetition_penalty': 1.2}\n",
      "QLoRA Parameters: {'lora_r': 8, 'lora_alpha': 8, 'lora_dropout': 0.01, 'lora_target_modules': ['q_proj', 'v_proj'], 'gradient_accumulation_steps': 2, 'lora_num_epochs': 2, 'lora_val_iterations': 100, 'lora_early_stopping_patience': 10, 'lora_lr': 0.0001, 'lora_micro_batch_size': 1}\n",
      "Prompt Template: {'instruction': \"### Instruction:\\n You are a recommender system specialized in creating user profiles. Your task is to create a comprehensive user profile based on a user's review. The user Profile consists of: 1. Short-term Interests: Examine the user's most recent items along with their personalized descriptions 2. Long-term Preferences: Analyze all reviews and items from the user’s entire history to capture deeper, stable interests that define the user’s lasting preferences. Use this comprehensive historical data to outline consistent themes and inclinations that have remained steady over time. 3. User Profile Summary: Synthesize these findings into a concise profile (max 200 words) that combines insights from both short-term interests and long-term preferences. Use the short-term interests as a query to refine or highlight relevant themes from the long-term history, and provide a cohesive picture of the user’s tastes, typical habits, and potential future interests.Present your analysis under clear section headings.Do Not include any Code in your response. Think step by step\", 'input': '{user_review}', 'output': '### Response:'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "\n",
    "# Set environment variable for memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Load configuration settings\n",
    "from config import TOKENIZER_PATH, MODEL_PATH, PIPELINE_PARAMS, QLORA_PARAMS,ALPACA_LORA_PROMPTS_USER_PROFILE\n",
    "\n",
    "# Verification\n",
    "print(\"Configuration Loaded:\")\n",
    "print(\"Tokenizer Path:\", TOKENIZER_PATH)\n",
    "print(\"Model Path:\", MODEL_PATH)\n",
    "print(\"Pipeline Parameters:\", PIPELINE_PARAMS)\n",
    "print(\"QLoRA Parameters:\", QLORA_PARAMS)\n",
    "print(\"Prompt Template:\", ALPACA_LORA_PROMPTS_USER_PROFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and verify training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Sample: [{'User_ID': 'AEE4M36AZAKURLEYGV23TM3BE7OQ', 'User_Profile': '\"Short-Term Interests\": The user has recently engaged with hair care products focused on moisturizing and detangling, such as shampoos and conditioners with natural oils, hair towels, and convenient towelettes.\\n\"Long-Term Preferences\": An analysis of the user\\'s reviews reveals consistent themes:\\n* Preference for moisturizing hair products that leave fine, shoulder-length hair shiny and bouncy without greasiness\\n* Interest in products that help with detangling, reducing frizz, and minimizing static electricity\\n* Appreciation for high-quality hair accessories like twist towels and soft hair towels\\n* Values convenience items for on-the-go use, such as individually packaged towelettes\\n* Preference for natural ingredients like coconut oil and avocado oil in hair care\\n\"User_Profile\": The user appears to prioritize hair care and values products that enhance the health and appearance of their fine, shoulder-length hair. They seek moisturizing shampoos and conditioners that provide shine and manageability without weighing hair down. The user appreciates effective detangling solutions and products that combat frizz and static. They value quality hair accessories and convenient personal care items, showing a preference for natural ingredients in their hair care routine.', 'Candidate_Items': {'1': 'Anti-Frizz Hair Serums', '2': 'Leave-In Conditioners with Natural Oils', '3': 'Detangling Hair Brushes', '4': 'Travel-Friendly Dry Shampoos', '5': 'Silk Pillowcases for Hair Care'}}, {'User_ID': 'AEFRTLVCVRALKXBED77KHPIXEPWQ', 'User_Profile': '\"Short-Term Interests\": The user has recently engaged with skincare products suitable for sensitive skin, including face masks, headbands designed to relieve mask pressure, and lip balms with sun protection.\\n\"Long-Term Preferences\": An analysis of the user\\'s reviews reveals consistent themes:\\n* Interest in skincare products that soothe and hydrate sensitive skin without causing irritation\\n* Preference for products that provide noticeable results, such as firmer or more hydrated skin\\n* Appreciation for convenience and comfort in products, like headbands with buttons for mask straps\\n* Values sun protection in lip care products without adverse reactions\\n* Open to trying new skincare treatments, even if instructions are not in their native language\\n\"User_Profile\": The user is focused on skincare solutions that cater to sensitive skin, seeking products that offer hydration, soothing effects, and anti-aging benefits without irritation. They value practicality and comfort in accessories that enhance their daily routines. The user is health-conscious, preferring products with sun protection, and appreciates items that deliver visible results.', 'Candidate_Items': {'1': 'Hypoallergenic Mascara for Sensitive Eyes', '2': 'SPF Lip Balms for Sensitive Lips', '3': 'Cooling Gel Eye Masks', '4': 'Gentle Facial Cleansers for Sensitive Skin', '5': 'Soft Makeup Brushes for Delicate Skin'}}]\n",
      "Data Structure Verification:\n",
      "Data verification successful!\n"
     ]
    }
   ],
   "source": [
    "data_path = \"QLoRa_finetuning/matching_ids_chatGPT.json\"\n",
    "\n",
    "# Load the training data\n",
    "with open(data_path, \"r\") as file:\n",
    "    training_data = json.load(file)\n",
    "\n",
    "# Sample a couple of data points to verify format\n",
    "print(\"Training Data Sample:\", training_data[:2])  # Display first two entries\n",
    "# Clear cache before loading model\n",
    "torch.cuda.empty_cache()\n",
    "# Verify data structure\n",
    "print(\"Data Structure Verification:\")\n",
    "for i, sample in enumerate(training_data[:2]):\n",
    "    assert \"User_ID\" in sample, f\"User_ID missing in sample {i}\"\n",
    "    assert \"User_Profile\" in sample, f\"User_Profile missing in sample {i}\"\n",
    "    assert \"Candidate_Items\" in sample, f\"Candidate_Items missing in sample {i}\"\n",
    "print(\"Data verification successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize the Tokenizer and Model with Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\quantizers\\auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS if not already set\n",
    "\n",
    "# Set 4-bit quantization configuration for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',  # Use NormalFloat4 for better memory efficiency\n",
    "    bnb_4bit_use_double_quant=True  # Double quantization for more memory saving\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"  # Automatically maps layers to available GPU memory\n",
    ")\n",
    "# Load user reviews\n",
    "reviews_path = \"new_data/new_train_output.json\"\n",
    "with open(reviews_path, \"r\") as file:\n",
    "    reviews_data = json.load(file)\n",
    "\n",
    "# Index reviews by user_id for easy matching\n",
    "reviews_by_user = {entry[\"user_id\"]: entry[\"reviews\"] for entry in reviews_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Preprocessing Function to Match Reviews with Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Sample: \n",
      "### Instruction:\n",
      " You are a recommender system specialized in creating user profiles. Your task is to create a comprehensive user profile based on a user's review. The user Profile consists of: 1. Short-term Interests: Examine the user's most recent items along with their personalized descriptions 2. Long-term Preferences: Analyze all reviews and items from the user’s entire history to capture deeper, stable interests that define the user’s lasting preferences. Use this comprehensive historical data to outline consistent themes and inclinations that have remained steady over time. 3. User Profile Summary: Synthesize these findings into a concise profile (max 200 words) that combines insights from both short-term interests and long-term preferences. Use the short-term interests as a query to refine or highlight relevant themes from the long-term history, and provide a cohesive picture of the user’s tastes, typical habits, and potential future interests.Present your analysis under clear section headings.Do Not include any Code in your response. Think step by step\n",
      "\n",
      "### Input:\n",
      "User reviews:\n",
      "Product: Lemon Oil Towelettes - 20 Count\n",
      "Rating: 4.0\n",
      "Title: Very Convenient\n",
      "Review: These towelettes are great for keeping in your car's glove compartment, putting inside your tote bag, or stashing away in your office desk. They're very handy for times when you can't get to a sink and use soap and water. They also smell great. They're a little pricey for everyday use, so I took off one star for that.\n",
      "\n",
      "Product: John Frieda Detox and Repair Shampoo and Conditioner Set with Nourishing Avocado Oil and Green Tea, 8.45 Fl Oz (Pack of 2)\n",
      "Rating: 4.0\n",
      "Title: Good Set\n",
      "Review: Avocado oil seems to be good at moisturizing without being greasy. I have some body lotion with avocado oil and really like that, so I decided to try this shampoo/conditioner set. It does a very good job of cleaning your hair and leaving it shiny and bouncy. I still prefer coconut oil for hair, but avocado oil is doing a good job, too.\n",
      "\n",
      "Product: Microfiber Hair Towels - 5 Towel Set – 5 Beautiful Colors - Great for all Hair Types - Size (10 x 26 inches) Lightweight, Absorbent, and Ultra Soft. Colors Rose, Pink, Gray, Blue, and Lilac.\n",
      "Rating: 4.0\n",
      "Title: Nice Towels\n",
      "Review: These hair towels are very soft and comfortable. This set is a good value because you get 5 of them. They're well-made and should last a long time.\n",
      "\n",
      "Product: Maya Mari -  Castor Oil Shampoo for Women, Strengthening and Moisturizing Shampoo for Coarse, Textured, and Curly Hair, 32 oz\n",
      "Rating: 4.0\n",
      "Title: Very Moisturizing\n",
      "Review: This shampoo does an excellent job of moisturizing (and cleaning) your hair. However, you don't want to use it more than 3-4 times a week, because the castor oil builds up and starts making your hair look greasy. I alternate it with another shampoo, and it works great that way.\n",
      "\n",
      "Product: 2 Pack Hair Towel Wrap Turban Microfiber Drying Shower and Bath, Twist Head Towels with Button, Magic Quick Dryer, Hair Care for Women and Girls, One Wrap Plus One Cap and Two Velvet Hair Bands\n",
      "Rating: 4.0\n",
      "Title: Okay\n",
      "Review: I like the twist towel a lot, but I found the shower-cap-like towel to be a little hard to use. My hair is below shoulder-length, and it just didn't work well for me. The twist towel is great, though. I didn't care for the hair ties and won't be using those. If you have longer hair and just want some twist towels, I would go with a different product because the cap probably won't work for you. But if you're hair is shorter, this set would be a pretty good choice.\n",
      "\n",
      "Product: Taya Beauty Buriti Nut Intensive Repair Shampoo & Conditioner Duo – Organic Hydrating Shampoo and Conditioner for Dry Damaged Hair - Restorative Hair Care – Travel Size 2 fl oz\n",
      "Rating: 5.0\n",
      "Title: Nice\n",
      "Review: This shampoo and conditioner set is very moisturizing, but doesn't leave your hair limp or greasy-looking. It leaves my fine, shoulder-length hair bouncy and shiny. The travel size is also very convenient.\n",
      "\n",
      "Product: AG Care Natural Remedy Apple Cider Vinegar Leave On Mist\n",
      "Rating: 5.0\n",
      "Title: Detangles Well\n",
      "Review: This is pretty good. I was surprised at how well it worked for detangling wet hair. The scent is also nice -- it does not smell like vinegar. It does help with frizziness, too, on those really humid days. It also reduces static electricity in your hair on really dry days. I would definitely use this again.\n",
      "\n",
      "\n",
      "### Response:\n",
      "\"Short-Term Interests\": The user has recently engaged with hair care products focused on moisturizing and detangling, such as shampoos and conditioners with natural oils, hair towels, and convenient towelettes.\n",
      "\"Long-Term Preferences\": An analysis of the user's reviews reveals consistent themes:\n",
      "* Preference for moisturizing hair products that leave fine, shoulder-length hair shiny and bouncy without greasiness\n",
      "* Interest in products that help with detangling, reducing frizz, and minimizing static electricity\n",
      "* Appreciation for high-quality hair accessories like twist towels and soft hair towels\n",
      "* Values convenience items for on-the-go use, such as individually packaged towelettes\n",
      "* Preference for natural ingredients like coconut oil and avocado oil in hair care\n",
      "\"User_Profile\": The user appears to prioritize hair care and values products that enhance the health and appearance of their fine, shoulder-length hair. They seek moisturizing shampoos and conditioners that provide shine and manageability without weighing hair down. The user appreciates effective detangling solutions and products that combat frizz and static. They value quality hair accessories and convenient personal care items, showing a preference for natural ingredients in their hair care routine.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(profile_sample):\n",
    "    user_id = profile_sample[\"User_ID\"]\n",
    "    reviews = reviews_by_user.get(user_id, [])\n",
    "    review_texts = [f\"Product: {review['product_name']}\\nRating: {review['rating']}\\nTitle: {review['title']}\\nReview: {review['text']}\\n\" \n",
    "                    for review in reviews]\n",
    "    review_texts = review_texts[-10:]\n",
    "    formatted_reviews = \"\\n\".join(review_texts)\n",
    "    \n",
    "    instruction = ALPACA_LORA_PROMPTS_USER_PROFILE['instruction']\n",
    "    input_text = f\"User reviews:\\n{formatted_reviews}\" if formatted_reviews else \"No reviews available for this user.\"\n",
    "    output_text = profile_sample[\"User_Profile\"]\n",
    "    \n",
    "    full_text = f\"\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output_text}\"\n",
    "    return full_text\n",
    "\n",
    "# Verify preprocessing\n",
    "print(\"Preprocessed Sample:\", preprocess_function(training_data[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Tokenize and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(sample):\n",
    "    processed_text = preprocess_function(sample)\n",
    "    tokenized = tokenizer(\n",
    "        processed_text,\n",
    "        truncation=True,\n",
    "        max_length=PIPELINE_PARAMS['max_length'],\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # Set labels identical to input_ids\n",
    "    return tokenized\n",
    "\n",
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=QLORA_PARAMS['lora_r'],\n",
    "    lora_alpha=QLORA_PARAMS['lora_alpha'],\n",
    "    lora_dropout=QLORA_PARAMS['lora_dropout'],\n",
    "    target_modules=QLORA_PARAMS['lora_target_modules'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure LoRA and Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 32 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 10/32 [00:47<01:44,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.6208, 'grad_norm': 13.78166389465332, 'learning_rate': 7.8125e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 20/32 [01:35<00:56,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7246, 'grad_norm': 11.917302131652832, 'learning_rate': 5e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 30/32 [02:22<00:09,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1181, 'grad_norm': 7.511727333068848, 'learning_rate': 1.8750000000000002e-05, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [02:32<00:00,  4.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 152.3073, 'train_samples_per_second': 0.42, 'train_steps_per_second': 0.21, 'train_loss': 3.690214417874813, 'epoch': 2.0}\n",
      "Model trained with 32 samples saved to outputs/adapter_test_user_profile_epoch_2_32_chatgpt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 64 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 10/64 [00:47<04:15,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6716, 'grad_norm': 0.6801082491874695, 'learning_rate': 8.593750000000001e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 20/64 [01:34<03:28,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3797, 'grad_norm': 0.5264413356781006, 'learning_rate': 7.031250000000001e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 30/64 [02:21<02:41,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6693, 'grad_norm': 0.5875918865203857, 'learning_rate': 5.46875e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 40/64 [03:09<01:53,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3365, 'grad_norm': 0.48923295736312866, 'learning_rate': 3.90625e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 50/64 [03:56<01:06,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.42, 'grad_norm': 0.5539653897285461, 'learning_rate': 2.34375e-05, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 60/64 [04:43<00:18,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4544, 'grad_norm': 0.7127138376235962, 'learning_rate': 7.8125e-06, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [05:02<00:00,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 302.3773, 'train_samples_per_second': 0.423, 'train_steps_per_second': 0.212, 'train_loss': 1.4941842406988144, 'epoch': 2.0}\n",
      "Model trained with 64 samples saved to outputs/adapter_test_user_profile_epoch_2_64_chatgpt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRA configuration to the model\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.gradient_checkpointing_enable()\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Training sizes\n",
    "training_sizes = [32,64]\n",
    "\n",
    "# Loop through different training sizes\n",
    "for train_size in training_sizes:\n",
    "    # Split the dataset\n",
    "    train_data = training_data[:train_size]\n",
    "    eval_data = training_data[train_size:train_size + int(0.2 * train_size)]  # 20% of training data for evaluation\n",
    "\n",
    "    # Tokenize datasets\n",
    "    train_tokenized_data = [tokenize_function(sample) for sample in train_data]\n",
    "    eval_tokenized_data = [tokenize_function(sample) for sample in eval_data]\n",
    "\n",
    "    # Convert tokenized data to Dataset format\n",
    "    train_dataset = Dataset.from_dict({\n",
    "        \"input_ids\": [x[\"input_ids\"][0] for x in train_tokenized_data],\n",
    "        \"attention_mask\": [x[\"attention_mask\"][0] for x in train_tokenized_data],\n",
    "        \"labels\": [x[\"labels\"][0] for x in train_tokenized_data]\n",
    "    })\n",
    "    eval_dataset = Dataset.from_dict({\n",
    "        \"input_ids\": [x[\"input_ids\"][0] for x in eval_tokenized_data],\n",
    "        \"attention_mask\": [x[\"attention_mask\"][0] for x in eval_tokenized_data],\n",
    "        \"labels\": [x[\"labels\"][0] for x in eval_tokenized_data]\n",
    "    })\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"outputs/adapter_test_user_profile_epoch_{QLORA_PARAMS['lora_num_epochs']}_{train_size}_chatgpt_data\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=QLORA_PARAMS['gradient_accumulation_steps'],\n",
    "        num_train_epochs=QLORA_PARAMS['lora_num_epochs'],\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=QLORA_PARAMS['lora_val_iterations'],\n",
    "        save_steps=QLORA_PARAMS['lora_val_iterations'],\n",
    "        logging_steps=10,\n",
    "        learning_rate=QLORA_PARAMS['lora_lr'],\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=\"none\",\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, padding=True)\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    # Clear GPU cache before training\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Start training\n",
    "    print(f\"Starting training with {train_size} samples.\")\n",
    "    trainer.train()\n",
    "    adapter_name = f\"adapter_test_user_profile_epoch_{QLORA_PARAMS['lora_num_epochs']}_{train_size}_chatgpt_data\"\n",
    "    # Save the model and tokenizer in separate directories for each training size\n",
    "    model.save_pretrained(f\"outputs/{adapter_name}\")\n",
    "    tokenizer.save_pretrained(f\"outputs/{adapter_name}\")\n",
    "    print(f\"Model trained with {train_size} samples saved to outputs/{adapter_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Loaded:\n",
      "Tokenizer Path: models/hf-frompretrained-download/meta-llama/Meta-Llama-3-8B-Instruct\n",
      "Model Path: models/hf-frompretrained-downloadmeta-llama/Meta-Llama-3-8B-Instruct\n",
      "Pipeline Parameters: {'max_length': 2048, 'num_return_sequences': 1, 'temperature': 0.7, 'top_k': 50, 'top_p': 0.95, 'repetition_penalty': 1.2}\n",
      "QLoRA Parameters: {'lora_r': 8, 'lora_alpha': 8, 'lora_dropout': 0.01, 'lora_target_modules': ['q_proj', 'v_proj'], 'gradient_accumulation_steps': 2, 'lora_num_epochs': 2, 'lora_val_iterations': 100, 'lora_early_stopping_patience': 10, 'lora_lr': 0.0001, 'lora_micro_batch_size': 1}\n",
      "Prompt Template: {'instruction': \"### Instruction:\\n You are a recommender system specialized in creating user profiles. Your task is to create a comprehensive user profile based on a user's review. The user Profile consists of: 1. Short-term Interests: Examine the user's most recent items along with their personalized descriptions 2. Long-term Preferences: Analyze all reviews and items from the user’s entire history to capture deeper, stable interests that define the user’s lasting preferences. Use this comprehensive historical data to outline consistent themes and inclinations that have remained steady over time. 3. User Profile Summary: Synthesize these findings into a concise profile (max 200 words) that combines insights from both short-term interests and long-term preferences. Use the short-term interests as a query to refine or highlight relevant themes from the long-term history, and provide a cohesive picture of the user’s tastes, typical habits, and potential future interests.Present your analysis under clear section headings.Do Not include any Code in your response. Think step by step\", 'input': '{user_review}', 'output': '### Response:'}\n",
      "Training Data Sample: [{'User_ID': 'AFQQQ5LGNSQUEBGDCYBAZZE5T3DA', 'User_Profile': '\"Short-Term Interests\": The user recently reviewed products focused on facial skincare, specifically creams and serums targeting anti-aging concerns such as fine lines, wrinkles, and dehydration. They showed interest in natural and organic ingredients, vegan-friendly options, and cruelty-free practices. Their preference leans towards face washes and moisturizers that cater to specific skin concerns like acne-prone, sensitive, and dry skin.\\n\"Long-Term Preferences\": Based on the entirety of their reviews, some common denominators emerge:\\n* Focus on natural ingredients, particularly plant-based extracts, essential oils, and herbal remedies\\n* Concern for anti-aging and rejuvenation, seeking effective solutions for maintaining healthy-looking skin\\n* Interest in sheet masks, especially those with unique features like eye patches and targeted application areas\\n* Appreciation for moisturized and hydrated skin, often mentioning specific requirements like deep hydration and non-sticky textures\\n* Willingness to explore various brands and products, demonstrating adaptability and open-mindedness\\n\"User_Profile\": Our user appears to be someone who prioritizes natural and organic approaches to skincare while focusing on addressing specific skin concerns. They exhibit a willingness to experiment with diverse products and techniques to achieve optimal results. As they continue exploring the world of skincare, they may gravitate toward more niche markets, such as bespoke formulas tailored to individual skin types or advanced technology-driven treatments. For now, our recommendation would focus on recommending products featuring natural, sustainable, and innovative formulations catering to their varied skin needs, including hydration, anti-aging, and sensitivity management. A suggested next step could involve introducing them to emerging trends in customized skincare regimens and cutting-edge technologies for enhanced customer experience.', 'Candidate_Items': {'1': 'Nourishing Skin Serenity', '2': 'Botanical Beauty Discovery', '3': 'Hydrating Harmony Regimen', '4': 'Natural Radiance Revival', '5': 'Advanced Anti-Aging Adaptations'}}, {'User_ID': 'AEHLKY7Q5O3D3E6YEV67JIBVFNFA', 'User_Profile': '\"Short-Term Interests\": The user has recently engaged with therapeutic home lighting and Korean skincare products suitable for sensitive skin.\\n\"Long-Term Preferences\": An analysis of the user\\'s reviews reveals consistent themes:\\n* Passion for Korean skincare brands like Easydew\\n* Preference for products that are gentle, effective, and absorb well without residue\\n* Critical of heavy creams that do not absorb properly\\n* Appreciation for therapeutic and aesthetic home products that enhance relaxation\\n* Values quality and reasonable pricing in products\\n\"User_Profile\": The user is highly engaged with skincare, especially Korean brands catering to sensitive skin. They value effective, gentle products that provide a pleasant experience. Additionally, they enjoy home products that contribute to a calming environment. Quality and effectiveness, along with reasonable pricing, are important to them.', 'Candidate_Items': {'1': 'Aromatherapy Essential Oil Diffuser with LED Lights', '2': 'Gentle Exfoliating Korean Peeling Gel', '3': 'Hydrating Facial Toner for Sensitive Skin', '4': 'LED Light Therapy Face Mask for Skin Rejuvenation', '5': 'Silk Pillowcase for Hair and Skin Health'}}]\n",
      "Data Structure Verification:\n",
      "Data verification successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\quantizers\\auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Sample: \n",
      "### Instruction:\n",
      " You are a recommender system specialized in creating user profiles. Your task is to create a comprehensive user profile based on a user's review. The user Profile consists of: 1. Short-term Interests: Examine the user's most recent items along with their personalized descriptions 2. Long-term Preferences: Analyze all reviews and items from the user’s entire history to capture deeper, stable interests that define the user’s lasting preferences. Use this comprehensive historical data to outline consistent themes and inclinations that have remained steady over time. 3. User Profile Summary: Synthesize these findings into a concise profile (max 200 words) that combines insights from both short-term interests and long-term preferences. Use the short-term interests as a query to refine or highlight relevant themes from the long-term history, and provide a cohesive picture of the user’s tastes, typical habits, and potential future interests.Present your analysis under clear section headings.Do Not include any Code in your response. Think step by step\n",
      "\n",
      "### Input:\n",
      "User reviews:\n",
      "Product: Moisturizing Facial Emulsion for Restoring Hydrating Smoothing Skin from Manilla Natural Skincare\n",
      "Rating: 5.0\n",
      "Title: Intense Hydration\n",
      "Review: With winter approaching, it's time to start thinking about cold-weather skincare. This moisturizer is a good choice because it's a blend of several skin-loving oils like argan, coconut and macadamia. It's fairly lightweight but it protects and hydrates like a heavier cream. It can be used both day and night. The packaging is pretty enough for display. It can be used by all skin types. I highly recommend this product and the entire line from this brand.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "Product: Dr. Denese SkinScience Essential Lipid Anti Aging Power Infusion Dry Oil - Skin Nutrients 97% Organic 100% Natural - Rejuvinating Blend with Amaranth & Resveratrol - Paraben-Free, Cruelty-Free - 2oz\n",
      "Rating: 5.0\n",
      "Title: Youth Serum\n",
      "Review: One of the reasons skin starts to wrinkle as we age is that we make less and less of the natural oil that keeps skin looking youthful. This serum is a good way to supplement what your skin lacks. The oils in this blend are all high quality. The texture is lightweight and easily absorbed. Dr. Denese is a trusted brand when it comes to care for aging skin. Whether you're trying to slow the clock or reverse it a little, this serum should be part of your daily skincare routine. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "Product: [Abib] Creme coating mask Tone-up solution 17g (5pcs)\n",
      "Rating: 5.0\n",
      "Title: Doesn't Leave Skin Sticky\n",
      "Review: I use lots of sheet masks. Most leave your skin a little sticky when the serum dries. This mask works differently. The sheet locks the moisture in and doesn't leave a sticky residue. It's a great hydration and brightening mask. I highly recommend it.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "Product: Easydew 2-Step Face Contour Sheet Mask - Tightening & Anti-Aging & Hydrating Korean Face Mask with DW-EGF, Hyaluronic Acid for Anti-Winkle, Reducing Fine Line & Producing Collagen (Qty 5)\n",
      "Rating: 5.0\n",
      "Title: Special Care Mask\n",
      "Review: I really like that this mask includes special patches for the eyes and smile lines. If you're over 30, you'll appreciate the extra help. You apply the patches first, the the sheet mask. I keep all of my sheet masks in the refrigerator. You'll appreciate the cooling and anti-inflammatory effect if you do so with this mask. I highly recommend this product.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "Product: Higher Education Skincare: Goal Digger - Silky Moisturizing Cucumber Creme; formulated for dry and sensitive skin; cucumber extract; natural extract: melon, kale, cabbage, ginger, turmeric - 1.7 fl oz\n",
      "Rating: 5.0\n",
      "Title: Lightweight and Creamy\n",
      "Review: I really like this moisturizer. It's lightweight and creamy. It has a light cucumber scent that I find refreshing. It's a powerhouse at deeply hydrating parched skin. It also locks in moisture to keep skin looking fresh. It contains botanicals that soothe irritated or inflamed skin. It's ideal for those with acne, rosacea or eczema. You can use it as both a day and night cream. I highly recommend this product.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "Product: Queendom Unlashed Mascara | Volumizing and Lengthening | Boosts Lash Length | Vegan, Cruelty Free, Paraben Free\n",
      "Rating: 5.0\n",
      "Title: New Favorite!!\n",
      "Review: Every mascara claims to be new and different. This one actually lives up to the hype. The fibers have a novel shape that really makes your lashes POP. If you're not a fan of falsies, this is a great alternative. The formula isn't waterproof, so keep that in mind. I HIGHLY recommend this product.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "Product: ABOUT ME MediAnswer Collagen Firming Up Mask 4ea (1box) 25g - 77% Pure Collagen Extract Anti-Aging Total Care Facial Mask Sheet, Powerful Hydrating and Anti-Wrinkle Night Skin Care\n",
      "Rating: 5.0\n",
      "Title: Effective Mask\n",
      "Review: If you're reading this review, you probably already know the anti-aging benefits of collagen. This mask has a high-potency form of collagen along with a special delivery system to help firm your skin. The mask is in two parts, so you can get a better fit than a regular sheet mask. I can't say it performs miracles, but it's definitely worth a try.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "Product: 13.5 fl. Oz. Goat Milk Facial Cleanser, Moisturizing Face Wash for Women, Hydrating Natural Face Wash, Anti-aging Face Wash, Face Wash for Aging Ski\n",
      "Rating: 5.0\n",
      "Title: Gentle & Natural\n",
      "Review: I'm trying to do a better job with paying as much attention to the ingredients in my skincare as those in my food. This is a great natural cleanser. It's very gentle and is good for sensitive skin. The lactic acid hydrates and gently exfoliates over time. You won't need a science degree to understand the majority of the ingredients. This is a great clean beauty cleanser that I highly recommend.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "\n",
      "### Response:\n",
      "\"Short-Term Interests\": The user recently reviewed products focused on facial skincare, specifically creams and serums targeting anti-aging concerns such as fine lines, wrinkles, and dehydration. They showed interest in natural and organic ingredients, vegan-friendly options, and cruelty-free practices. Their preference leans towards face washes and moisturizers that cater to specific skin concerns like acne-prone, sensitive, and dry skin.\n",
      "\"Long-Term Preferences\": Based on the entirety of their reviews, some common denominators emerge:\n",
      "* Focus on natural ingredients, particularly plant-based extracts, essential oils, and herbal remedies\n",
      "* Concern for anti-aging and rejuvenation, seeking effective solutions for maintaining healthy-looking skin\n",
      "* Interest in sheet masks, especially those with unique features like eye patches and targeted application areas\n",
      "* Appreciation for moisturized and hydrated skin, often mentioning specific requirements like deep hydration and non-sticky textures\n",
      "* Willingness to explore various brands and products, demonstrating adaptability and open-mindedness\n",
      "\"User_Profile\": Our user appears to be someone who prioritizes natural and organic approaches to skincare while focusing on addressing specific skin concerns. They exhibit a willingness to experiment with diverse products and techniques to achieve optimal results. As they continue exploring the world of skincare, they may gravitate toward more niche markets, such as bespoke formulas tailored to individual skin types or advanced technology-driven treatments. For now, our recommendation would focus on recommending products featuring natural, sustainable, and innovative formulations catering to their varied skin needs, including hydration, anti-aging, and sensitivity management. A suggested next step could involve introducing them to emerging trends in customized skincare regimens and cutting-edge technologies for enhanced customer experience.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 16 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      " 62%|██████▎   | 10/16 [00:47<00:28,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5189, 'grad_norm': 7.166027545928955, 'learning_rate': 5e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:16<00:00,  4.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 76.8281, 'train_samples_per_second': 0.417, 'train_steps_per_second': 0.208, 'train_loss': 4.366653680801392, 'epoch': 2.0}\n",
      "Model trained with 16 samples saved to outputs/adapter_test_user_profile_epoch_2_16_pipeline_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 32 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 10/32 [00:47<01:44,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7148, 'grad_norm': 3.15006422996521, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 20/32 [01:35<00:57,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.818, 'grad_norm': 0.3939323425292969, 'learning_rate': 4.375e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 30/32 [02:23<00:09,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8396, 'grad_norm': 0.3886214792728424, 'learning_rate': 1.25e-05, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [02:33<00:00,  4.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 153.08, 'train_samples_per_second': 0.418, 'train_steps_per_second': 0.209, 'train_loss': 2.1109755784273148, 'epoch': 2.0}\n",
      "Model trained with 32 samples saved to outputs/adapter_test_user_profile_epoch_2_32_pipeline_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 64 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 10/64 [00:47<04:15,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7333, 'grad_norm': 0.31806567311286926, 'learning_rate': 8.4375e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 20/64 [01:35<03:30,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6215, 'grad_norm': 0.2867985665798187, 'learning_rate': 6.875e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 30/64 [02:22<02:42,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6316, 'grad_norm': 0.30970242619514465, 'learning_rate': 5.3125000000000004e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 40/64 [03:10<01:54,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7365, 'grad_norm': 0.3858627676963806, 'learning_rate': 3.7500000000000003e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 50/64 [03:58<01:07,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4125, 'grad_norm': 0.4139052629470825, 'learning_rate': 2.1875e-05, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 60/64 [04:45<00:18,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5368, 'grad_norm': 0.40809908509254456, 'learning_rate': 6.25e-06, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [05:05<00:00,  4.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 305.3601, 'train_samples_per_second': 0.419, 'train_steps_per_second': 0.21, 'train_loss': 1.5938149020075798, 'epoch': 2.0}\n",
      "Model trained with 64 samples saved to outputs/adapter_test_user_profile_epoch_2_64_pipeline_data\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "\n",
    "# Set environment variable for memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Load configuration settings\n",
    "from config import TOKENIZER_PATH, MODEL_PATH, PIPELINE_PARAMS, QLORA_PARAMS,ALPACA_LORA_PROMPTS_USER_PROFILE\n",
    "\n",
    "# Verification\n",
    "print(\"Configuration Loaded:\")\n",
    "print(\"Tokenizer Path:\", TOKENIZER_PATH)\n",
    "print(\"Model Path:\", MODEL_PATH)\n",
    "print(\"Pipeline Parameters:\", PIPELINE_PARAMS)\n",
    "print(\"QLoRA Parameters:\", QLORA_PARAMS)\n",
    "print(\"Prompt Template:\", ALPACA_LORA_PROMPTS_USER_PROFILE)\n",
    "data_path = \"QLoRa_finetuning/updated_matching_ids_pipeline.json\"\n",
    "\n",
    "# Load the training data\n",
    "with open(data_path, \"r\") as file:\n",
    "    training_data = json.load(file)\n",
    "\n",
    "# Sample a couple of data points to verify format\n",
    "print(\"Training Data Sample:\", training_data[:2])  # Display first two entries\n",
    "# Clear cache before loading model\n",
    "torch.cuda.empty_cache()\n",
    "# Verify data structure\n",
    "print(\"Data Structure Verification:\")\n",
    "for i, sample in enumerate(training_data[:2]):\n",
    "    assert \"User_ID\" in sample, f\"User_ID missing in sample {i}\"\n",
    "    assert \"User_Profile\" in sample, f\"User_Profile missing in sample {i}\"\n",
    "    assert \"Candidate_Items\" in sample, f\"Candidate_Items missing in sample {i}\"\n",
    "print(\"Data verification successful!\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS if not already set\n",
    "\n",
    "# Set 4-bit quantization configuration for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',  # Use NormalFloat4 for better memory efficiency\n",
    "    bnb_4bit_use_double_quant=True  # Double quantization for more memory saving\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"  # Automatically maps layers to available GPU memory\n",
    ")\n",
    "# Load user reviews\n",
    "reviews_path = \"new_data/new_train_output.json\"\n",
    "with open(reviews_path, \"r\") as file:\n",
    "    reviews_data = json.load(file)\n",
    "\n",
    "# Index reviews by user_id for easy matching\n",
    "reviews_by_user = {entry[\"user_id\"]: entry[\"reviews\"] for entry in reviews_data}\n",
    "def preprocess_function(profile_sample):\n",
    "    user_id = profile_sample[\"User_ID\"]\n",
    "    reviews = reviews_by_user.get(user_id, [])\n",
    "    review_texts = [f\"Product: {review['product_name']}\\nRating: {review['rating']}\\nTitle: {review['title']}\\nReview: {review['text']}\\n\" \n",
    "                    for review in reviews]\n",
    "    review_texts = review_texts[-10:]\n",
    "    formatted_reviews = \"\\n\".join(review_texts)\n",
    "    \n",
    "    instruction = ALPACA_LORA_PROMPTS_USER_PROFILE['instruction']\n",
    "    input_text = f\"User reviews:\\n{formatted_reviews}\" if formatted_reviews else \"No reviews available for this user.\"\n",
    "    output_text = profile_sample[\"User_Profile\"]\n",
    "    \n",
    "    full_text = f\"\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output_text}\"\n",
    "    return full_text\n",
    "\n",
    "# Verify preprocessing\n",
    "print(\"Preprocessed Sample:\", preprocess_function(training_data[0]))\n",
    "# Tokenization function\n",
    "def tokenize_function(sample):\n",
    "    processed_text = preprocess_function(sample)\n",
    "    tokenized = tokenizer(\n",
    "        processed_text,\n",
    "        truncation=True,\n",
    "        max_length=PIPELINE_PARAMS['max_length'],\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # Set labels identical to input_ids\n",
    "    return tokenized\n",
    "\n",
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=QLORA_PARAMS['lora_r'],\n",
    "    lora_alpha=QLORA_PARAMS['lora_alpha'],\n",
    "    lora_dropout=QLORA_PARAMS['lora_dropout'],\n",
    "    target_modules=QLORA_PARAMS['lora_target_modules'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "# Apply LoRA configuration to the model\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.gradient_checkpointing_enable()\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Training sizes\n",
    "training_sizes = [16,32,64]\n",
    "\n",
    "# Loop through different training sizes\n",
    "for train_size in training_sizes:\n",
    "    # Split the dataset\n",
    "    train_data = training_data[:train_size]\n",
    "    eval_data = training_data[train_size:train_size + int(0.2 * train_size)]  # 20% of training data for evaluation\n",
    "\n",
    "    # Tokenize datasets\n",
    "    train_tokenized_data = [tokenize_function(sample) for sample in train_data]\n",
    "    eval_tokenized_data = [tokenize_function(sample) for sample in eval_data]\n",
    "\n",
    "    # Convert tokenized data to Dataset format\n",
    "    train_dataset = Dataset.from_dict({\n",
    "        \"input_ids\": [x[\"input_ids\"][0] for x in train_tokenized_data],\n",
    "        \"attention_mask\": [x[\"attention_mask\"][0] for x in train_tokenized_data],\n",
    "        \"labels\": [x[\"labels\"][0] for x in train_tokenized_data]\n",
    "    })\n",
    "    eval_dataset = Dataset.from_dict({\n",
    "        \"input_ids\": [x[\"input_ids\"][0] for x in eval_tokenized_data],\n",
    "        \"attention_mask\": [x[\"attention_mask\"][0] for x in eval_tokenized_data],\n",
    "        \"labels\": [x[\"labels\"][0] for x in eval_tokenized_data]\n",
    "    })\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"outputs/adapter_test_user_profile_epoch_{QLORA_PARAMS['lora_num_epochs']}_{train_size}_pipeline_data\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=QLORA_PARAMS['gradient_accumulation_steps'],\n",
    "        num_train_epochs=QLORA_PARAMS['lora_num_epochs'],\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=QLORA_PARAMS['lora_val_iterations'],\n",
    "        save_steps=QLORA_PARAMS['lora_val_iterations'],\n",
    "        logging_steps=10,\n",
    "        learning_rate=QLORA_PARAMS['lora_lr'],\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=\"none\",\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, padding=True)\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    # Clear GPU cache before training\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Start training\n",
    "    print(f\"Starting training with {train_size} samples.\")\n",
    "    trainer.train()\n",
    "    adapter_name = f\"adapter_test_user_profile_epoch_{QLORA_PARAMS['lora_num_epochs']}_{train_size}_pipeline_data\"\n",
    "    # Save the model and tokenizer in separate directories for each training size\n",
    "    model.save_pretrained(f\"outputs/{adapter_name}\")\n",
    "    tokenizer.save_pretrained(f\"outputs/{adapter_name}\")\n",
    "    print(f\"Model trained with {train_size} samples saved to outputs/{adapter_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video games chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Loaded:\n",
      "Tokenizer Path: models/hf-frompretrained-download/meta-llama/Meta-Llama-3-8B-Instruct\n",
      "Model Path: models/hf-frompretrained-downloadmeta-llama/Meta-Llama-3-8B-Instruct\n",
      "Pipeline Parameters: {'max_length': 2048, 'num_return_sequences': 1, 'temperature': 0.7, 'top_k': 50, 'top_p': 0.95, 'repetition_penalty': 1.2}\n",
      "QLoRA Parameters: {'lora_r': 8, 'lora_alpha': 8, 'lora_dropout': 0.01, 'lora_target_modules': ['q_proj', 'v_proj'], 'gradient_accumulation_steps': 2, 'lora_num_epochs': 2, 'lora_val_iterations': 100, 'lora_early_stopping_patience': 10, 'lora_lr': 0.0001, 'lora_micro_batch_size': 1}\n",
      "Prompt Template: {'instruction': \"### Instruction:\\n You are a recommender system specialized in creating user profiles. Your task is to create a comprehensive user profile based on a user's review. The user Profile consists of: 1. Short-term Interests: Examine the user's most recent items along with their personalized descriptions 2. Long-term Preferences: Analyze all reviews and items from the user’s entire history to capture deeper, stable interests that define the user’s lasting preferences. Use this comprehensive historical data to outline consistent themes and inclinations that have remained steady over time. 3. User Profile Summary: Synthesize these findings into a concise profile (max 200 words) that combines insights from both short-term interests and long-term preferences. Use the short-term interests as a query to refine or highlight relevant themes from the long-term history, and provide a cohesive picture of the user’s tastes, typical habits, and potential future interests.Present your analysis under clear section headings.Do Not include any Code in your response. Think step by step\", 'input': '{user_review}', 'output': '### Response:'}\n",
      "Training Data Sample: [{'User_ID': 'AEVPPTMG43C6GWSR7I2UGRQN7WFQ', 'User_Profile': '\"Short-Term Interests\": The user has recently engaged with VR accessories, including replacement head straps, controller grips, and display stands. They are particularly focused on comfort, ease of use, and whether accessories securely fit the VR headset.\\n\"Long-Term Preferences\": An analysis of the user’s reviews reveals consistent themes:\\n* Critical of products that damage or do not fit VR hardware properly\\n* Values adjustable, comfortable straps and grips that stay in place for active games like Beat Saber\\n* Prefers accessories that allow for easy customization without blocking sensors\\n* Seeks quality materials that hold up over time, especially foam and padding\\n\"User_Profile\": The user is a VR enthusiast who invests in accessories to enhance gameplay experience. They are sensitive to products that may cause wear or damage to their VR gear and value adjustability, comfort, and durability. They often play active or rhythm-based VR games, meaning reliable, well-fitting accessories are a priority.', 'Candidate_Items': {'1': 'VR Cable Management Pulley System', '2': 'Adjustable VR Face Interface with Cooling Vents', '3': 'Hard Shell Carrying Case for VR Headsets', '4': 'VR Controller Grips with Extended Handles', '5': 'Quick-Dry VR Face Pad Covers'}}, {'User_ID': 'AFZUK3MTBIBEDQOPAK3OATUOUKLA', 'User_Profile': '\"Short-Term Interests\": The user has recently engaged with family-friendly video games on various Nintendo platforms and accessories for both Wii and Switch. They enjoy dancing and party games, as well as classic board games adapted for consoles.\\n\"Long-Term Preferences\": An analysis of the user’s reviews reveals consistent themes:\\n* Focus on fun, active, and social gaming experiences (e.g., Just Dance, Monopoly, etc.)\\n* Appreciation for accessories that organize and enhance gameplay with family and friends\\n* Enjoys games that can be played both solo and in group settings (especially with grandchildren)\\n* Sometimes prefers outdoor activities, indicating they use gaming for occasional, not constant, entertainment\\n\"User_Profile\": The user is looking for games and accessories that promote social interaction and easy fun, often playing with grandchildren or during family gatherings. They favor well-known titles, expansions of classic board games, or party-style games on Nintendo consoles. Convenience and organization are also important, as they have multiple game cartridges and systems to manage.', 'Candidate_Items': {'1': 'Family Board Game Expansion Pack for Nintendo Switch', '2': 'Fitness Adventure Game with Motion Controllers', '3': 'Carrying Case Organizer for Switch Game Cartridges', '4': 'Multi-Player Microphone Bundle for Party Games', '5': 'Protective Joy-Con Grips with Adjustable Hand Straps'}}]\n",
      "Data Structure Verification:\n",
      "Data verification successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\quantizers\\auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Sample: \n",
      "### Instruction:\n",
      " You are a recommender system specialized in creating user profiles. Your task is to create a comprehensive user profile based on a user's review. The user Profile consists of: 1. Short-term Interests: Examine the user's most recent items along with their personalized descriptions 2. Long-term Preferences: Analyze all reviews and items from the user’s entire history to capture deeper, stable interests that define the user’s lasting preferences. Use this comprehensive historical data to outline consistent themes and inclinations that have remained steady over time. 3. User Profile Summary: Synthesize these findings into a concise profile (max 200 words) that combines insights from both short-term interests and long-term preferences. Use the short-term interests as a query to refine or highlight relevant themes from the long-term history, and provide a cohesive picture of the user’s tastes, typical habits, and potential future interests.Present your analysis under clear section headings.Do Not include any Code in your response. Think step by step\n",
      "\n",
      "### Input:\n",
      "User reviews:\n",
      "Product: Advanced 3D Deluxe Audio Strap Adapter Kit for Oculus Quest 2 DAS HTC Vive(V2)-FrankenQuest 2(White)\n",
      "Rating: 1.0\n",
      "Title: DO NOT BUY THIS PRODUCT! Ruined my brand new DAS!\n",
      "Review: The portion of the mount that attaches to the DAS is just tight enough to damaged the VIVE DAS but not tight enough to hold it into place when just a bit of force is applied. At first I thought it was supposed to come off until it did it 3-4 more times, each time a bit easier and now the fitting on the DAS that locks it into place is damaged and no longer will hold onto even my VIVE! Thanks for nothing! I REPEAT DO NOT BUY THIS!~<br />They need to redesign this.\n",
      "\n",
      "Product: KIWI design VR Stand Compatible with Quest 2/Rift/Rift S/GO/HTC Vive/Vive Pro/Valve Index/Quest VR Headset and Touch Controllers (Black)\n",
      "Rating: 4.0\n",
      "Title: Looks good, not sure how functional it is though\n",
      "Review: See pics, I have a Vive DAS and rubber insulators on the controllers. I had to put a zip tie under it to hold it a bit higher to allow the controllers to fit properly.\n",
      "\n",
      "Product: Seltureone Accessories Compatible for Quest 2 Controller Full Grip Cover with 2 Controller Caps, Anti-Slip Anti-Fall Protective Silicone Sleeve with Hand Strap for Quest 2 Headset, White\n",
      "Rating: 5.0\n",
      "Title: Really like the controller look with these on\n",
      "Review: The straps are very functional. I do wish they were at a bit of an angle for shooting games. I feel like I have to loosen the straps and it makes them less functional. Works great for Synth Ryder's and Beat Saber though.\n",
      "\n",
      "Product: Eyglo Replacement Elite Strap for Oculus/Meta Quest 2 - Large Head Back Pad, Rigid Headband Reduces Facial Pressure and Enhances Support and Comfort\n",
      "Rating: 4.0\n",
      "Title: Comfortable, Nice fit, no other accessories though\n",
      "Review: This is pretty comfortable and seems to do the job well. The band adjusts easily and seems to hold the headset in place well, even during Beat Saber or Fit XR.<br />Not sure about the Longevity and it's a bit expensive considering you don't get any additional foam for it and there doesn't seem to be any options to replace. Hoping the seller stands behind their product in the event the foam doesn't last long.\n",
      "\n",
      "Product: AMVR Controller Grips for Quest 2, Controller Protector Cover with Anti-Throw Adjustable Knuckle Straps (Black)\n",
      "Rating: 5.0\n",
      "Title: Nice quality for the money\n",
      "Review: I ordered these in black and white and they both came with different strap options. I have 2 quest 2's and it's nice having the different colors for each one. They work really well. The straps have a lot of adjustment options and they don't seem to block the sensors at all. I ended up removing the stock wrist strap so it doesn't flop around when using the controllers.\n",
      "\n",
      "\n",
      "### Response:\n",
      "\"Short-Term Interests\": The user has recently engaged with VR accessories, including replacement head straps, controller grips, and display stands. They are particularly focused on comfort, ease of use, and whether accessories securely fit the VR headset.\n",
      "\"Long-Term Preferences\": An analysis of the user’s reviews reveals consistent themes:\n",
      "* Critical of products that damage or do not fit VR hardware properly\n",
      "* Values adjustable, comfortable straps and grips that stay in place for active games like Beat Saber\n",
      "* Prefers accessories that allow for easy customization without blocking sensors\n",
      "* Seeks quality materials that hold up over time, especially foam and padding\n",
      "\"User_Profile\": The user is a VR enthusiast who invests in accessories to enhance gameplay experience. They are sensitive to products that may cause wear or damage to their VR gear and value adjustability, comfort, and durability. They often play active or rhythm-based VR games, meaning reliable, well-fitting accessories are a priority.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 16 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      " 62%|██████▎   | 10/16 [00:47<00:28,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.5768, 'grad_norm': 5.350457191467285, 'learning_rate': 5e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:16<00:00,  4.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 76.3742, 'train_samples_per_second': 0.419, 'train_steps_per_second': 0.209, 'train_loss': 5.365243315696716, 'epoch': 2.0}\n",
      "Model trained with 16 samples saved to outputs/adapter_test_user_profile_epoch_2_16_chatGPT_data_video_games\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 32 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 10/32 [00:47<01:44,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5151, 'grad_norm': 7.907433986663818, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 20/32 [01:35<00:57,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5558, 'grad_norm': 0.630147397518158, 'learning_rate': 4.375e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 30/32 [02:22<00:09,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2829, 'grad_norm': 0.31669318675994873, 'learning_rate': 1.25e-05, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [02:32<00:00,  4.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 152.4344, 'train_samples_per_second': 0.42, 'train_steps_per_second': 0.21, 'train_loss': 2.0576883628964424, 'epoch': 2.0}\n",
      "Model trained with 32 samples saved to outputs/adapter_test_user_profile_epoch_2_32_chatGPT_data_video_games\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 64 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 10/64 [00:47<04:15,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3325, 'grad_norm': 0.28784531354904175, 'learning_rate': 8.4375e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 20/64 [01:34<03:27,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3786, 'grad_norm': 0.3062327802181244, 'learning_rate': 6.875e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 30/64 [02:22<02:40,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.155, 'grad_norm': 0.3392610549926758, 'learning_rate': 5.3125000000000004e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 40/64 [03:08<01:52,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2023, 'grad_norm': 0.308029443025589, 'learning_rate': 3.7500000000000003e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 50/64 [03:56<01:06,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1089, 'grad_norm': 0.4175208806991577, 'learning_rate': 2.1875e-05, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 60/64 [04:43<00:18,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9826, 'grad_norm': 0.40842369198799133, 'learning_rate': 6.25e-06, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [05:02<00:00,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 302.7249, 'train_samples_per_second': 0.423, 'train_steps_per_second': 0.211, 'train_loss': 1.2022178024053574, 'epoch': 2.0}\n",
      "Model trained with 64 samples saved to outputs/adapter_test_user_profile_epoch_2_64_chatGPT_data_video_games\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "\n",
    "# Set environment variable for memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Load configuration settings\n",
    "from config import TOKENIZER_PATH, MODEL_PATH, PIPELINE_PARAMS, QLORA_PARAMS,ALPACA_LORA_PROMPTS_USER_PROFILE\n",
    "\n",
    "# Verification\n",
    "print(\"Configuration Loaded:\")\n",
    "print(\"Tokenizer Path:\", TOKENIZER_PATH)\n",
    "print(\"Model Path:\", MODEL_PATH)\n",
    "print(\"Pipeline Parameters:\", PIPELINE_PARAMS)\n",
    "print(\"QLoRA Parameters:\", QLORA_PARAMS)\n",
    "print(\"Prompt Template:\", ALPACA_LORA_PROMPTS_USER_PROFILE)\n",
    "data_path = \"QLoRa_finetuning/chatGPT_UP_output_video_games.json\"\n",
    "\n",
    "# Load the training data\n",
    "with open(data_path, \"r\") as file:\n",
    "    training_data = json.load(file)\n",
    "\n",
    "# Sample a couple of data points to verify format\n",
    "print(\"Training Data Sample:\", training_data[:2])  # Display first two entries\n",
    "# Clear cache before loading model\n",
    "torch.cuda.empty_cache()\n",
    "# Verify data structure\n",
    "print(\"Data Structure Verification:\")\n",
    "for i, sample in enumerate(training_data[:2]):\n",
    "    assert \"User_ID\" in sample, f\"User_ID missing in sample {i}\"\n",
    "    assert \"User_Profile\" in sample, f\"User_Profile missing in sample {i}\"\n",
    "    assert \"Candidate_Items\" in sample, f\"Candidate_Items missing in sample {i}\"\n",
    "print(\"Data verification successful!\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS if not already set\n",
    "\n",
    "# Set 4-bit quantization configuration for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',  # Use NormalFloat4 for better memory efficiency\n",
    "    bnb_4bit_use_double_quant=True  # Double quantization for more memory saving\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"  # Automatically maps layers to available GPU memory\n",
    ")\n",
    "# Load user reviews\n",
    "reviews_path = \"new_data/output.json\"\n",
    "with open(reviews_path, \"r\") as file:\n",
    "    reviews_data = json.load(file)\n",
    "\n",
    "# Index reviews by user_id for easy matching\n",
    "reviews_by_user = {entry[\"user_id\"]: entry[\"reviews\"] for entry in reviews_data}\n",
    "def preprocess_function(profile_sample):\n",
    "    user_id = profile_sample[\"User_ID\"]\n",
    "    reviews = reviews_by_user.get(user_id, [])\n",
    "    review_texts = [f\"Product: {review['product_name']}\\nRating: {review['rating']}\\nTitle: {review['title']}\\nReview: {review['text']}\\n\" \n",
    "                    for review in reviews]\n",
    "    review_texts = review_texts[-10:]\n",
    "    formatted_reviews = \"\\n\".join(review_texts)\n",
    "    \n",
    "    instruction = ALPACA_LORA_PROMPTS_USER_PROFILE['instruction']\n",
    "    input_text = f\"User reviews:\\n{formatted_reviews}\" if formatted_reviews else \"No reviews available for this user.\"\n",
    "    output_text = profile_sample[\"User_Profile\"]\n",
    "    \n",
    "    full_text = f\"\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output_text}\"\n",
    "    return full_text\n",
    "\n",
    "# Verify preprocessing\n",
    "print(\"Preprocessed Sample:\", preprocess_function(training_data[0]))\n",
    "# Tokenization function\n",
    "def tokenize_function(sample):\n",
    "    processed_text = preprocess_function(sample)\n",
    "    tokenized = tokenizer(\n",
    "        processed_text,\n",
    "        truncation=True,\n",
    "        max_length=PIPELINE_PARAMS['max_length'],\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # Set labels identical to input_ids\n",
    "    return tokenized\n",
    "\n",
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=QLORA_PARAMS['lora_r'],\n",
    "    lora_alpha=QLORA_PARAMS['lora_alpha'],\n",
    "    lora_dropout=QLORA_PARAMS['lora_dropout'],\n",
    "    target_modules=QLORA_PARAMS['lora_target_modules'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "# Apply LoRA configuration to the model\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.gradient_checkpointing_enable()\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Training sizes\n",
    "training_sizes = [16,32,64]\n",
    "\n",
    "# Loop through different training sizes\n",
    "for train_size in training_sizes:\n",
    "    # Split the dataset\n",
    "    train_data = training_data[:train_size]\n",
    "    eval_data = training_data[train_size:train_size + int(0.2 * train_size)]  # 20% of training data for evaluation\n",
    "\n",
    "    # Tokenize datasets\n",
    "    train_tokenized_data = [tokenize_function(sample) for sample in train_data]\n",
    "    eval_tokenized_data = [tokenize_function(sample) for sample in eval_data]\n",
    "\n",
    "    # Convert tokenized data to Dataset format\n",
    "    train_dataset = Dataset.from_dict({\n",
    "        \"input_ids\": [x[\"input_ids\"][0] for x in train_tokenized_data],\n",
    "        \"attention_mask\": [x[\"attention_mask\"][0] for x in train_tokenized_data],\n",
    "        \"labels\": [x[\"labels\"][0] for x in train_tokenized_data]\n",
    "    })\n",
    "    eval_dataset = Dataset.from_dict({\n",
    "        \"input_ids\": [x[\"input_ids\"][0] for x in eval_tokenized_data],\n",
    "        \"attention_mask\": [x[\"attention_mask\"][0] for x in eval_tokenized_data],\n",
    "        \"labels\": [x[\"labels\"][0] for x in eval_tokenized_data]\n",
    "    })\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"outputs/adapter_test_user_profile_epoch_{QLORA_PARAMS['lora_num_epochs']}_{train_size}_chatGPT_data_video_games\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=QLORA_PARAMS['gradient_accumulation_steps'],\n",
    "        num_train_epochs=QLORA_PARAMS['lora_num_epochs'],\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=QLORA_PARAMS['lora_val_iterations'],\n",
    "        save_steps=QLORA_PARAMS['lora_val_iterations'],\n",
    "        logging_steps=10,\n",
    "        learning_rate=QLORA_PARAMS['lora_lr'],\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=\"none\",\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, padding=True)\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    # Clear GPU cache before training\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Start training\n",
    "    print(f\"Starting training with {train_size} samples.\")\n",
    "    trainer.train()\n",
    "    adapter_name = f\"adapter_test_user_profile_epoch_{QLORA_PARAMS['lora_num_epochs']}_{train_size}_chatGPT_data_video_games\"\n",
    "    # Save the model and tokenizer in separate directories for each training size\n",
    "    model.save_pretrained(f\"outputs/{adapter_name}\")\n",
    "    tokenizer.save_pretrained(f\"outputs/{adapter_name}\")\n",
    "    print(f\"Model trained with {train_size} samples saved to outputs/{adapter_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\accelerate\\utils\\modeling.py:1390: UserWarning: Current model requires 1056.0 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[1;32m---> 18\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#quantization_config=bnb_config,\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#torch_dtype=torch.float16,\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Load the user profile adapter\u001b[39;00m\n\u001b[0;32m     26\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m  \u001b[38;5;66;03m# Adjust based on your adapter's training size\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\modeling_utils.py:4174\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4171\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[0;32m   4173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4174\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4176\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4177\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32mc:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:102\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 102\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    103\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m         )\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    115\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from config import *\n",
    "\n",
    "\n",
    "TOKENIZER_PATH = \"models/hf-frompretrained-download/meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# Load the base model and tokenizer\n",
    "base_model_path =\"models/hf-frompretrained-downloadmeta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=False)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    #quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    #torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Load the user profile adapter\n",
    "train_size = 64  # Adjust based on your adapter's training size\n",
    "adapter_path_user_profile =f\"outputs/adapter_test_user_profile_epoch_{QLORA_PARAMS['lora_num_epochs']}_{train_size}_samples\"\n",
    "adapter_name_user_profile = \"user_profile\"\n",
    "model = PeftModel.from_pretrained(model, adapter_path_user_profile, adapter_name=adapter_name_user_profile)\n",
    "\n",
    "print(f\"Loaded user profile adapter: {adapter_name_user_profile}\")\n",
    "\n",
    "# Load the candidate items adapter\n",
    "adapter_path_candidate_items = f\"outputs/adapter_test_candidate_items_epoch_{QLORA_PARAMS['lora_num_epochs']}_{64}_samples\"\n",
    "adapter_name_candidate_items = \"candidate_items\"\n",
    "model.load_adapter(adapter_path_candidate_items, adapter_name=adapter_name_candidate_items)\n",
    "\n",
    "print(f\"Loaded candidate items adapter: {adapter_name_candidate_items}\")\n",
    "\n",
    "# Print the list of adapters loaded into the model\n",
    "print(\"Adapters loaded in the model:\", list(model.peft_config.keys()))\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the text generation function\n",
    "def generate_text(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=PIPELINE_PARAMS['max_length'],\n",
    "        do_sample=True,\n",
    "        temperature=PIPELINE_PARAMS['temperature'],\n",
    "        top_k=PIPELINE_PARAMS['top_k'],\n",
    "        top_p=PIPELINE_PARAMS['top_p'],\n",
    "        repetition_penalty=PIPELINE_PARAMS['repetition_penalty'],\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Generate user profile\n",
    "user_reviews = \"\"\"User reviews:\n",
    "Product: Moisturizing Facial Emulsion for Restoring Hydrating Smoothing Skin from Manilla Natural Skincare\n",
    "Rating: 5.0\n",
    "Title: Intense Hydration\n",
    "Review: With winter approaching, it's time to start thinking about cold-weather skincare. This moisturizer is a good choice because it's a blend of several skin-loving oils like argan, coconut and macadamia. It's fairly lightweight but it protects and hydrates like a heavier cream. It can be used both day and night. The packaging is pretty enough for display. It can be used by all skin types. I highly recommend this product and the entire line from this brand.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
    "\n",
    "Product: Dr. Denese SkinScience Essential Lipid Anti Aging Power Infusion Dry Oil - Skin Nutrients 97% Organic 100% Natural - Rejuvinating Blend with Amaranth & Resveratrol - Paraben-Free, Cruelty-Free - 2oz\n",
    "Rating: 5.0\n",
    "Title: Youth Serum\n",
    "Review: One of the reasons skin starts to wrinkle as we age is that we make less and less of the natural oil that keeps skin looking youthful. This serum is a good way to supplement what your skin lacks. The oils in this blend are all high quality. The texture is lightweight and easily absorbed. Dr. Denese is a trusted brand when it comes to care for aging skin. Whether you're trying to slow the clock or reverse it a little, this serum should be part of your daily skincare routine. I hope the information that I've provided makes your decision a little easier.\n",
    "\n",
    "Product: [Abib] Creme coating mask Tone-up solution 17g (5pcs)\n",
    "Rating: 5.0\n",
    "Title: Doesn't Leave Skin Sticky\n",
    "Review: I use lots of sheet masks. Most leave your skin a little sticky when the serum dries. This mask works differently. The sheet locks the moisture in and doesn't leave a sticky residue. It's a great hydration and brightening mask. I highly recommend it.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
    "\n",
    "Product: Easydew 2-Step Face Contour Sheet Mask - Tightening & Anti-Aging & Hydrating Korean Face Mask with DW-EGF, Hyaluronic Acid for Anti-Winkle, Reducing Fine Line & Producing Collagen (Qty 5)\n",
    "Rating: 5.0\n",
    "Title: Special Care Mask\n",
    "Review: I really like that this mask includes special patches for the eyes and smile lines. If you're over 30, you'll appreciate the extra help. You apply the patches first, the the sheet mask. I keep all of my sheet masks in the refrigerator. You'll appreciate the cooling and anti-inflammatory effect if you do so with this mask. I highly recommend this product.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
    "\n",
    "Product: Higher Education Skincare: Goal Digger - Silky Moisturizing Cucumber Creme; formulated for dry and sensitive skin; cucumber extract; natural extract: melon, kale, cabbage, ginger, turmeric - 1.7 fl oz\n",
    "Rating: 5.0\n",
    "Title: Lightweight and Creamy\n",
    "Review: I really like this moisturizer. It's lightweight and creamy. It has a light cucumber scent that I find refreshing. It's a powerhouse at deeply hydrating parched skin. It also locks in moisture to keep skin looking fresh. It contains botanicals that soothe irritated or inflamed skin. It's ideal for those with acne, rosacea or eczema. You can use it as both a day and night cream. I highly recommend this product.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
    "\n",
    "Product: Queendom Unlashed Mascara | Volumizing and Lengthening | Boosts Lash Length | Vegan, Cruelty Free, Paraben Free\n",
    "Rating: 5.0\n",
    "Title: New Favorite!!\n",
    "Review: Every mascara claims to be new and different. This one actually lives up to the hype. The fibers have a novel shape that really makes your lashes POP. If you're not a fan of falsies, this is a great alternative. The formula isn't waterproof, so keep that in mind. I HIGHLY recommend this product.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
    "\n",
    "Product: ABOUT ME MediAnswer Collagen Firming Up Mask 4ea (1box) 25g - 77% Pure Collagen Extract Anti-Aging Total Care Facial Mask Sheet, Powerful Hydrating and Anti-Wrinkle Night Skin Care\n",
    "Rating: 5.0\n",
    "Title: Effective Mask\n",
    "Review: If you're reading this review, you probably already know the anti-aging benefits of collagen. This mask has a high-potency form of collagen along with a special delivery system to help firm your skin. The mask is in two parts, so you can get a better fit than a regular sheet mask. I can't say it performs miracles, but it's definitely worth a try.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
    "\"\"\"\n",
    "\n",
    "prompt_user_profile = (\n",
    "    ALPACA_LORA_PROMPTS_USER_PROFILE['instruction'] + \"\\n\\n\" +\n",
    "    ALPACA_LORA_PROMPTS_USER_PROFILE['input'].replace(\"{user_review}\", user_reviews)+ \"\\n### Response\"\n",
    ")\n",
    "\n",
    "# Set the active adapter to user profile adapter\n",
    "print(f\"\\nSetting active adapter to: {adapter_name_user_profile}\")\n",
    "model.set_adapter(adapter_name_user_profile)\n",
    "print(f\"Current active adapter: {model.active_adapter}\")\n",
    "\n",
    "generated_profile = generate_text(prompt_user_profile)\n",
    "generated_profile = generated_profile[len(prompt_user_profile):].strip()\n",
    "print(\"\\nGenerated User Profile:\")\n",
    "print(generated_profile)\n",
    "\n",
    "# Generate candidate items using the generated user profile\n",
    "prompt_candidate_items = (\n",
    "    ALPACA_LORA_PROMPTS_CANDIDATE_ITEMS['instruction'] + \"\\n\\n\" +\n",
    "    ALPACA_LORA_PROMPTS_CANDIDATE_ITEMS['input'].replace(\"{user_profile}\", generated_profile) + \"\\n### Response\"\n",
    ")\n",
    "\n",
    "# Set the active adapter to candidate items adapter\n",
    "print(f\"\\nSetting active adapter to: {adapter_name_candidate_items}\")\n",
    "model.set_adapter(adapter_name_candidate_items)\n",
    "print(f\"Current active adapter: {model.active_adapter}\")\n",
    "\n",
    "generated_items = generate_text(prompt_candidate_items)\n",
    "generated_items = generated_items[len(prompt_candidate_items):].strip()\n",
    "print(\"\\nGenerated Candidate Items:\")\n",
    "print(generated_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
