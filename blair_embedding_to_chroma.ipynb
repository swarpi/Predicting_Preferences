{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_reviews = pd.read_json(\"data\\All_beauty_more_than_3_with_product.jsonl\",lines = True)\n",
    "df_all_items_meta = pd.read_csv(\"data/meta_all_beauty_filtered_simple.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         B01CUPMQZE\n",
       "1         B076WQZGPM\n",
       "2         B000B658RI\n",
       "3         B088FKY3VD\n",
       "4         B07NGFDN6G\n",
       "             ...    \n",
       "112585    B077D2Z5RF\n",
       "112586    B07DLRYKQZ\n",
       "112587    B07HNP2NTF\n",
       "112588    B00U3OB8PY\n",
       "112589    B09B9TCYWG\n",
       "Name: parent_asin, Length: 112590, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_items_meta['parent_asin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def extract_ids_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads a text file, searches for the 'Key: smap' section,\n",
    "    extracts the dictionary that follows, and returns the list of IDs (keys).\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the text file.\n",
    "\n",
    "    Returns:\n",
    "    - ids (list): A list of IDs extracted from the dictionary.\n",
    "    \"\"\"\n",
    "    # Open the file and read all lines\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Initialize variables\n",
    "    start_reading = False\n",
    "    dict_lines = []\n",
    "\n",
    "    # Iterate over each line to find 'Key: smap' and collect the dictionary lines\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith('Key: smap'):\n",
    "            start_reading = True\n",
    "            continue  # Skip the line with 'Key: smap'\n",
    "        if start_reading:\n",
    "            if line == '':\n",
    "                break  # Stop reading when an empty line is encountered\n",
    "            dict_lines.append(line)\n",
    "\n",
    "    # Combine the collected lines into a single string\n",
    "    dict_str = ' '.join(dict_lines)\n",
    "    # Debug: Print the raw dictionary string\n",
    "    # print(\"Raw dictionary string:\")\n",
    "    # print(dict_str)\n",
    "\n",
    "    # Replace single quotes with double quotes to make it valid JSON\n",
    "    json_str = dict_str.replace(\"'\", '\"')\n",
    "\n",
    "    # Remove potential trailing commas in the dictionary\n",
    "    json_str = json_str.rstrip(',')\n",
    "\n",
    "    # Handle any potential issues with the dictionary string\n",
    "    try:\n",
    "        # Parse the dictionary string into a Python dictionary\n",
    "        smap_dict = ast.literal_eval(dict_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing dictionary with ast.literal_eval: {e}\")\n",
    "        print(\"Attempting to fix the dictionary string...\")\n",
    "\n",
    "        # Fix common issues in the dictionary string\n",
    "        # Replace single quotes with double quotes\n",
    "        dict_str_fixed = dict_str.replace(\"'\", '\"')\n",
    "        # Remove trailing commas\n",
    "        dict_str_fixed = dict_str_fixed.rstrip(',')\n",
    "        # Remove any potential invalid characters\n",
    "\n",
    "        try:\n",
    "            import json\n",
    "            smap_dict = json.loads(dict_str_fixed)\n",
    "        except Exception as e_json:\n",
    "            print(f\"Error parsing dictionary with json.loads: {e_json}\")\n",
    "            return []\n",
    "\n",
    "    # Extract the IDs (keys) from the dictionary\n",
    "    ids = list(smap_dict.keys())\n",
    "\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted IDs:\n",
      "1. B091GG4N2X\n",
      "2. B09KX5N1DR\n",
      "3. B07KXM94BT\n",
      "4. B08HVRP54L\n",
      "5. B086N2SY91\n",
      "6. B07DFNPVSF\n",
      "7. B08BZ1RHPS\n",
      "8. B086Q2J3F2\n",
      "9. B087PJ121M\n",
      "10. B0845LNDJK\n",
      "11. B09KYJJ3D5\n",
      "12. B009GPP546\n",
      "13. B0BVQQ58G8\n",
      "14. B08HQSK576\n",
      "15. B086ST4W1C\n",
      "16. B08HZ5Y8RY\n",
      "17. B0914JBBTV\n",
      "18. B081632HX6\n",
      "19. B076DBLG1K\n",
      "20. B09HMXY36F\n",
      "21. B0842BXM5M\n",
      "22. B08L3W279N\n",
      "23. B0837K9W6P\n",
      "24. B0998BD871\n",
      "25. B087G54FLM\n",
      "26. B08DHZW3LK\n",
      "27. B089CSF11Y\n",
      "28. B0B2L218H2\n",
      "29. B08TB3DZ1D\n",
      "30. B08S1LWF9V\n",
      "31. B0813ZQG3T\n",
      "32. B082DXHTX8\n",
      "33. B08BRS98SF\n",
      "34. B087GB8ZRF\n",
      "35. B08G5YVHQP\n",
      "36. B08HRHCZV2\n",
      "37. B07PHWX88W\n",
      "38. B08G4Y4SFV\n",
      "39. B085ZRVDKP\n",
      "40. B089NJG212\n",
      "41. B08NTD1NM1\n",
      "42. B08J7R5HHX\n",
      "43. B08PTZL95G\n",
      "44. B08QFGV5ZQ\n",
      "45. B08JPKXJMV\n",
      "46. B0851C4YPC\n",
      "47. B0BC273JJR\n",
      "48. B08BYK8SKR\n",
      "49. B08KT7FCYY\n",
      "50. B08W8LKLHB\n",
      "51. B084WP4XS8\n",
      "52. B08BY91SGT\n",
      "53. B08YNR839W\n",
      "54. B08R3F946X\n",
      "55. B08BKWLHYP\n",
      "56. B087Z9X39L\n",
      "57. B08S3B8Y5G\n",
      "58. B085RQKWNF\n",
      "59. B08CL46XNM\n",
      "60. B089D16BWJ\n",
      "61. B08L3J4FB9\n",
      "62. B08DKFV9SR\n",
      "63. B07HN1L8NQ\n",
      "64. B07J3GH1W1\n",
      "65. B08SQ31H8P\n",
      "66. B07THLR7RR\n",
      "67. B088TYPM71\n",
      "68. B089KBMST6\n",
      "69. B08G149DSD\n",
      "70. B07ZS3DKL5\n",
      "71. B08VDCWKHV\n",
      "72. B088GWCVVJ\n",
      "73. B08N6CLJ6P\n",
      "74. B07HVRP37G\n",
      "75. B088PYN4VM\n",
      "76. B096YHL82K\n",
      "77. B08BFH35VX\n",
      "78. B08WCQWP3T\n",
      "79. B0859CYSTM\n",
      "80. B08H4SYXR4\n",
      "81. B08TR1MKHD\n",
      "82. B07JGD2T2J\n",
      "83. B08FD2KP9R\n",
      "84. B086VYKNDF\n",
      "85. B08K8P7LN6\n",
      "86. B015A5DGG4\n",
      "87. B07KG1TWP5\n",
      "88. B081M5R5L5\n",
      "89. B09G22586Y\n",
      "90. B08CVTNQP1\n",
      "91. B0851T7HDS\n",
      "92. B07NPWK167\n",
      "93. B08BLKFGND\n",
      "94. B08T7GPT1D\n",
      "95. B086VK22QT\n",
      "96. B086M8MZGB\n",
      "97. B08DXZ5VXB\n",
      "98. B08RQZ3F3L\n",
      "99. B086YBQ8M7\n",
      "100. B08MRRNL18\n",
      "101. B0949MJRHK\n",
      "102. B07VGBBNTH\n",
      "103. B09LWV3HH2\n",
      "104. B086N136NH\n",
      "105. B07R3BMYBG\n",
      "106. B08C2T24WG\n",
      "107. B08LG68MGH\n",
      "108. B093H28PD4\n",
      "109. B082NKQ4ZT\n",
      "110. B07SLFWZKN\n",
      "111. B08HGZXLP6\n",
      "112. B09G3KBK7Y\n",
      "113. B08DNCR582\n",
      "114. B08P5579KC\n",
      "115. B07ZQRX7FX\n",
      "116. B08JTNQFZY\n",
      "117. B0844X21MJ\n",
      "118. B08JZD8HN4\n",
      "119. B086XH7BWW\n",
      "120. B08BLX72TP\n",
      "121. B00O2FGBJS\n",
      "122. B08F7877DC\n",
      "123. B08GKVYS1Y\n",
      "124. B07Z3NRMBS\n",
      "125. B0895FHBVG\n",
      "126. B0B4MR9BXN\n",
      "127. B0888CX245\n",
      "128. B096S4RQNH\n",
      "129. B07ND4HPHT\n",
      "130. B01MSF13T1\n",
      "131. B07PZ8JNWJ\n",
      "132. B08FBHNBP1\n",
      "133. B0BTJ6SYKB\n",
      "134. B088LWRYJC\n",
      "135. B08QVJ4NVD\n",
      "136. B0851QJPZY\n",
      "137. B092M5K59T\n",
      "138. B07T4LKC4D\n",
      "139. B08L6GXY2S\n",
      "140. B07RM722DH\n",
      "141. B07HR754WV\n",
      "142. B088LPYXNQ\n",
      "143. B07W1WJZFG\n",
      "144. B07L9H27SH\n",
      "145. B08879N2W3\n",
      "146. B08LZ6W8RF\n",
      "147. B08F79Z1Q5\n",
      "148. B08RYN11N9\n",
      "149. B07X8W7GJZ\n",
      "150. B08BXVJMRY\n",
      "151. B089QR2Y5B\n",
      "152. B08575Y9V3\n",
      "153. B08K2V6WGH\n",
      "154. B08KWN77LW\n",
      "155. B08SBV6Z57\n",
      "156. B08P7PSMRR\n",
      "157. B0895XPZNT\n",
      "158. B087D7MVHB\n",
      "159. B088X2WPTX\n",
      "160. B08HRNPNR5\n",
      "161. B085J354CL\n",
      "162. B086W18W1W\n",
      "163. B08HDG9F44\n",
      "164. B086MN4SRX\n",
      "165. B07TLMZL3T\n",
      "166. B08RRSPNWV\n",
      "167. B00TK0VV68\n",
      "168. B086TS3BKQ\n",
      "169. B088M5P8FN\n",
      "170. B0863FZFPV\n",
      "171. B08N9RT9YD\n",
      "172. B0977LVHF8\n",
      "173. B07P4LLZS6\n",
      "174. B0885W8GCL\n",
      "175. B07RBSLNFR\n",
      "176. B09FNTZFZZ\n",
      "177. B08LYT4Q2X\n",
      "178. B07W397QG4\n",
      "179. B087ZQG11L\n",
      "180. B086GST51S\n",
      "181. B08DK4NDM3\n",
      "182. B07XVNJFNF\n",
      "183. B08GC5GSNG\n",
      "184. B09671G4KH\n",
      "185. B07SV4HTMC\n",
      "186. B08KHRF9NY\n",
      "187. B099959RFF\n",
      "188. B08KY7VYDS\n",
      "189. B08RY6S25W\n",
      "190. B08LPJT4MT\n",
      "191. B08DXCVRNY\n",
      "192. B083B67373\n",
      "193. B08SJKR877\n",
      "194. B07ZJX5MNJ\n",
      "195. B08BY9P4HR\n",
      "196. B082VKPJV5\n",
      "197. B08VKQY815\n",
      "198. B095C1WF44\n",
      "199. B08P15TB2V\n",
      "200. B08NPBQR9L\n",
      "201. B08PVH18Z6\n",
      "202. B083PXJBVY\n",
      "203. B0841WQNNZ\n",
      "204. B085RS26YH\n",
      "205. B08343FS9P\n",
      "206. B087F14JSH\n",
      "207. B085NYYLQ8\n",
      "208. B08XQYZWCS\n",
      "209. B08SG2MBRY\n",
      "210. B083BGJ4P9\n",
      "211. B082MDFNZM\n",
      "212. B09571TNVG\n",
      "213. B07SW7D6ZR\n",
      "214. B07Z4CVTRP\n",
      "215. B08Q3F3L7S\n",
      "216. B08XZDXTG2\n",
      "217. B083J1Y349\n",
      "218. B07M9D3WYW\n",
      "219. B08HMLXW65\n",
      "220. B08C71WBLC\n",
      "221. B09JLSCTR3\n",
      "222. B086JN17SK\n",
      "223. B08D6PPL42\n",
      "224. B08693T3XR\n",
      "225. B07DKRK8ZW\n",
      "226. B08B3SJNL5\n",
      "227. B07YL4485K\n",
      "228. B0713VF172\n",
      "229. B09NFQ69KT\n",
      "230. B07YS9W97B\n",
      "231. B08C72C56Z\n",
      "232. B08W4WQMNM\n",
      "233. B087ZQK2G8\n",
      "234. B088ZVSLYP\n",
      "235. B081D2R47W\n",
      "236. B08G83G24X\n",
      "237. B085TFXLH1\n",
      "238. B086SSHLWT\n",
      "239. B087FTZSZH\n",
      "240. B085J24382\n",
      "241. B0BG6WX4DH\n",
      "242. B08MC3ZLV4\n",
      "243. B083TLNBJJ\n",
      "244. B09C5NQSC5\n",
      "245. B08MT8L5ZL\n",
      "246. B08QHP717Z\n",
      "247. B084ZHP45Y\n",
      "248. B07Z548TKH\n",
      "249. B08BB3P4VQ\n",
      "250. B082VYX7G8\n",
      "251. B07LCHCD6Q\n",
      "252. B08GMC48QF\n",
      "253. B08DRBZNZJ\n",
      "254. B08QKXY1KN\n",
      "255. B08GYJY8F2\n",
      "256. B095CG2ZV1\n",
      "257. B08BS3WDPJ\n",
      "258. B08DK74M1P\n",
      "259. B08N6YHQXT\n",
      "260. B07VG2W8TT\n",
      "261. B087GD4BJ1\n",
      "262. B08CC6YRLJ\n",
      "263. B0994HWXVV\n",
      "264. B088YHN4JT\n",
      "265. B08DXLRTSB\n",
      "266. B07VDCD17L\n",
      "267. B08B5FJMHM\n",
      "268. B0941YDPSW\n",
      "269. B085SY4WC3\n",
      "270. B07T3Z58HL\n",
      "271. B07MZT83KK\n",
      "272. B08L4HTQ3R\n",
      "273. B088TGQFNM\n",
      "274. B089RLLT5C\n",
      "275. B07VQR3W3Z\n",
      "276. B07SY7WPS4\n",
      "277. B08PLFZB89\n",
      "278. B07PBWVV5K\n",
      "279. B08K2WH8LK\n",
      "280. B07D5FBFQ4\n",
      "281. B088TXBCB5\n",
      "282. B08GS1G9BH\n",
      "283. B07WNBZQGT\n",
      "284. B081ZN3TD5\n",
      "285. B01MA3LXIL\n",
      "286. B08VQY1Z1S\n",
      "287. B08F4ZDVZQ\n",
      "288. B005AL5H9S\n",
      "289. B0855L611L\n",
      "290. B08NC4378M\n",
      "291. B07MN1KJ15\n",
      "292. B08LPGZMQK\n",
      "293. B07MMW5654\n",
      "294. B085MCMZLX\n",
      "295. B09BKK8G76\n",
      "296. B08VJ7CZW3\n",
      "297. B01N7A5AGF\n",
      "298. B09GNXK3N1\n",
      "299. B08BYKQS9S\n",
      "300. B08S2LYN64\n",
      "301. B08C24Q6LB\n",
      "302. B08CVCLVS2\n",
      "303. B01EK2F1PC\n",
      "304. B07YV9J2YZ\n",
      "305. B08DK5D9J5\n",
      "306. B07YNDWRCB\n",
      "307. B089R7S73D\n",
      "308. B08N5NDVGH\n",
      "309. B08WF29DM9\n",
      "310. B08HXQ3T9K\n",
      "311. B08HCMNLKD\n",
      "312. B08CZ5JZMZ\n",
      "313. B07YG4PRTN\n",
      "314. B08SQZ1B3W\n",
      "315. B086VRRD99\n",
      "316. B09BJM95J7\n",
      "317. B08YJV9YL2\n",
      "318. B09473GGM4\n",
      "319. B0876VC4KM\n",
      "320. B01M24DUXH\n",
      "321. B0B8SFDJN9\n",
      "322. B08JCX3DL7\n",
      "323. B08KJF6Y6C\n",
      "324. B08BF4BKKM\n",
      "325. B09TTC9Q9W\n",
      "326. B08KGVBW41\n",
      "327. B087J3H22J\n",
      "328. B08B3TCMSQ\n",
      "329. B07PCSRSND\n",
      "330. B09GVHT2D3\n",
      "331. B09L33VJCL\n",
      "332. B08P4YQ3K5\n",
      "333. B088NVJSJT\n",
      "334. B08CH5PC9W\n",
      "335. B088FBNQXW\n",
      "336. B07J1LYVHC\n",
      "337. B09SH1FD9S\n",
      "338. B089YGZZYP\n",
      "339. B0912BNP4J\n",
      "340. B0919R1CYT\n",
      "341. B089ZQ8Y95\n",
      "342. B08P2DZB4X\n",
      "343. B0932Z1NM1\n",
      "344. B08C37KWRR\n",
      "345. B083122WRC\n",
      "346. B08XJWLLKQ\n",
      "347. B08NJ5BTWG\n",
      "348. B085QDWHQJ\n",
      "349. B07PCGSFGQ\n",
      "350. B089CSR3KF\n",
      "351. B07FPS2VFK\n",
      "352. B081MQD52C\n",
      "353. B07FP2C8N8\n",
      "354. B07NPCT6L5\n",
      "355. B08GJJ5RV9\n",
      "356. B08MPK4JRB\n"
     ]
    }
   ],
   "source": [
    "file_path = 'data/dataset.txt'\n",
    "\n",
    "# Call the function to extract IDs\n",
    "ids = extract_ids_from_file(file_path)\n",
    "\n",
    "# Print the extracted IDs\n",
    "print(\"Extracted IDs:\")\n",
    "for idx, id_value in enumerate(ids, 1):\n",
    "    print(f\"{idx}. {id_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>details</th>\n",
       "      <th>parent_asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>Organic Sweet Almond Oil and Fractionated Coco...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Brand': 'Shiny Leaf', 'Scent': 'Almond Oil a...</td>\n",
       "      <td>B08LYT4Q2X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>Empty Brown Glass Spray Bottles2-Pack, Refilla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Brand': 'Cherry', 'Material': 'Glass', 'Capa...</td>\n",
       "      <td>B089CSF11Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109</td>\n",
       "      <td>JPNK 4PCS Anti-Static Detangling Fine &amp; Wide T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Material': 'Plastic', 'Brand': 'JPNK', 'Hair...</td>\n",
       "      <td>B081ZN3TD5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>268</td>\n",
       "      <td>TULA Probiotic Skin Care Supersize 24-7 Moistu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Brand': 'TULA', 'Scent': 'Watermelon', 'Item...</td>\n",
       "      <td>B08WF29DM9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>421</td>\n",
       "      <td>Claw Hair Clips, IKOCO 6 Pack Jaw Clips Stylis...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Brand': 'IKOCO', 'Hair Type': 'Fine', 'Color...</td>\n",
       "      <td>B08HMLXW65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>100151</td>\n",
       "      <td>Facial Skincare,Cucumber Essence Hydrating Moi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>B08GJJ5RV9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>101077</td>\n",
       "      <td>4D Silk Fiber Lash Mascara Black, Natural Wate...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Color': 'Black', 'Special Feature': 'Smudge ...</td>\n",
       "      <td>B086ST4W1C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>101177</td>\n",
       "      <td>Dr. Denese SkinScience Essential Lipid Anti Ag...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.99</td>\n",
       "      <td>{'Brand': 'Dr. Denese', 'Item Form': 'Dr. Dene...</td>\n",
       "      <td>B07DFNPVSF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>101459</td>\n",
       "      <td>KISSIO Eyeshadow Set Of 9 Colors, Practical Co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Brand': 'KISSIO', 'Item Form': 'Powder', 'Fi...</td>\n",
       "      <td>B08F7877DC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>101741</td>\n",
       "      <td>Magnetic Eyelashes and Magnetic Eyeliner Kit, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Brand': 'Eyenjoy', 'Item Dimensions LxWxH': ...</td>\n",
       "      <td>B087GD4BJ1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>356 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                              title description  \\\n",
       "0        44  Organic Sweet Almond Oil and Fractionated Coco...         NaN   \n",
       "1        45  Empty Brown Glass Spray Bottles2-Pack, Refilla...         NaN   \n",
       "2       109  JPNK 4PCS Anti-Static Detangling Fine & Wide T...         NaN   \n",
       "3       268  TULA Probiotic Skin Care Supersize 24-7 Moistu...         NaN   \n",
       "4       421  Claw Hair Clips, IKOCO 6 Pack Jaw Clips Stylis...         NaN   \n",
       "..      ...                                                ...         ...   \n",
       "351  100151  Facial Skincare,Cucumber Essence Hydrating Moi...         NaN   \n",
       "352  101077  4D Silk Fiber Lash Mascara Black, Natural Wate...         NaN   \n",
       "353  101177  Dr. Denese SkinScience Essential Lipid Anti Ag...         NaN   \n",
       "354  101459  KISSIO Eyeshadow Set Of 9 Colors, Practical Co...         NaN   \n",
       "355  101741  Magnetic Eyelashes and Magnetic Eyeliner Kit, ...         NaN   \n",
       "\n",
       "     price                                            details parent_asin  \n",
       "0      NaN  {'Brand': 'Shiny Leaf', 'Scent': 'Almond Oil a...  B08LYT4Q2X  \n",
       "1      NaN  {'Brand': 'Cherry', 'Material': 'Glass', 'Capa...  B089CSF11Y  \n",
       "2      NaN  {'Material': 'Plastic', 'Brand': 'JPNK', 'Hair...  B081ZN3TD5  \n",
       "3      NaN  {'Brand': 'TULA', 'Scent': 'Watermelon', 'Item...  B08WF29DM9  \n",
       "4      NaN  {'Brand': 'IKOCO', 'Hair Type': 'Fine', 'Color...  B08HMLXW65  \n",
       "..     ...                                                ...         ...  \n",
       "351    NaN                                                 {}  B08GJJ5RV9  \n",
       "352    NaN  {'Color': 'Black', 'Special Feature': 'Smudge ...  B086ST4W1C  \n",
       "353  32.99  {'Brand': 'Dr. Denese', 'Item Form': 'Dr. Dene...  B07DFNPVSF  \n",
       "354    NaN  {'Brand': 'KISSIO', 'Item Form': 'Powder', 'Fi...  B08F7877DC  \n",
       "355    NaN  {'Brand': 'Eyenjoy', 'Item Dimensions LxWxH': ...  B087GD4BJ1  \n",
       "\n",
       "[356 rows x 6 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filt_ids = df_all_items_meta['parent_asin'].isin(ids)\n",
    "filtered_df_all_items_meta = df_all_items_meta[filt_ids]\n",
    "filtered_df_all_items_meta.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_all_items_meta.to_csv(\"data/filterd_meta_items.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\master_torch_gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:__main__:Using device: cuda\n",
      "INFO:__main__:Model loaded and moved to device.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pymongo\n",
    "from tqdm import tqdm  # Optional: for progress bar\n",
    "import logging\n",
    "import uuid\n",
    "\n",
    "# Set up logging for debugging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check if GPU is available and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize MongoDB client\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client['product_embeddings_filtered']  # Replace with your database name\n",
    "collection_mongo = db['embeddings']      # Replace with your collection name\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hyp1231/blair-roberta-large\")\n",
    "model = AutoModel.from_pretrained(\"hyp1231/blair-roberta-large\")\n",
    "model.to(device)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "logger.info(\"Model loaded and moved to device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_store(chunk, chunk_number, batch_size=32):\n",
    "    logger.info(f\"Processing chunk {chunk_number}\")\n",
    "    # Preprocess texts and metadata\n",
    "    texts = []\n",
    "    metadata = []\n",
    "    for index, row in chunk.iterrows():\n",
    "        # Safely handle missing values\n",
    "        title = str(row['title']) if pd.notnull(row['title']) else ''\n",
    "        description = str(row['description']) if pd.notnull(row['description']) else ''\n",
    "        details = str(row['details']) if pd.notnull(row['details']) else ''\n",
    "        parent_asin = str(row['parent_asin']) if pd.notnull(row['parent_asin']) else ''\n",
    "        \n",
    "        # Combine text fields\n",
    "        combined_text = '. '.join(filter(None, [title, description, details]))\n",
    "        texts.append(combined_text)\n",
    "        metadata.append(parent_asin)\n",
    "    \n",
    "    if not texts:\n",
    "        logger.warning(f\"No texts to process in chunk {chunk_number}. Skipping.\")\n",
    "        return  # Skip if no texts to process\n",
    "\n",
    "    total_texts = len(texts)\n",
    "    logger.info(f\"Total texts in chunk {chunk_number}: {total_texts}\")\n",
    "\n",
    "    # Process in batches\n",
    "    for i in range(0, total_texts, batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_metadata = metadata[i:i+batch_size]\n",
    "\n",
    "        # Tokenize batch texts\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}  # Move inputs to device\n",
    "\n",
    "        # Compute embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, return_dict=True)\n",
    "            embeddings = outputs.last_hidden_state[:, 0]  # CLS token\n",
    "            embeddings = embeddings / embeddings.norm(dim=1, keepdim=True)  # Normalize embeddings\n",
    "        \n",
    "        # Move embeddings to CPU and convert to lists\n",
    "        embeddings_list = embeddings.cpu().numpy().tolist()\n",
    "\n",
    "        # Prepare documents for MongoDB\n",
    "        documents = []\n",
    "        for j in range(len(batch_texts)):\n",
    "            doc = {\n",
    "                \"embedding\": embeddings_list[j],\n",
    "                \"metadata\": batch_metadata[j]\n",
    "            }\n",
    "            documents.append(doc)\n",
    "        \n",
    "        # Insert documents into MongoDB\n",
    "        if documents:\n",
    "            try:\n",
    "                collection_mongo.insert_many(documents)\n",
    "                logger.info(f\"Inserted batch {i//batch_size + 1} of chunk {chunk_number} into MongoDB.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error inserting documents from batch {i//batch_size + 1} of chunk {chunk_number}: {e}\")\n",
    "        else:\n",
    "            logger.warning(f\"No documents to insert for batch {i//batch_size + 1} of chunk {chunk_number}.\")\n",
    "\n",
    "        # Clear cache to free up memory\n",
    "        del inputs, outputs, embeddings\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    logger.info(f\"Finished processing chunk {chunk_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]INFO:__main__:Processing chunk 1\n",
      "INFO:__main__:Total texts in chunk 1: 356\n",
      "INFO:__main__:Inserted batch 1 of chunk 1 into MongoDB.\n",
      "INFO:__main__:Inserted batch 2 of chunk 1 into MongoDB.\n",
      "INFO:__main__:Inserted batch 3 of chunk 1 into MongoDB.\n",
      "INFO:__main__:Inserted batch 4 of chunk 1 into MongoDB.\n",
      "INFO:__main__:Inserted batch 5 of chunk 1 into MongoDB.\n",
      "INFO:__main__:Inserted batch 6 of chunk 1 into MongoDB.\n",
      "INFO:__main__:Inserted batch 7 of chunk 1 into MongoDB.\n",
      "INFO:__main__:Inserted batch 8 of chunk 1 into MongoDB.\n",
      "INFO:__main__:Inserted batch 9 of chunk 1 into MongoDB.\n",
      "INFO:__main__:Inserted batch 10 of chunk 1 into MongoDB.\n",
      "INFO:__main__:Inserted batch 11 of chunk 1 into MongoDB.\n",
      "INFO:__main__:Inserted batch 12 of chunk 1 into MongoDB.\n",
      "INFO:__main__:Finished processing chunk 1\n",
      "1it [00:04,  4.65s/it]\n"
     ]
    }
   ],
   "source": [
    "# Read CSV in chunks\n",
    "chunksize = 1000\n",
    "csv_file = 'data/filterd_meta_items.csv'  # Replace with your CSV file path\n",
    "\n",
    "# Use tqdm for a progress bar (optional)\n",
    "for chunk_number, chunk in enumerate(tqdm(pd.read_csv(csv_file, chunksize=chunksize)), start=1):\n",
    "    process_and_store(chunk, chunk_number, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:__main__:ChromaDB client initialized.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pymongo\n",
    "from tqdm import tqdm  # Optional: for progress bar\n",
    "import logging\n",
    "import uuid\n",
    "import chromadb\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "import pandas as pd\n",
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db_mpnet\")\n",
    "collection_name = 'product_embeddings_filtered'\n",
    "\n",
    "if collection_name in chroma_client.list_collections():\n",
    "    collection_chroma = chroma_client.get_collection(name=collection_name)\n",
    "else:\n",
    "    collection_chroma = chroma_client.create_collection(name=collection_name)\n",
    "logger.info(\"ChromaDB client initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Reading CSV file to build parent_asin to text mapping...\n",
      "INFO:__main__:Completed building parent_asin to text mapping.\n"
     ]
    }
   ],
   "source": [
    "asin_to_text = {}\n",
    "\n",
    "logger.info(\"Reading CSV file to build parent_asin to text mapping...\")\n",
    "df = pd.read_csv(csv_file)\n",
    "for index, row in df.iterrows():\n",
    "    title = str(row['title']) if pd.notnull(row['title']) else ''\n",
    "    description = str(row['description']) if pd.notnull(row['description']) else ''\n",
    "    details = str(row['details']) if pd.notnull(row['details']) else ''\n",
    "    parent_asin = str(row['parent_asin']) if pd.notnull(row['parent_asin']) else ''\n",
    "    \n",
    "    combined_text = '. '.join(filter(None, [title, description, details]))\n",
    "    asin_to_text[parent_asin] = combined_text\n",
    "\n",
    "logger.info(\"Completed building parent_asin to text mapping.\")\n",
    "\n",
    "# Function to read from MongoDB and add to ChromaDB with documents\n",
    "def transfer_embeddings_with_documents(batch_size=1000):\n",
    "    total_documents = collection_mongo.count_documents({})\n",
    "    logger.info(f\"Total documents in MongoDB: {total_documents}\")\n",
    "\n",
    "    # Initialize cursor\n",
    "    cursor = collection_mongo.find({})\n",
    "\n",
    "    batch_ids = []\n",
    "    batch_embeddings = []\n",
    "    batch_documents = []\n",
    "    batch_metadatas = []\n",
    "\n",
    "    count = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    for document in tqdm(cursor, total=total_documents):\n",
    "        count += 1\n",
    "        batch_count += 1\n",
    "\n",
    "        # Extract data from MongoDB document\n",
    "        embedding = document.get('embedding')\n",
    "        metadata = document.get('metadata')  # This should be 'parent_asin'\n",
    "        # Retrieve the combined text using the parent_asin\n",
    "        text = asin_to_text.get(metadata, None)\n",
    "\n",
    "        # Generate a unique ID for the document (or use an existing one if available)\n",
    "        doc_id = str(document.get('_id', uuid.uuid4()))\n",
    "\n",
    "        batch_ids.append(doc_id)\n",
    "        batch_embeddings.append(embedding)\n",
    "        batch_documents.append(text)\n",
    "        batch_metadatas.append({'metadata': metadata})\n",
    "\n",
    "        # When batch is full or last document, add to ChromaDB\n",
    "        if batch_count >= batch_size or count == total_documents:\n",
    "            # Remove documents with None embeddings (if any)\n",
    "            valid_indices = [i for i, emb in enumerate(batch_embeddings) if emb is not None]\n",
    "            if valid_indices:\n",
    "                valid_ids = [batch_ids[i] for i in valid_indices]\n",
    "                valid_embeddings = [batch_embeddings[i] for i in valid_indices]\n",
    "                valid_documents = [batch_documents[i] for i in valid_indices]\n",
    "                valid_metadatas = [batch_metadatas[i] for i in valid_indices]\n",
    "\n",
    "                # Add to ChromaDB\n",
    "                try:\n",
    "                    collection_chroma.add(\n",
    "                        ids=valid_ids,\n",
    "                        documents=valid_documents,\n",
    "                        embeddings=valid_embeddings,\n",
    "                        metadatas=valid_metadatas\n",
    "                    )\n",
    "                    logger.info(f\"Inserted batch of {len(valid_ids)} documents into ChromaDB.\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error inserting batch into ChromaDB: {e}\")\n",
    "            else:\n",
    "                logger.warning(\"No valid embeddings found in batch.\")\n",
    "\n",
    "            # Reset batch\n",
    "            batch_ids = []\n",
    "            batch_embeddings = []\n",
    "            batch_documents = []\n",
    "            batch_metadatas = []\n",
    "            batch_count = 0\n",
    "\n",
    "    logger.info(\"Completed transferring embeddings with documents to ChromaDB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Total documents in MongoDB: 356\n",
      "  0%|          | 0/356 [00:00<?, ?it/s]INFO:__main__:Inserted batch of 356 documents into ChromaDB.\n",
      "100%|██████████| 356/356 [00:01<00:00, 230.08it/s]\n",
      "INFO:__main__:Completed transferring embeddings with documents to ChromaDB.\n"
     ]
    }
   ],
   "source": [
    "transfer_embeddings_with_documents(batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total embeddings in collection: 356\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "def initialize_chromadb(db_path):\n",
    "    \"\"\"Initialize the ChromaDB client.\"\"\"\n",
    "    db = chromadb.PersistentClient(path=db_path)\n",
    "    return db\n",
    "\n",
    "db_path = \"./chroma_db_mpnet\"\n",
    "db = initialize_chromadb(db_path)\n",
    "collection_name = 'product_embeddings_filtered'  # Use the same collection name\n",
    "\n",
    "\n",
    "collection = db.get_collection(name=collection_name)\n",
    "count = collection.count()\n",
    "print(f\"Total embeddings in collection: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
