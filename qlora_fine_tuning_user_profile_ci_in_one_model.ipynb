{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import necessary modules and load configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Loaded:\n",
      "Tokenizer Path: models/hf-frompretrained-download/meta-llama/Meta-Llama-3-8B-Instruct\n",
      "Model Path: models/hf-frompretrained-downloadmeta-llama/Meta-Llama-3-8B-Instruct\n",
      "Pipeline Parameters: {'max_length': 2048, 'num_return_sequences': 1, 'temperature': 0.7, 'top_k': 50, 'top_p': 0.95, 'repetition_penalty': 1.2}\n",
      "QLoRA Parameters: {'lora_r': 8, 'lora_alpha': 8, 'lora_dropout': 0.01, 'lora_target_modules': ['q_proj', 'v_proj'], 'gradient_accumulation_steps': 2, 'lora_num_epochs': 2, 'lora_val_iterations': 100, 'lora_early_stopping_patience': 10, 'lora_lr': 0.0001, 'lora_micro_batch_size': 1}\n",
      "Prompt Template: {'instruction': \"### Instruction:\\n You are a recommender system specialized in creating user profiles. Your task is to create a comprehensive user profile and a list of candidate items based on a user's review. The user Profile consists of: 1. Short-term Interests: Examine the user's most recent items along with their personalized descriptions 2. Long-term Preferences: Analyze all reviews and items from the user’s entire history to capture deeper, stable interests that define the user’s lasting preferences. Use this comprehensive historical data to outline consistent themes and inclinations that have remained steady over time. 3. User Profile Summary: Synthesize these findings into a concise profile (max 200 words) that combines insights from both short-term interests and long-term preferences. Use the short-term interests as a query to refine or highlight relevant themes from the long-term history, and provide a cohesive picture of the user’s tastes, typical habits, and potential future interests.Present your analysis under clear section headings.Do Not include any Code in your response. Think step by step\", 'input': '{user_review}', 'output': '### Response:'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "\n",
    "# Set environment variable for memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Load configuration settings\n",
    "from config import TOKENIZER_PATH, MODEL_PATH, PIPELINE_PARAMS, QLORA_PARAMS,ALPACA_LORA_PROMPTS_USER_PROFILE\n",
    "\n",
    "# Verification\n",
    "print(\"Configuration Loaded:\")\n",
    "print(\"Tokenizer Path:\", TOKENIZER_PATH)\n",
    "print(\"Model Path:\", MODEL_PATH)\n",
    "print(\"Pipeline Parameters:\", PIPELINE_PARAMS)\n",
    "print(\"QLoRA Parameters:\", QLORA_PARAMS)\n",
    "print(\"Prompt Template:\", ALPACA_LORA_PROMPTS_USER_PROFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and verify training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Sample: [{'User_ID': 'AEE4M36AZAKURLEYGV23TM3BE7OQ', 'User_Profile': '\"Short-Term Interests\": The user has recently engaged with hair care products focused on moisturizing and detangling, such as shampoos and conditioners with natural oils, hair towels, and convenient towelettes.\\n\"Long-Term Preferences\": An analysis of the user\\'s reviews reveals consistent themes:\\n* Preference for moisturizing hair products that leave fine, shoulder-length hair shiny and bouncy without greasiness\\n* Interest in products that help with detangling, reducing frizz, and minimizing static electricity\\n* Appreciation for high-quality hair accessories like twist towels and soft hair towels\\n* Values convenience items for on-the-go use, such as individually packaged towelettes\\n* Preference for natural ingredients like coconut oil and avocado oil in hair care\\n\"User_Profile\": The user appears to prioritize hair care and values products that enhance the health and appearance of their fine, shoulder-length hair. They seek moisturizing shampoos and conditioners that provide shine and manageability without weighing hair down. The user appreciates effective detangling solutions and products that combat frizz and static. They value quality hair accessories and convenient personal care items, showing a preference for natural ingredients in their hair care routine.', 'Candidate_Items': {'1': 'Anti-Frizz Hair Serums', '2': 'Leave-In Conditioners with Natural Oils', '3': 'Detangling Hair Brushes', '4': 'Travel-Friendly Dry Shampoos', '5': 'Silk Pillowcases for Hair Care'}}, {'User_ID': 'AEFRTLVCVRALKXBED77KHPIXEPWQ', 'User_Profile': '\"Short-Term Interests\": The user has recently engaged with skincare products suitable for sensitive skin, including face masks, headbands designed to relieve mask pressure, and lip balms with sun protection.\\n\"Long-Term Preferences\": An analysis of the user\\'s reviews reveals consistent themes:\\n* Interest in skincare products that soothe and hydrate sensitive skin without causing irritation\\n* Preference for products that provide noticeable results, such as firmer or more hydrated skin\\n* Appreciation for convenience and comfort in products, like headbands with buttons for mask straps\\n* Values sun protection in lip care products without adverse reactions\\n* Open to trying new skincare treatments, even if instructions are not in their native language\\n\"User_Profile\": The user is focused on skincare solutions that cater to sensitive skin, seeking products that offer hydration, soothing effects, and anti-aging benefits without irritation. They value practicality and comfort in accessories that enhance their daily routines. The user is health-conscious, preferring products with sun protection, and appreciates items that deliver visible results.', 'Candidate_Items': {'1': 'Hypoallergenic Mascara for Sensitive Eyes', '2': 'SPF Lip Balms for Sensitive Lips', '3': 'Cooling Gel Eye Masks', '4': 'Gentle Facial Cleansers for Sensitive Skin', '5': 'Soft Makeup Brushes for Delicate Skin'}}]\n",
      "Data Structure Verification:\n",
      "Data verification successful!\n"
     ]
    }
   ],
   "source": [
    "data_path = \"QLoRa_finetuning/matching_ids_chatGPT.json\"\n",
    "\n",
    "# Load the training data\n",
    "with open(data_path, \"r\") as file:\n",
    "    training_data = json.load(file)\n",
    "\n",
    "# Sample a couple of data points to verify format\n",
    "print(\"Training Data Sample:\", training_data[:2])  # Display first two entries\n",
    "# Clear cache before loading model\n",
    "torch.cuda.empty_cache()\n",
    "# Verify data structure\n",
    "print(\"Data Structure Verification:\")\n",
    "for i, sample in enumerate(training_data[:2]):\n",
    "    assert \"User_ID\" in sample, f\"User_ID missing in sample {i}\"\n",
    "    assert \"User_Profile\" in sample, f\"User_Profile missing in sample {i}\"\n",
    "    assert \"Candidate_Items\" in sample, f\"Candidate_Items missing in sample {i}\"\n",
    "print(\"Data verification successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize the Tokenizer and Model with Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\quantizers\\auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.09s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS if not already set\n",
    "\n",
    "# Set 4-bit quantization configuration for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',  # Use NormalFloat4 for better memory efficiency\n",
    "    bnb_4bit_use_double_quant=True  # Double quantization for more memory saving\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"  # Automatically maps layers to available GPU memory\n",
    ")\n",
    "# Load user reviews\n",
    "reviews_path = \"new_data/new_train_output.json\"\n",
    "with open(reviews_path, \"r\") as file:\n",
    "    reviews_data = json.load(file)\n",
    "\n",
    "# Index reviews by user_id for easy matching\n",
    "reviews_by_user = {entry[\"user_id\"]: entry[\"reviews\"] for entry in reviews_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Preprocessing Function to Match Reviews with Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Sample: \n",
      "### Instruction:\n",
      " You are a recommender system specialized in creating user profiles. Your task is to create a comprehensive user profile and a list of candidate items based on a user's review. The user Profile consists of: 1. Short-term Interests: Examine the user's most recent items along with their personalized descriptions 2. Long-term Preferences: Analyze all reviews and items from the user’s entire history to capture deeper, stable interests that define the user’s lasting preferences. Use this comprehensive historical data to outline consistent themes and inclinations that have remained steady over time. 3. User Profile Summary: Synthesize these findings into a concise profile (max 200 words) that combines insights from both short-term interests and long-term preferences. Use the short-term interests as a query to refine or highlight relevant themes from the long-term history, and provide a cohesive picture of the user’s tastes, typical habits, and potential future interests.Present your analysis under clear section headings.Do Not include any Code in your response. Think step by step\n",
      "\n",
      "### Input:\n",
      "User reviews:\n",
      "Product: Lemon Oil Towelettes - 20 Count\n",
      "Rating: 4.0\n",
      "Title: Very Convenient\n",
      "Review: These towelettes are great for keeping in your car's glove compartment, putting inside your tote bag, or stashing away in your office desk. They're very handy for times when you can't get to a sink and use soap and water. They also smell great. They're a little pricey for everyday use, so I took off one star for that.\n",
      "\n",
      "Product: John Frieda Detox and Repair Shampoo and Conditioner Set with Nourishing Avocado Oil and Green Tea, 8.45 Fl Oz (Pack of 2)\n",
      "Rating: 4.0\n",
      "Title: Good Set\n",
      "Review: Avocado oil seems to be good at moisturizing without being greasy. I have some body lotion with avocado oil and really like that, so I decided to try this shampoo/conditioner set. It does a very good job of cleaning your hair and leaving it shiny and bouncy. I still prefer coconut oil for hair, but avocado oil is doing a good job, too.\n",
      "\n",
      "Product: Microfiber Hair Towels - 5 Towel Set – 5 Beautiful Colors - Great for all Hair Types - Size (10 x 26 inches) Lightweight, Absorbent, and Ultra Soft. Colors Rose, Pink, Gray, Blue, and Lilac.\n",
      "Rating: 4.0\n",
      "Title: Nice Towels\n",
      "Review: These hair towels are very soft and comfortable. This set is a good value because you get 5 of them. They're well-made and should last a long time.\n",
      "\n",
      "Product: Maya Mari -  Castor Oil Shampoo for Women, Strengthening and Moisturizing Shampoo for Coarse, Textured, and Curly Hair, 32 oz\n",
      "Rating: 4.0\n",
      "Title: Very Moisturizing\n",
      "Review: This shampoo does an excellent job of moisturizing (and cleaning) your hair. However, you don't want to use it more than 3-4 times a week, because the castor oil builds up and starts making your hair look greasy. I alternate it with another shampoo, and it works great that way.\n",
      "\n",
      "Product: 2 Pack Hair Towel Wrap Turban Microfiber Drying Shower and Bath, Twist Head Towels with Button, Magic Quick Dryer, Hair Care for Women and Girls, One Wrap Plus One Cap and Two Velvet Hair Bands\n",
      "Rating: 4.0\n",
      "Title: Okay\n",
      "Review: I like the twist towel a lot, but I found the shower-cap-like towel to be a little hard to use. My hair is below shoulder-length, and it just didn't work well for me. The twist towel is great, though. I didn't care for the hair ties and won't be using those. If you have longer hair and just want some twist towels, I would go with a different product because the cap probably won't work for you. But if you're hair is shorter, this set would be a pretty good choice.\n",
      "\n",
      "Product: Taya Beauty Buriti Nut Intensive Repair Shampoo & Conditioner Duo – Organic Hydrating Shampoo and Conditioner for Dry Damaged Hair - Restorative Hair Care – Travel Size 2 fl oz\n",
      "Rating: 5.0\n",
      "Title: Nice\n",
      "Review: This shampoo and conditioner set is very moisturizing, but doesn't leave your hair limp or greasy-looking. It leaves my fine, shoulder-length hair bouncy and shiny. The travel size is also very convenient.\n",
      "\n",
      "Product: AG Care Natural Remedy Apple Cider Vinegar Leave On Mist\n",
      "Rating: 5.0\n",
      "Title: Detangles Well\n",
      "Review: This is pretty good. I was surprised at how well it worked for detangling wet hair. The scent is also nice -- it does not smell like vinegar. It does help with frizziness, too, on those really humid days. It also reduces static electricity in your hair on really dry days. I would definitely use this again.\n",
      "\n",
      "\n",
      "### Response:\n",
      "\"Short-Term Interests\": The user has recently engaged with hair care products focused on moisturizing and detangling, such as shampoos and conditioners with natural oils, hair towels, and convenient towelettes.\n",
      "\"Long-Term Preferences\": An analysis of the user's reviews reveals consistent themes:\n",
      "* Preference for moisturizing hair products that leave fine, shoulder-length hair shiny and bouncy without greasiness\n",
      "* Interest in products that help with detangling, reducing frizz, and minimizing static electricity\n",
      "* Appreciation for high-quality hair accessories like twist towels and soft hair towels\n",
      "* Values convenience items for on-the-go use, such as individually packaged towelettes\n",
      "* Preference for natural ingredients like coconut oil and avocado oil in hair care\n",
      "\"User_Profile\": The user appears to prioritize hair care and values products that enhance the health and appearance of their fine, shoulder-length hair. They seek moisturizing shampoos and conditioners that provide shine and manageability without weighing hair down. The user appreciates effective detangling solutions and products that combat frizz and static. They value quality hair accessories and convenient personal care items, showing a preference for natural ingredients in their hair care routine.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(profile_sample):\n",
    "    user_id = profile_sample[\"User_ID\"]\n",
    "    reviews = reviews_by_user.get(user_id, [])\n",
    "    review_texts = [f\"Product: {review['product_name']}\\nRating: {review['rating']}\\nTitle: {review['title']}\\nReview: {review['text']}\\n\" \n",
    "                    for review in reviews]\n",
    "    review_texts = review_texts[-10:]\n",
    "    formatted_reviews = \"\\n\".join(review_texts)\n",
    "    \n",
    "    instruction = ALPACA_LORA_PROMPTS_USER_PROFILE['instruction']\n",
    "    input_text = f\"User reviews:\\n{formatted_reviews}\" if formatted_reviews else \"No reviews available for this user.\"\n",
    "    output_text = profile_sample[\"User_Profile\"]\n",
    "    \n",
    "    full_text = f\"\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output_text}\"\n",
    "    return full_text\n",
    "\n",
    "# Verify preprocessing\n",
    "print(\"Preprocessed Sample:\", preprocess_function(training_data[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Tokenize and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(sample):\n",
    "    processed_text = preprocess_function(sample)\n",
    "    tokenized = tokenizer(\n",
    "        processed_text,\n",
    "        truncation=True,\n",
    "        max_length=PIPELINE_PARAMS['max_length'],\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # Set labels identical to input_ids\n",
    "    return tokenized\n",
    "\n",
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=QLORA_PARAMS['lora_r'],\n",
    "    lora_alpha=QLORA_PARAMS['lora_alpha'],\n",
    "    lora_dropout=QLORA_PARAMS['lora_dropout'],\n",
    "    target_modules=QLORA_PARAMS['lora_target_modules'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure LoRA and Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 16 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 10/16 [06:42<04:12, 42.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.6668, 'grad_norm': 7.947358131408691, 'learning_rate': 5e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [11:02<00:00, 41.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 662.8875, 'train_samples_per_second': 0.048, 'train_steps_per_second': 0.024, 'train_loss': 5.277340888977051, 'epoch': 2.0}\n",
      "Model trained with 16 samples saved to outputs/adapter_test_user_profile_epoch_2_16_chatgpt_data_samples\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRA configuration to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Training sizes\n",
    "training_sizes = [16]\n",
    "\n",
    "# Loop through different training sizes\n",
    "for train_size in training_sizes:\n",
    "    # Split the dataset\n",
    "    train_data = training_data[:train_size]\n",
    "    eval_data = training_data[train_size:train_size + int(0.2 * train_size)]  # 20% of training data for evaluation\n",
    "\n",
    "    # Tokenize datasets\n",
    "    train_tokenized_data = [tokenize_function(sample) for sample in train_data]\n",
    "    eval_tokenized_data = [tokenize_function(sample) for sample in eval_data]\n",
    "\n",
    "    # Convert tokenized data to Dataset format\n",
    "    train_dataset = Dataset.from_dict({\n",
    "        \"input_ids\": [x[\"input_ids\"][0] for x in train_tokenized_data],\n",
    "        \"attention_mask\": [x[\"attention_mask\"][0] for x in train_tokenized_data],\n",
    "        \"labels\": [x[\"labels\"][0] for x in train_tokenized_data]\n",
    "    })\n",
    "    eval_dataset = Dataset.from_dict({\n",
    "        \"input_ids\": [x[\"input_ids\"][0] for x in eval_tokenized_data],\n",
    "        \"attention_mask\": [x[\"attention_mask\"][0] for x in eval_tokenized_data],\n",
    "        \"labels\": [x[\"labels\"][0] for x in eval_tokenized_data]\n",
    "    })\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"outputs/adapter_test_candidate_items_epoch_{QLORA_PARAMS['lora_num_epochs']}_{train_size}_samples\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=QLORA_PARAMS['gradient_accumulation_steps'],\n",
    "        num_train_epochs=QLORA_PARAMS['lora_num_epochs'],\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=QLORA_PARAMS['lora_val_iterations'],\n",
    "        save_steps=QLORA_PARAMS['lora_val_iterations'],\n",
    "        logging_steps=10,\n",
    "        learning_rate=QLORA_PARAMS['lora_lr'],\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=\"none\",\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, padding=True)\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    # Clear GPU cache before training\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Start training\n",
    "    print(f\"Starting training with {train_size} samples.\")\n",
    "    trainer.train()\n",
    "    adapter_name = f\"adapter_test_user_profile_epoch_{QLORA_PARAMS['lora_num_epochs']}_{train_size}_chatgpt_data_samples\"\n",
    "    # Save the model and tokenizer in separate directories for each training size\n",
    "    model.save_pretrained(f\"outputs/{adapter_name}\")\n",
    "    tokenizer.save_pretrained(f\"outputs/{adapter_name}\")\n",
    "    print(f\"Model trained with {train_size} samples saved to outputs/{adapter_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]\n",
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\peft\\utils\\save_and_load.py:198: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adapters_weights = torch.load(filename, map_location=torch.device(device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded user profile adapter: user_profile\n",
      "Loaded candidate items adapter: candidate_items\n",
      "Adapters loaded in the model: ['user_profile', 'candidate_items']\n",
      "\n",
      "Setting active adapter to: user_profile\n",
      "Current active adapter: user_profile\n",
      "\n",
      "Generated User Profile:\n",
      ":\n",
      "\n",
      "Short-Term Interests:\n",
      "Based on recent products reviewed, there seems to be an interest in skincare routines, specifically focusing on moisturization and hydration during colder months. There may also be an emphasis on using masks that target specific areas such as eye and smile lines.\n",
      "\n",
      "Long-Term Preferences:\n",
      "Historical Reviews suggest a focus on mature skin care. Consistent themes revolve around maintaining hydrated and healthy-looking skin through various treatments including creams, serums, and facial masks. Products often contain ingredients known for promoting anti-aging effects and addressing signs of fine lines and wrinkles. In addition, users tend to look for gentle, non-comedogenic formulas suitable for sensitive skins, particularly rosacea-prone or acne-inclined complexions.\n",
      "\n",
      "User Profile Summary:\n",
      "This individual appears concerned with preserving a youthful appearance while adapting to seasonal changes in temperature and humidity. They seek versatile skincare solutions emphasizing intense hydration, nourishing antioxidants, and targeted treatments for common concerns such as crow's feet and nasolabial folds. Given their history of reviewing premium, science-backed formulations, they likely prioritize efficacy alongside ease-of-use, tolerability, and eco-friendly practices. As they navigate wintertime, expect them to continue exploring rich moisturizers, antioxidant-rich masks, and advanced serums addressing deep-seated issues related to aging and environmental stressors.\n",
      "\n",
      "Setting active adapter to: candidate_items\n",
      "Current active adapter: candidate_items\n",
      "\n",
      "Generated Candidate Items:\n",
      "General Product Categories:\n",
      "\n",
      "1. Moisturizing Creams & Serums\n",
      "2. Anti-Aging Treatments\n",
      "3. Hydrating Masks\n",
      "4. Sensitive Skin Care Formulations\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from config import *\n",
    "\n",
    "\n",
    "TOKENIZER_PATH = \"models/hf-frompretrained-download/meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# Load the base model and tokenizer\n",
    "base_model_path =\"models/hf-frompretrained-downloadmeta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=False)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    #quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    #torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Load the user profile adapter\n",
    "train_size = 64  # Adjust based on your adapter's training size\n",
    "adapter_path_user_profile =f\"outputs/adapter_test_user_profile_epoch_{QLORA_PARAMS['lora_num_epochs']}_{train_size}_samples\"\n",
    "adapter_name_user_profile = \"user_profile\"\n",
    "model = PeftModel.from_pretrained(model, adapter_path_user_profile, adapter_name=adapter_name_user_profile)\n",
    "\n",
    "print(f\"Loaded user profile adapter: {adapter_name_user_profile}\")\n",
    "\n",
    "# Load the candidate items adapter\n",
    "adapter_path_candidate_items = f\"outputs/adapter_test_candidate_items_epoch_{QLORA_PARAMS['lora_num_epochs']}_{64}_samples\"\n",
    "adapter_name_candidate_items = \"candidate_items\"\n",
    "model.load_adapter(adapter_path_candidate_items, adapter_name=adapter_name_candidate_items)\n",
    "\n",
    "print(f\"Loaded candidate items adapter: {adapter_name_candidate_items}\")\n",
    "\n",
    "# Print the list of adapters loaded into the model\n",
    "print(\"Adapters loaded in the model:\", list(model.peft_config.keys()))\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the text generation function\n",
    "def generate_text(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=PIPELINE_PARAMS['max_length'],\n",
    "        do_sample=True,\n",
    "        temperature=PIPELINE_PARAMS['temperature'],\n",
    "        top_k=PIPELINE_PARAMS['top_k'],\n",
    "        top_p=PIPELINE_PARAMS['top_p'],\n",
    "        repetition_penalty=PIPELINE_PARAMS['repetition_penalty'],\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Generate user profile\n",
    "user_reviews = \"\"\"User reviews:\n",
    "Product: Moisturizing Facial Emulsion for Restoring Hydrating Smoothing Skin from Manilla Natural Skincare\n",
    "Rating: 5.0\n",
    "Title: Intense Hydration\n",
    "Review: With winter approaching, it's time to start thinking about cold-weather skincare. This moisturizer is a good choice because it's a blend of several skin-loving oils like argan, coconut and macadamia. It's fairly lightweight but it protects and hydrates like a heavier cream. It can be used both day and night. The packaging is pretty enough for display. It can be used by all skin types. I highly recommend this product and the entire line from this brand.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
    "\n",
    "Product: Dr. Denese SkinScience Essential Lipid Anti Aging Power Infusion Dry Oil - Skin Nutrients 97% Organic 100% Natural - Rejuvinating Blend with Amaranth & Resveratrol - Paraben-Free, Cruelty-Free - 2oz\n",
    "Rating: 5.0\n",
    "Title: Youth Serum\n",
    "Review: One of the reasons skin starts to wrinkle as we age is that we make less and less of the natural oil that keeps skin looking youthful. This serum is a good way to supplement what your skin lacks. The oils in this blend are all high quality. The texture is lightweight and easily absorbed. Dr. Denese is a trusted brand when it comes to care for aging skin. Whether you're trying to slow the clock or reverse it a little, this serum should be part of your daily skincare routine. I hope the information that I've provided makes your decision a little easier.\n",
    "\n",
    "Product: [Abib] Creme coating mask Tone-up solution 17g (5pcs)\n",
    "Rating: 5.0\n",
    "Title: Doesn't Leave Skin Sticky\n",
    "Review: I use lots of sheet masks. Most leave your skin a little sticky when the serum dries. This mask works differently. The sheet locks the moisture in and doesn't leave a sticky residue. It's a great hydration and brightening mask. I highly recommend it.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
    "\n",
    "Product: Easydew 2-Step Face Contour Sheet Mask - Tightening & Anti-Aging & Hydrating Korean Face Mask with DW-EGF, Hyaluronic Acid for Anti-Winkle, Reducing Fine Line & Producing Collagen (Qty 5)\n",
    "Rating: 5.0\n",
    "Title: Special Care Mask\n",
    "Review: I really like that this mask includes special patches for the eyes and smile lines. If you're over 30, you'll appreciate the extra help. You apply the patches first, the the sheet mask. I keep all of my sheet masks in the refrigerator. You'll appreciate the cooling and anti-inflammatory effect if you do so with this mask. I highly recommend this product.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
    "\n",
    "Product: Higher Education Skincare: Goal Digger - Silky Moisturizing Cucumber Creme; formulated for dry and sensitive skin; cucumber extract; natural extract: melon, kale, cabbage, ginger, turmeric - 1.7 fl oz\n",
    "Rating: 5.0\n",
    "Title: Lightweight and Creamy\n",
    "Review: I really like this moisturizer. It's lightweight and creamy. It has a light cucumber scent that I find refreshing. It's a powerhouse at deeply hydrating parched skin. It also locks in moisture to keep skin looking fresh. It contains botanicals that soothe irritated or inflamed skin. It's ideal for those with acne, rosacea or eczema. You can use it as both a day and night cream. I highly recommend this product.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
    "\n",
    "Product: Queendom Unlashed Mascara | Volumizing and Lengthening | Boosts Lash Length | Vegan, Cruelty Free, Paraben Free\n",
    "Rating: 5.0\n",
    "Title: New Favorite!!\n",
    "Review: Every mascara claims to be new and different. This one actually lives up to the hype. The fibers have a novel shape that really makes your lashes POP. If you're not a fan of falsies, this is a great alternative. The formula isn't waterproof, so keep that in mind. I HIGHLY recommend this product.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
    "\n",
    "Product: ABOUT ME MediAnswer Collagen Firming Up Mask 4ea (1box) 25g - 77% Pure Collagen Extract Anti-Aging Total Care Facial Mask Sheet, Powerful Hydrating and Anti-Wrinkle Night Skin Care\n",
    "Rating: 5.0\n",
    "Title: Effective Mask\n",
    "Review: If you're reading this review, you probably already know the anti-aging benefits of collagen. This mask has a high-potency form of collagen along with a special delivery system to help firm your skin. The mask is in two parts, so you can get a better fit than a regular sheet mask. I can't say it performs miracles, but it's definitely worth a try.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
    "\"\"\"\n",
    "\n",
    "prompt_user_profile = (\n",
    "    ALPACA_LORA_PROMPTS_USER_PROFILE['instruction'] + \"\\n\\n\" +\n",
    "    ALPACA_LORA_PROMPTS_USER_PROFILE['input'].replace(\"{user_review}\", user_reviews)+ \"\\n### Response\"\n",
    ")\n",
    "\n",
    "# Set the active adapter to user profile adapter\n",
    "print(f\"\\nSetting active adapter to: {adapter_name_user_profile}\")\n",
    "model.set_adapter(adapter_name_user_profile)\n",
    "print(f\"Current active adapter: {model.active_adapter}\")\n",
    "\n",
    "generated_profile = generate_text(prompt_user_profile)\n",
    "generated_profile = generated_profile[len(prompt_user_profile):].strip()\n",
    "print(\"\\nGenerated User Profile:\")\n",
    "print(generated_profile)\n",
    "\n",
    "# Generate candidate items using the generated user profile\n",
    "prompt_candidate_items = (\n",
    "    ALPACA_LORA_PROMPTS_CANDIDATE_ITEMS['instruction'] + \"\\n\\n\" +\n",
    "    ALPACA_LORA_PROMPTS_CANDIDATE_ITEMS['input'].replace(\"{user_profile}\", generated_profile) + \"\\n### Response\"\n",
    ")\n",
    "\n",
    "# Set the active adapter to candidate items adapter\n",
    "print(f\"\\nSetting active adapter to: {adapter_name_candidate_items}\")\n",
    "model.set_adapter(adapter_name_candidate_items)\n",
    "print(f\"Current active adapter: {model.active_adapter}\")\n",
    "\n",
    "generated_items = generate_text(prompt_candidate_items)\n",
    "generated_items = generated_items[len(prompt_candidate_items):].strip()\n",
    "print(\"\\nGenerated Candidate Items:\")\n",
    "print(generated_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
