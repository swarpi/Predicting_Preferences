{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete: All_Beauty.valid.csv\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def extract_gz_to_file(gz_file_path, output_file_path=None):\n",
    "    \"\"\"\n",
    "    Extracts a .gz (gzip) file and saves it as a new file.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    gz_file_path : str\n",
    "        The path to the .gz file you want to extract.\n",
    "    \n",
    "    output_file_path : str, optional\n",
    "        The path where the extracted file will be saved. If not provided,\n",
    "        the file will be saved in the same directory as the .gz file with\n",
    "        the .gz extension removed.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    output_file_path : str\n",
    "        The path to the extracted file.\n",
    "    \n",
    "    Raises:\n",
    "    ------\n",
    "    FileNotFoundError:\n",
    "        If the specified .gz file does not exist.\n",
    "    \n",
    "    OSError:\n",
    "        If there is an error during the extraction process.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(gz_file_path):\n",
    "        raise FileNotFoundError(f\"The file {gz_file_path} does not exist.\")\n",
    "\n",
    "    # Set output path by default in the same directory as gz_file_path, with .gz removed\n",
    "    if output_file_path is None:\n",
    "        output_file_path = gz_file_path.rstrip('.gz')\n",
    "\n",
    "    try:\n",
    "        # Open the gzip file and the output file, then copy contents\n",
    "        with gzip.open(gz_file_path, 'rb') as f_in:\n",
    "            with open(output_file_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        \n",
    "        print(f\"Extraction complete: {output_file_path}\")\n",
    "        return output_file_path\n",
    "    except OSError as e:\n",
    "        raise OSError(f\"Error during extraction: {e}\")\n",
    "\n",
    "# Example Usage\n",
    "gz_input_path = 'All_Beauty.valid.csv.gz'         # Replace with your .gz file path\n",
    "gz_output_path = 'All_Beauty.valid.csv'      # Optional: specify the path for the extracted file\n",
    "\n",
    "try:\n",
    "    extracted_file_path = extract_gz_to_file(gz_input_path, gz_output_path)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define file paths\n",
    "REVIEWS_FILE = 'data/All_beauty_more_than_3_with_product.jsonl'        # Replace with your actual path\n",
    "TRAIN_SPLIT_FILE = 'All_Beauty.train.csv'      # Replace with your actual path\n",
    "TEST_SPLIT_FILE = 'All_Beauty.test.csv'        # Replace with your actual path\n",
    "VAL_SPLIT_FILE = 'All_Beauty.valid.csv'          # Replace with your actual path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['rating', 'title', 'text', 'images', 'asin', 'parent_asin', 'user_id',\n",
      "       'timestamp', 'helpful_vote', 'verified_purchase', 'product_name'],\n",
      "      dtype='object')\n",
      "Total reviews loaded: 39427\n",
      "'product_name' is present.\n",
      "Train reviews: 2029\n",
      "Test reviews: 253\n",
      "Validation reviews: 253\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load Reviews Data\n",
    "reviews = []\n",
    "with open(REVIEWS_FILE, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            review = json.loads(line.strip())\n",
    "            reviews.append(review)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "            continue\n",
    "\n",
    "reviews_df = pd.DataFrame(reviews)\n",
    "print(reviews_df.columns)\n",
    "print(f\"Total reviews loaded: {len(reviews_df)}\")\n",
    "\n",
    "# Verify if 'product_name' exists\n",
    "if 'product_name' not in reviews_df.columns:\n",
    "    print(\"'product_name' is missing. Attempting to fill with 'asin'.\")\n",
    "    # Option 1: Use 'asin' as 'product_name'\n",
    "    reviews_df['product_name'] = reviews_df['asin']\n",
    "    # Option 2: If you have another key, adjust accordingly\n",
    "    # Example:\n",
    "    # if 'productTitle' in reviews_df.columns:\n",
    "    #     reviews_df.rename(columns={'productTitle': 'product_name'}, inplace=True)\n",
    "else:\n",
    "    print(\"'product_name' is present.\")\n",
    "\n",
    "# Step 4: Load Split Data\n",
    "train_df = pd.read_csv(TRAIN_SPLIT_FILE)\n",
    "test_df = pd.read_csv(TEST_SPLIT_FILE)\n",
    "val_df = pd.read_csv(VAL_SPLIT_FILE)\n",
    "\n",
    "print(f\"Train reviews: {len(train_df)}\")\n",
    "print(f\"Test reviews: {len(test_df)}\")\n",
    "print(f\"Validation reviews: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removed: 2575\n",
      "Duplicates removed: 0\n",
      "Duplicates removed: 0\n",
      "Duplicates removed: 0\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicates(df, subset_cols, keep='first'):\n",
    "    \"\"\"\n",
    "    Removes duplicate rows from a DataFrame based on specified columns.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame from which to remove duplicates.\n",
    "    subset_cols : list\n",
    "        List of column names to consider for identifying duplicates.\n",
    "    keep : {'first', 'last', False}, default 'first'\n",
    "        Determines which duplicates (if any) to keep.\n",
    "        - 'first': Keep the first occurrence.\n",
    "        - 'last': Keep the last occurrence.\n",
    "        - False: Drop all duplicates.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with duplicates removed.\n",
    "    \"\"\"\n",
    "    before = len(df)\n",
    "    df_cleaned = df.drop_duplicates(subset=subset_cols, keep=keep).reset_index(drop=True)\n",
    "    after = len(df_cleaned)\n",
    "    print(f\"Duplicates removed: {before - after}\")\n",
    "    return df_cleaned\n",
    "\n",
    "# Define the columns to identify duplicates\n",
    "duplicate_subset = ['user_id', 'parent_asin', 'timestamp']\n",
    "\n",
    "# Remove duplicates from reviews_df\n",
    "reviews_df = remove_duplicates(reviews_df, subset_cols=duplicate_subset, keep='first')\n",
    "\n",
    "# (Optional) Remove duplicates from split DataFrames if necessary\n",
    "train_df = remove_duplicates(train_df, subset_cols=duplicate_subset, keep='first')\n",
    "test_df = remove_duplicates(test_df, subset_cols=duplicate_subset, keep='first')\n",
    "val_df = remove_duplicates(val_df, subset_cols=duplicate_subset, keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "unknown    34317\n",
      "train       2029\n",
      "test         253\n",
      "val          253\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Initialize 'split' column\n",
    "reviews_df['split'] = None\n",
    "\n",
    "# Function to assign split labels\n",
    "def assign_split(df, split_label):\n",
    "    # Create a unique identifier for matching\n",
    "    identifiers = set(zip(df['user_id'], df['parent_asin'], df['timestamp']))\n",
    "    return identifiers\n",
    "\n",
    "# Get identifiers for each split\n",
    "train_identifiers = assign_split(train_df, 'train')\n",
    "test_identifiers = assign_split(test_df, 'test')\n",
    "val_identifiers = assign_split(val_df, 'val')\n",
    "\n",
    "# Function to determine the split for each review\n",
    "def determine_split(row):\n",
    "    identifier = (row['user_id'], row['parent_asin'], row['timestamp'])\n",
    "    if identifier in train_identifiers:\n",
    "        return 'train'\n",
    "    elif identifier in test_identifiers:\n",
    "        return 'test'\n",
    "    elif identifier in val_identifiers:\n",
    "        return 'val'\n",
    "    else:\n",
    "        return 'unknown'  # Or handle as needed\n",
    "\n",
    "# Apply the function to assign splits\n",
    "reviews_df['split'] = reviews_df.apply(determine_split, axis=1)\n",
    "\n",
    "# Check distribution\n",
    "print(reviews_df['split'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rating', 'title', 'text', 'images', 'asin', 'parent_asin', 'user_id',\n",
       "       'timestamp', 'helpful_vote', 'verified_purchase', 'product_name',\n",
       "       'split'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"user_id\": \"AFSKPY37N3C43SOI5IEXEK5JSIYA\",\n",
      "        \"reviews\": [\n",
      "            {\n",
      "                \"product_name\": \"Keratin Secrets Do It Yourself Home Keratin System\",\n",
      "                \"parent_asin\": \"B07SLFWZKN\",\n",
      "                \"rating\": 3.0,\n",
      "                \"title\": \"Just ok\",\n",
      "                \"text\": \"I try to get Keratin treatments every 3 months, but honestly it has been getting costly. So, when I saw this I was excited to try it. I found it difficult to use and almost impossible to get to saturate the back of my hair and straight iron it the way they do in the salon. Front and sides were ok, but I couldn't maneuver the back to get it straight. Then I saw the ingredients after the first time and saw it contained formaldehyde and that was the last time I used the actual treatment. I did, however, use the shampoo and conditioner (and I still am). I wish they sold the S&C separate because I really did like it and I am always in the market for a good hair wash which won't strip my hair between treatments. I will resume my regular treatments at my salon.\",\n",
      "                \"timestamp\": 1619737501209\n",
      "            },\n",
      "            {\n",
      "                \"product_name\": \"GAINWELL\",\n",
      "                \"parent_asin\": \"B08JTNQFZY\",\n",
      "                \"rating\": 5.0,\n",
      "                \"title\": \"Good quality hair brush!\",\n",
      "                \"text\": \"Really nice small brush. Made well, nice wood made with boar bristle, my son absolutely loves this. It brushes his hair well and keeps him looking his best. This compact size makes it nice to keep in the center console of his car or to take on vacation with him. Highly recommend!\",\n",
      "                \"timestamp\": 1617904219785\n",
      "            },\n",
      "            {\n",
      "                \"product_name\": \"Organic Bamboo Cotton Ear Swabs by Bali Boo - 200 - Natural Wooden Qtips Cotton Swabs for Cleaning Ears, Baby or Makeup and Nails - Sustainable & Vegan Buds Sticks - Eco Friendly & Biodegradable\",\n",
      "                \"parent_asin\": \"B07KG1TWP5\",\n",
      "                \"rating\": 5.0,\n",
      "                \"title\": \"Great all natural ear swabs!\",\n",
      "                \"text\": \"I really like these ear swabs. First they come in a large, handy box and are easy to store (will last a long time). Second, they are all organic and good for the environment. Third, they are strong and don't fall apart very easily. I highly recommend these over Q-Tips (which I have used for years).\",\n",
      "                \"timestamp\": 1596473351088\n",
      "            },\n",
      "            {\n",
      "                \"product_name\": \"Iryasa Night Indulge Cream - Natural Face Cream for Dry Skin - Vegan Anti Aging Night Cream for Women - Firming Cream for Face and Neck - Organic Vitamin C Moisturizer for Face - 1.7oz\",\n",
      "                \"parent_asin\": \"B07W397QG4\",\n",
      "                \"rating\": 5.0,\n",
      "                \"title\": \"Wonderful overnight cream!\",\n",
      "                \"text\": \"To be honest, I rarely have used an overnight cream. Typically, my skin care routine is the same morning and night (wash, apply hyalauronic serum, and some kind of moistutrizer). So, when I saw this Iryasa cream I wanted to try it. Easy to apply, you can immediately feel the moisture starting to work. I do apply this about 30 minutes before I actually go to sleep to allow it time to be absorbed (before putting my face on my pillow) as it is a thicker type of cream and needs time. My skin in the morning feels better than ever. Soft and supple, I definitely feel a difference after using this for over 2 weeks now. I highly recommend this cream for women and men.\",\n",
      "                \"timestamp\": 1593352422858\n",
      "            },\n",
      "            {\n",
      "                \"product_name\": \"Manicure and Pedicure Nail Clipper from POWERGROOMING - Powerful Trimmer for Thick and Thin Finger Nails and Toe Nails - Included Nail File and\\\"Catcher\\\" for Easy Cleanup (1 Pack)\",\n",
      "                \"parent_asin\": \"B07J3GH1W1\",\n",
      "                \"rating\": 5.0,\n",
      "                \"title\": \"Nice manicure set for men or women\",\n",
      "                \"text\": \"This a really cute kit which would make for a great gift for someone. It is in a little leather like pouch and has everything you need to give yourself a quality manicure. The nail clipper is a perfect size and works just as well on a women or man's nails. The file is nice as well (although I still prefer to use emory boards on mine). I actually bought another one of these to give to my son as a stocking stuffer this last Christmas for him to use at college. Just a nice, quality made kit at a reasonable price.\",\n",
      "                \"timestamp\": 1547589356557\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"user_id\": \"AHV6QCNBJNSGLATP56JAWJ3C4G2A\",\n",
      "        \"reviews\": [\n",
      "            {\n",
      "                \"product_name\": \"invisibobble Sprunchie Spiral Hair Ring - True Black - Scrunchie Stylish Bracelet, Strong Elastic Grip Coil Accessories for Women - Gentle for Girls Teens and Thick Hair\",\n",
      "                \"parent_asin\": \"B08HXQ3T9K\",\n",
      "                \"rating\": 4.0,\n",
      "                \"title\": \"My First Scrunchie!\",\n",
      "                \"text\": \"This is my first scrunchie. I have not had my hair cut in about 9 months because of the pandemic, so my hair has gotten longer than I've worn it in years! I'm in my early 50's and it's a little below my shoulders now. My hair is thick and coarse. It's long enough to wear in a messy bun for the first time in years! The scrunchie is easy to use and holds up my thick hair pretty well. I don't have to readjust it because it's falling. You can feel the coils inside the fabric. I find it fascinating how it works. When I take it off, it shrinks up again. I like it!\",\n",
      "                \"timestamp\": 1607744901734\n",
      "            },\n",
      "            {\n",
      "                \"product_name\": \"PINROSE Perfumes Bold Soul - Eau de Parfum Petals (Fragrance Towelettes) - Vegan, Cruelty-free, and Hypoallergenic Scent with Essential Oils - Notes of Crushed Blackberry, Tuberose, Vanilla, Cinnamon and Patchouli\",\n",
      "                \"parent_asin\": \"B0B2L218H2\",\n",
      "                \"rating\": 4.0,\n",
      "                \"title\": \"Fun Way to Try Out a New Scent\",\n",
      "                \"text\": \"These Pinrose Gilded Fox petals are a fun way to try out a new scent! The box comes with 12 packets. Each packet has a fragrance infused towelette. They aren't too wet, but you are able to rub or wipe them on your pulse points or wherever and transfer the scent to your body. This one has a very interesting smell. At first I didn't detect the chocolate, so it's a bit subtle. The smell is a powdery sweetness with a hint of chocolate. I threw away the wipe in my office trash can and caught pleasant whiffs of it throughout the day. Since the box comes with so many, I plan to put some in my daughter's Christmas stocking so she can try them out. They would also be great for traveling.\",\n",
      "                \"timestamp\": 1605814409168\n",
      "            },\n",
      "            {\n",
      "                \"product_name\": \"MD Complete Bright & Healthy Vitamin C+ Vitalizing Face Serum| with Vitamin C, Vitamin E and Herbal Extracts | Potent Antioxidant Protection, Nourishes, Brightens and Evens Skin 0.5 fl oz\",\n",
      "                \"parent_asin\": \"B08BZ1RHPS\",\n",
      "                \"rating\": 4.0,\n",
      "                \"title\": \"A Bit Oily\",\n",
      "                \"text\": \"I am in my early fifties and my morning skincare routine calls for Vitamin C serum. This doesn't smell like most other Vitamin C serums that I have used. It has a neutral scent that doesn't linger. The serum consistency is thick, but runny. It's a bit oily and doesn't absorb quickly. My skin can be sensitive at times, but I didn't have any problems with this. Overall, I like it, but I don't love it.\",\n",
      "                \"timestamp\": 1597942744359\n",
      "            },\n",
      "            {\n",
      "                \"product_name\": \"Caudalie Favorites Set\",\n",
      "                \"parent_asin\": \"B085NYYLQ8\",\n",
      "                \"rating\": 4.0,\n",
      "                \"title\": \"Good Variety of Travel Size Skincare Products\",\n",
      "                \"text\": \"It's difficult to find some targeted skincare products in travel sizes. I can usually find cleansers and moisturizers, but serums and eye creams are not so easy to track down. This travel set comes with a foaming cleanser, an eye cream, a serum, and a moisturizer as well as a beauty spritz. I wasn't sure what the spritz was for, so I sprayed it on my face after washing it, kind of like a toner, but later I discovered that it's more of a setting spray. The sizes are typical for travel. They're small and should last up to a week, I'm guessing. The individual products seem to be fine. I've used the foaming cleanser before, but the other items were all new to me. The scents were light and floral. None of the products were irritating. I have sensitive eyes and skin and nothing bothered them. I do think the kit is a bit pricey, but that's not affecting my review.\",\n",
      "                \"timestamp\": 1597526701753\n",
      "            },\n",
      "            {\n",
      "                \"product_name\": \"MainBasics Headband with Buttons for Face Masks (Gray)\",\n",
      "                \"parent_asin\": \"B088FBNQXW\",\n",
      "                \"rating\": 4.0,\n",
      "                \"title\": \"Comfortable Head Band\",\n",
      "                \"text\": \"I work at home, so I can't get by without wearing a face mask for a good part of the day, but I do wear one if I am going out. I have had issues with some masks not staying over my ears. This is a good way to utilize those masks. The  headband is comfortable and light weight. It has some stretch to it. The buttons are a little loose, so I tightened them before I wore the headband out in public. I found that I had to pay attention to how I was putting it on my head so the buttons were even, otherwise my mask was crooked. It's definitely a helpful item.\",\n",
      "                \"timestamp\": 1593043740906\n",
      "            },\n",
      "            {\n",
      "                \"product_name\": \"2-Pack Puretize 16.9 Oz Hand Sanitizing Gel w/Pump Tops - Extra Strength 70% Ethyl Alcohol - Moisturizing Formula with Aloe Vera, Vitamin E & Botanical Extracts - Quick Drying Formula\",\n",
      "                \"parent_asin\": \"B087D7MVHB\",\n",
      "                \"rating\": 4.0,\n",
      "                \"title\": \"Good Hand Sanitizer!\",\n",
      "                \"text\": \"For a new (or at least new to me) brand of hand sanitizer, I am pleased with it. I like the pump dispenser which is easy to use. The bottles are a good size. While you can detect the smell of the alcohol, it's not too strong or overpowering like some can be.  With so many new brands out there these day, I wouldn't hesitate to purchase again.\",\n",
      "                \"timestamp\": 1591977214185\n",
      "            },\n",
      "            {\n",
      "                \"product_name\": \"BE PLAIN Vitamin Ampoule 1.01 fl oz. - Korean Multi Vitamin Serum for Face with Vitamin B, Vitamin C, Vitamin E, Hyaluronic Acid\",\n",
      "                \"parent_asin\": \"B083TLNBJJ\",\n",
      "                \"rating\": 4.0,\n",
      "                \"title\": \"Light Weight\",\n",
      "                \"text\": \"This is a light weight serum. It has a runny, liquidy consistency that's non-greasy. It doesn't leave behind any stickiness or tackiness. There is no scent to it either. It absorbs really fast and immediately hydrates your skin. It leaves a smooth finish. It's non-irritating and doesn't annoy my sensitive skin.\",\n",
      "                \"timestamp\": 1585261263214\n",
      "            },\n",
      "            {\n",
      "                \"product_name\": \"6 Pack Soft Headband BEoffer Coral Fleece Women Makeup Spa Head Bands Turban Fashion Bow Bowknot Hairlace Headwear Wash Face Hair Holder Elastic Top Knot Bandage Girl Baby Hair Accessories 6 Colors\",\n",
      "                \"parent_asin\": \"B082NKQ4ZT\",\n",
      "                \"rating\": 4.0,\n",
      "                \"title\": \"Soft\",\n",
      "                \"text\": \"I wear headbands like this in the evening when washing my face and following my skincare routine. These headbands are very soft. I found these to be a little small, but they do stretch enough to accommodate my large head. I have just had other headbands that were a little larger and didn't require being stretched to the max just to fit. I like the variety of colors and have shared these with my eighteen year old daughter. One thing that I do like is that the bow doesn't get off centered as easily as my old one. My old one has to be adjusted every time I put it on. This one seems to be just fine.\",\n",
      "                \"timestamp\": 1583932042329\n",
      "            },\n",
      "            {\n",
      "                \"product_name\": \"SERUM By MERLOT - RESVERATROL - NATURAL ANTI-AGING - DAMAGE CORRECTOR - BOOST COLLAGEN - 1 FL OZ. - DARK CIRCLE CORRECTOR -\",\n",
      "                \"parent_asin\": \"B07WNBZQGT\",\n",
      "                \"rating\": 4.0,\n",
      "                \"title\": \"Nice Fruity Scent\",\n",
      "                \"text\": \"This serum is more the consistency of a lotion than serums that I have used in the past. It has a nice fruity smell. It doesn't irritate my sensitive skin. The serum absorbs quickly and isn't tacky or sticky. I am in my early fifties and use an assortment of anti-aging products daily. I haven't seen any new miraculous results with this, but it does moisturize. As a regular user of such products, I really don't think I would see anything new from them, but rather a continuation of what I have already started to achieve.\",\n",
      "                \"timestamp\": 1581772960365\n",
      "            },\n",
      "            {\n",
      "                \"product_name\": \"Kale Firm & Smooth Body Lotion - Superfood Infused Firming Body Lotion with Kale. 6.8 fl.oz, Anti-Aging Moisturizing Lotion For All Skin Types\",\n",
      "                \"parent_asin\": \"B07SW7D6ZR\",\n",
      "                \"rating\": 4.0,\n",
      "                \"title\": \"Non-Greasy\",\n",
      "                \"text\": \"I apply body lotion every morning and night. This one is a thinner lotion than some, but it absorbs quickly. My skin just drinks it up! It's non-greasy and has a light, pleasant scent to it. The bottle is a little oddly shaped. It's thin and flat which makes it a little difficult to grip in my opinion.\",\n",
      "                \"timestamp\": 1580742144072\n",
      "            },\n",
      "            {\n",
      "                \"product_name\": \"13.5 fl. Oz. Goat Milk Facial Cleanser, Moisturizing Face Wash for Women, Hydrating Natural Face Wash, Anti-aging Face Wash, Face Wash for Aging Ski\",\n",
      "                \"parent_asin\": \"B07NPWK167\",\n",
      "                \"rating\": 5.0,\n",
      "                \"title\": \"Gentle!\",\n",
      "                \"text\": \"This goat milk cleanser is very gentle. I have sensitive skin and it doesn't irritate or annoy my skin at all. It has a pump dispenser that works nicely. It's thick, like a lotion consistency. The cleanser doesn't lather or foam. I haven't been able to notice any scent, good or bad. It doesn't dry out my skin. I'm very pleased with it.\",\n",
      "                \"timestamp\": 1578928089878\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Function to create JSON structure for a given split\n",
    "def create_json_split(df, split_label):\n",
    "    split_df = df[df['split'] == split_label]\n",
    "    user_reviews = defaultdict(list)\n",
    "    \n",
    "    for _, row in split_df.iterrows():\n",
    "        review = {\n",
    "            \"product_name\": row['product_name'],\n",
    "            \"parent_asin\": row['parent_asin'],\n",
    "            \"rating\": row['rating'],\n",
    "            \"title\": row['title'],\n",
    "            \"text\": row['text'],\n",
    "            \"timestamp\": row['timestamp']\n",
    "        }\n",
    "        user_reviews[row['user_id']].append(review)\n",
    "    \n",
    "    # Convert to the desired list of dictionaries\n",
    "    output = [{\"user_id\": user_id, \"reviews\": reviews} for user_id, reviews in user_reviews.items()]\n",
    "    return output\n",
    "\n",
    "# Create JSON structures\n",
    "train_json = create_json_split(reviews_df, 'train')\n",
    "test_json = create_json_split(reviews_df, 'test')\n",
    "val_json = create_json_split(reviews_df, 'val')\n",
    "\n",
    "# Display sample\n",
    "print(json.dumps(train_json[:2], indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON files saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save to JSON files\n",
    "with open('train_output.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_json, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open('test_output.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_json, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open('val_output.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(val_json, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"JSON files saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped Users: [4, 7, 8, 12, 17, 28, 31, 35, 40, 41, 43, 62, 65, 81, 87, 90, 93, 124, 129, 143, 159, 164, 174, 204, 205, 207, 218, 221, 231, 236, 240, 244]\n",
      "Skipped Users: [4, 7, 8, 12, 17, 28, 31, 35, 40, 41, 43, 62, 65, 81, 87, 90, 93, 124, 129, 143, 159, 164, 174, 204, 205, 207, 218, 221, 231, 236, 240, 244]\n",
      "Skipped Users: [4, 7, 8, 12, 17, 28, 31, 35, 40, 41, 43, 62, 65, 81, 87, 90, 93, 124, 129, 143, 159, 164, 174, 204, 205, 207, 218, 221, 231, 236, 240, 244]\n",
      "Skipped Users: [4, 7, 8, 12, 17, 28, 31, 35, 40, 41, 43, 62, 65, 81, 87, 90, 93, 124, 129, 143, 159, 164, 174, 204, 205, 207, 218, 221, 231, 236, 240, 244]\n",
      "Skipped Users: [4, 7, 8, 12, 17, 28, 31, 35, 40, 41, 43, 62, 65, 81, 87, 90, 93, 124, 129, 143, 159, 164, 174, 204, 205, 207, 218, 221, 231, 236, 240, 244]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_skipped_users(file_path):\n",
    "    \"\"\"Reads a text file and extracts user numbers from skipped user entries.\"\"\"\n",
    "    skipped_users = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Match lines with the pattern \"User X skipped after Y retries\"\n",
    "            match = re.match(r'User (\\d+) skipped after \\d+ retries\\.', line.strip())\n",
    "            if match:\n",
    "                # Extract the user number (X) from the match\n",
    "                user_number = int(match.group(1))\n",
    "                skipped_users.append(user_number)\n",
    "    return skipped_users\n",
    "\n",
    "# Example usage:\n",
    "# Assuming the file 'skipped_users.txt' contains the input strings\n",
    "file_path = 'results_None_2024-11-15_with_description_2_samples.txt'\n",
    "skipped_users = extract_skipped_users(file_path)\n",
    "print(\"Skipped Users:\", skipped_users)\n",
    "file_path = 'results_None_2024-11-15_with_description_3_samples.txt'\n",
    "skipped_users = extract_skipped_users(file_path)\n",
    "print(\"Skipped Users:\", skipped_users)\n",
    "file_path = 'results_None_2024-11-14_12pm_samples.txt'\n",
    "skipped_users = extract_skipped_users(file_path)\n",
    "print(\"Skipped Users:\", skipped_users)\n",
    "file_path = 'results_None_2024-11-14_3_samples.txt'\n",
    "skipped_users = extract_skipped_users(file_path)\n",
    "print(\"Skipped Users:\", skipped_users)\n",
    "file_path = 'results_None_2024-11-14_2_samples.txt'\n",
    "skipped_users = extract_skipped_users(file_path)\n",
    "print(\"Skipped Users:\", skipped_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path results_None_2024-11-15_with_description_2_samples.txt\n",
      "User 23, Product ID: B07JGD2T2J, Position: 5\n",
      "User 61, Product ID: B092M5K59T, Position: 15\n",
      "User 61, Product ID: B092M5K59T, Position: 16\n",
      "User 67, Product ID: B09C5NQSC5, Position: 13\n",
      "User 67, Product ID: B09C5NQSC5, Position: 17\n",
      "User 72, Product ID: B09GVHT2D3, Position: 3\n",
      "User 75, Product ID: B08MC3ZLV4, Position: 14\n",
      "User 196, Product ID: B08KWN77LW, Position: 18\n",
      "User 203, Product ID: B09GVHT2D3, Position: 1\n",
      "File path results_None_2024-11-15_with_description_3_samples.txt\n",
      "User 20, Product ID: B08S1LWF9V, Position: 9\n",
      "User 23, Product ID: B07JGD2T2J, Position: 7\n",
      "User 23, Product ID: B07JGD2T2J, Position: 9\n",
      "User 54, Product ID: B07SLFWZKN, Position: 5\n",
      "User 61, Product ID: B092M5K59T, Position: 19\n",
      "User 72, Product ID: B09GVHT2D3, Position: 4\n",
      "User 95, Product ID: B08W8LKLHB, Position: 7\n",
      "User 120, Product ID: B088PYN4VM, Position: 7\n",
      "User 126, Product ID: B07SLFWZKN, Position: 5\n",
      "User 160, Product ID: B0949MJRHK, Position: 6\n",
      "User 162, Product ID: B09C5NQSC5, Position: 12\n",
      "User 200, Product ID: B08G4Y4SFV, Position: 19\n",
      "User 203, Product ID: B09GVHT2D3, Position: 8\n",
      "User 203, Product ID: B09GVHT2D3, Position: 9\n",
      "User 237, Product ID: B09BKK8G76, Position: 13\n",
      "File path results_None_2024-11-14_12pm_samples.txt\n",
      "User 20, Product ID: B08S1LWF9V, Position: 15\n",
      "User 23, Product ID: B07JGD2T2J, Position: 7\n",
      "User 54, Product ID: B07SLFWZKN, Position: 11\n",
      "User 58, Product ID: B092M5K59T, Position: 8\n",
      "User 61, Product ID: B092M5K59T, Position: 13\n",
      "User 61, Product ID: B092M5K59T, Position: 14\n",
      "User 67, Product ID: B09C5NQSC5, Position: 4\n",
      "User 72, Product ID: B09GVHT2D3, Position: 1\n",
      "User 83, Product ID: B07SLFWZKN, Position: 19\n",
      "User 131, Product ID: B08VDCWKHV, Position: 19\n",
      "User 162, Product ID: B09C5NQSC5, Position: 7\n",
      "User 196, Product ID: B08KWN77LW, Position: 19\n",
      "User 203, Product ID: B09GVHT2D3, Position: 1\n",
      "File path results_None_2024-11-14_3_samples.txt\n",
      "User 20, Product ID: B08S1LWF9V, Position: 4\n",
      "User 23, Product ID: B07JGD2T2J, Position: 18\n",
      "User 27, Product ID: B083PXJBVY, Position: 7\n",
      "User 54, Product ID: B07SLFWZKN, Position: 5\n",
      "User 61, Product ID: B092M5K59T, Position: 18\n",
      "User 67, Product ID: B09C5NQSC5, Position: 14\n",
      "User 67, Product ID: B09C5NQSC5, Position: 16\n",
      "User 67, Product ID: B09C5NQSC5, Position: 17\n",
      "User 72, Product ID: B09GVHT2D3, Position: 1\n",
      "User 120, Product ID: B088PYN4VM, Position: 12\n",
      "User 160, Product ID: B0949MJRHK, Position: 6\n",
      "User 200, Product ID: B08G4Y4SFV, Position: 10\n",
      "User 203, Product ID: B09GVHT2D3, Position: 5\n",
      "File path results_None_2024-11-14_2_samples.txt\n",
      "User 23, Product ID: B07JGD2T2J, Position: 2\n",
      "User 54, Product ID: B07SLFWZKN, Position: 5\n",
      "User 61, Product ID: B092M5K59T, Position: 12\n",
      "User 67, Product ID: B09C5NQSC5, Position: 7\n",
      "User 67, Product ID: B09C5NQSC5, Position: 13\n",
      "User 67, Product ID: B09C5NQSC5, Position: 18\n",
      "User 72, Product ID: B09GVHT2D3, Position: 4\n",
      "User 73, Product ID: B08JCX3DL7, Position: 14\n",
      "User 83, Product ID: B07SLFWZKN, Position: 10\n",
      "User 100, Product ID: B08S3B8Y5G, Position: 20\n",
      "User 126, Product ID: B07SLFWZKN, Position: 18\n",
      "User 162, Product ID: B09C5NQSC5, Position: 8\n",
      "User 162, Product ID: B09C5NQSC5, Position: 14\n",
      "User 170, Product ID: B0912BNP4J, Position: 12\n",
      "User 170, Product ID: B0912BNP4J, Position: 18\n",
      "User 197, Product ID: B08LZ6W8RF, Position: 5\n",
      "User 203, Product ID: B09GVHT2D3, Position: 14\n",
      "User 203, Product ID: B09GVHT2D3, Position: 19\n",
      "User 228, Product ID: B08GKVYS1Y, Position: 14\n",
      "User 235, Product ID: B09473GGM4, Position: 19\n",
      "User 235, Product ID: B09473GGM4, Position: 20\n",
      "User 23, Product ID: B07JGD2T2J, Position: 2\n",
      "User 54, Product ID: B07SLFWZKN, Position: 5\n",
      "User 61, Product ID: B092M5K59T, Position: 12\n",
      "User 67, Product ID: B09C5NQSC5, Position: 7\n",
      "User 67, Product ID: B09C5NQSC5, Position: 13\n",
      "User 67, Product ID: B09C5NQSC5, Position: 18\n",
      "User 72, Product ID: B09GVHT2D3, Position: 4\n",
      "User 73, Product ID: B08JCX3DL7, Position: 14\n",
      "User 83, Product ID: B07SLFWZKN, Position: 10\n",
      "User 100, Product ID: B08S3B8Y5G, Position: 20\n",
      "User 126, Product ID: B07SLFWZKN, Position: 18\n",
      "User 162, Product ID: B09C5NQSC5, Position: 8\n",
      "User 162, Product ID: B09C5NQSC5, Position: 14\n",
      "User 170, Product ID: B0912BNP4J, Position: 12\n",
      "User 170, Product ID: B0912BNP4J, Position: 18\n",
      "User 197, Product ID: B08LZ6W8RF, Position: 5\n",
      "User 203, Product ID: B09GVHT2D3, Position: 14\n",
      "User 203, Product ID: B09GVHT2D3, Position: 19\n",
      "User 228, Product ID: B08GKVYS1Y, Position: 14\n",
      "User 235, Product ID: B09473GGM4, Position: 19\n",
      "User 235, Product ID: B09473GGM4, Position: 20\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_perfect_matches(file_path):\n",
    "    \"\"\"Extract users, product IDs, and positions where similarity is 100.00% and the product matched.\"\"\"\n",
    "    perfect_matches = []\n",
    "    current_user = None\n",
    "    capturing = False\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Match the start of a user's results\n",
    "            user_match = re.match(r'^User (\\d+):$', line)\n",
    "            if user_match:\n",
    "                current_user = int(user_match.group(1))\n",
    "                capturing = True\n",
    "                continue\n",
    "            \n",
    "            # Stop capturing when encountering \"Profile\" or next user's results\n",
    "            if \"Profile\" in line or re.match(r'^User \\d+:$', line):\n",
    "                capturing = False\n",
    "\n",
    "            # Capture lines with \"100.00% - Match\"\n",
    "            if capturing and \"Similarity: 100.00% - Match\" in line:\n",
    "                # Match the product ID and position\n",
    "                product_match = re.match(r'^\\s*(\\d+)\\.\\s+([A-Z0-9]+)\\s+-', line)\n",
    "                if product_match:\n",
    "                    position = int(product_match.group(1))\n",
    "                    product_id = product_match.group(2)\n",
    "                    perfect_matches.append((current_user, product_id, position))\n",
    "    \n",
    "    return perfect_matches\n",
    "\n",
    "# Example usage:\n",
    "file_path = 'results_None_2024-11-15_with_description_2_samples.txt'\n",
    "matches = extract_perfect_matches(file_path)\n",
    "print(f\"File path {file_path}\")\n",
    "for user, product_id, position in matches:\n",
    "    print(f\"User {user}, Product ID: {product_id}, Position: {position}\")\n",
    "file_path = 'results_None_2024-11-15_with_description_3_samples.txt'\n",
    "matches = extract_perfect_matches(file_path)\n",
    "print(f\"File path {file_path}\")\n",
    "for user, product_id, position in matches:\n",
    "    print(f\"User {user}, Product ID: {product_id}, Position: {position}\")\n",
    "file_path = 'results_None_2024-11-14_12pm_samples.txt'\n",
    "matches = extract_perfect_matches(file_path)\n",
    "print(f\"File path {file_path}\")\n",
    "for user, product_id, position in matches:\n",
    "    print(f\"User {user}, Product ID: {product_id}, Position: {position}\")\n",
    "file_path = 'results_None_2024-11-14_3_samples.txt'\n",
    "matches = extract_perfect_matches(file_path)\n",
    "print(f\"File path {file_path}\")\n",
    "for user, product_id, position in matches:\n",
    "    print(f\"User {user}, Product ID: {product_id}, Position: {position}\")\n",
    "file_path = 'results_None_2024-11-14_2_samples.txt'\n",
    "matches = extract_perfect_matches(file_path)\n",
    "print(f\"File path {file_path}\")\n",
    "for user, product_id, position in matches:\n",
    "    print(f\"User {user}, Product ID: {product_id}, Position: {position}\")\n",
    "matches = extract_perfect_matches(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: results_None_2024-11-15_with_description_2_samples.txt\n",
      "User 23, Product ID: B07JGD2T2J, Position: 5\n",
      "User 61, Product ID: B092M5K59T, Position: 15\n",
      "User 67, Product ID: B09C5NQSC5, Position: 13\n",
      "User 72, Product ID: B09GVHT2D3, Position: 3\n",
      "User 75, Product ID: B08MC3ZLV4, Position: 14\n",
      "User 196, Product ID: B08KWN77LW, Position: 18\n",
      "User 203, Product ID: B09GVHT2D3, Position: 1\n",
      "----------------------------------------\n",
      "Processing file: results_None_2024-11-15_with_description_3_samples.txt\n",
      "User 20, Product ID: B08S1LWF9V, Position: 9\n",
      "User 23, Product ID: B07JGD2T2J, Position: 7\n",
      "User 54, Product ID: B07SLFWZKN, Position: 5\n",
      "User 61, Product ID: B092M5K59T, Position: 19\n",
      "User 72, Product ID: B09GVHT2D3, Position: 4\n",
      "User 95, Product ID: B08W8LKLHB, Position: 7\n",
      "User 120, Product ID: B088PYN4VM, Position: 7\n",
      "User 126, Product ID: B07SLFWZKN, Position: 5\n",
      "User 160, Product ID: B0949MJRHK, Position: 6\n",
      "User 162, Product ID: B09C5NQSC5, Position: 12\n",
      "User 200, Product ID: B08G4Y4SFV, Position: 19\n",
      "User 203, Product ID: B09GVHT2D3, Position: 8\n",
      "User 237, Product ID: B09BKK8G76, Position: 13\n",
      "----------------------------------------\n",
      "Processing file: results_None_2024-11-14_12pm_samples.txt\n",
      "User 20, Product ID: B08S1LWF9V, Position: 15\n",
      "User 23, Product ID: B07JGD2T2J, Position: 7\n",
      "User 54, Product ID: B07SLFWZKN, Position: 11\n",
      "User 58, Product ID: B092M5K59T, Position: 8\n",
      "User 61, Product ID: B092M5K59T, Position: 13\n",
      "User 67, Product ID: B09C5NQSC5, Position: 4\n",
      "User 72, Product ID: B09GVHT2D3, Position: 1\n",
      "User 83, Product ID: B07SLFWZKN, Position: 19\n",
      "User 131, Product ID: B08VDCWKHV, Position: 19\n",
      "User 162, Product ID: B09C5NQSC5, Position: 7\n",
      "User 196, Product ID: B08KWN77LW, Position: 19\n",
      "User 203, Product ID: B09GVHT2D3, Position: 1\n",
      "----------------------------------------\n",
      "Processing file: results_None_2024-11-14_3_samples.txt\n",
      "User 20, Product ID: B08S1LWF9V, Position: 4\n",
      "User 23, Product ID: B07JGD2T2J, Position: 18\n",
      "User 27, Product ID: B083PXJBVY, Position: 7\n",
      "User 54, Product ID: B07SLFWZKN, Position: 5\n",
      "User 61, Product ID: B092M5K59T, Position: 18\n",
      "User 67, Product ID: B09C5NQSC5, Position: 14\n",
      "User 72, Product ID: B09GVHT2D3, Position: 1\n",
      "User 120, Product ID: B088PYN4VM, Position: 12\n",
      "User 160, Product ID: B0949MJRHK, Position: 6\n",
      "User 200, Product ID: B08G4Y4SFV, Position: 10\n",
      "User 203, Product ID: B09GVHT2D3, Position: 5\n",
      "----------------------------------------\n",
      "Processing file: results_None_2024-11-14_2_samples.txt\n",
      "User 23, Product ID: B07JGD2T2J, Position: 2\n",
      "User 54, Product ID: B07SLFWZKN, Position: 5\n",
      "User 61, Product ID: B092M5K59T, Position: 12\n",
      "User 67, Product ID: B09C5NQSC5, Position: 7\n",
      "User 72, Product ID: B09GVHT2D3, Position: 4\n",
      "User 73, Product ID: B08JCX3DL7, Position: 14\n",
      "User 83, Product ID: B07SLFWZKN, Position: 10\n",
      "User 100, Product ID: B08S3B8Y5G, Position: 20\n",
      "User 126, Product ID: B07SLFWZKN, Position: 18\n",
      "User 162, Product ID: B09C5NQSC5, Position: 8\n",
      "User 170, Product ID: B0912BNP4J, Position: 12\n",
      "User 197, Product ID: B08LZ6W8RF, Position: 5\n",
      "User 203, Product ID: B09GVHT2D3, Position: 14\n",
      "User 228, Product ID: B08GKVYS1Y, Position: 14\n",
      "User 235, Product ID: B09473GGM4, Position: 19\n",
      "----------------------------------------\n",
      "\n",
      "Analyzing all matches...\n",
      "\n",
      "Most common users:\n",
      "User 23: 5 times\n",
      "User 61: 5 times\n",
      "User 72: 5 times\n",
      "User 203: 5 times\n",
      "User 67: 4 times\n",
      "User 54: 4 times\n",
      "User 20: 3 times\n",
      "User 162: 3 times\n",
      "User 196: 2 times\n",
      "User 120: 2 times\n",
      "User 126: 2 times\n",
      "User 160: 2 times\n",
      "User 200: 2 times\n",
      "User 83: 2 times\n",
      "User 75: 1 times\n",
      "User 95: 1 times\n",
      "User 237: 1 times\n",
      "User 58: 1 times\n",
      "User 131: 1 times\n",
      "User 27: 1 times\n",
      "User 73: 1 times\n",
      "User 100: 1 times\n",
      "User 170: 1 times\n",
      "User 197: 1 times\n",
      "User 228: 1 times\n",
      "User 235: 1 times\n",
      "\n",
      "Most common products:\n",
      "Product B09GVHT2D3: 10 times\n",
      "Product B07SLFWZKN: 8 times\n",
      "Product B09C5NQSC5: 7 times\n",
      "Product B092M5K59T: 6 times\n",
      "Product B07JGD2T2J: 5 times\n",
      "Product B08S1LWF9V: 3 times\n",
      "Product B08KWN77LW: 2 times\n",
      "Product B088PYN4VM: 2 times\n",
      "Product B0949MJRHK: 2 times\n",
      "Product B08G4Y4SFV: 2 times\n",
      "Product B08MC3ZLV4: 1 times\n",
      "Product B08W8LKLHB: 1 times\n",
      "Product B09BKK8G76: 1 times\n",
      "Product B08VDCWKHV: 1 times\n",
      "Product B083PXJBVY: 1 times\n",
      "Product B08JCX3DL7: 1 times\n",
      "Product B08S3B8Y5G: 1 times\n",
      "Product B0912BNP4J: 1 times\n",
      "Product B08LZ6W8RF: 1 times\n",
      "Product B08GKVYS1Y: 1 times\n",
      "Product B09473GGM4: 1 times\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def extract_perfect_matches(file_path):\n",
    "    \"\"\"Extract users, product IDs, and positions where similarity is 100.00% and the product matched.\"\"\"\n",
    "    perfect_matches = {}\n",
    "    current_user = None\n",
    "    capturing = False\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Match the start of a user's results\n",
    "            user_match = re.match(r'^User (\\d+):$', line)\n",
    "            if user_match:\n",
    "                current_user = int(user_match.group(1))\n",
    "                capturing = True\n",
    "                continue\n",
    "            \n",
    "            # Stop capturing when encountering \"Profile\" or next user's results\n",
    "            if \"Profile\" in line or re.match(r'^User \\d+:$', line):\n",
    "                capturing = False\n",
    "\n",
    "            # Capture lines with \"100.00% - Match\"\n",
    "            if capturing and \"Similarity: 100.00% - Match\" in line:\n",
    "                # Match the product ID and position\n",
    "                product_match = re.match(r'^\\s*(\\d+)\\.\\s+([A-Z0-9]+)\\s+-', line)\n",
    "                if product_match:\n",
    "                    position = int(product_match.group(1))\n",
    "                    product_id = product_match.group(2)\n",
    "\n",
    "                    # Only keep the best position for each user\n",
    "                    if current_user not in perfect_matches or perfect_matches[current_user][1] > position:\n",
    "                        perfect_matches[current_user] = (product_id, position)\n",
    "    \n",
    "    # Convert to a list of tuples for compatibility\n",
    "    return [(user, product_id, position) for user, (product_id, position) in perfect_matches.items()]\n",
    "\n",
    "def analyze_matches(all_matches):\n",
    "    \"\"\"Analyze the extracted matches to find patterns.\"\"\"\n",
    "    user_counter = Counter()\n",
    "    product_counter = Counter()\n",
    "\n",
    "    for match in all_matches:\n",
    "        user, product, _ = match\n",
    "        user_counter[user] += 1\n",
    "        product_counter[product] += 1\n",
    "\n",
    "    most_common_users = user_counter.most_common()\n",
    "    most_common_products = product_counter.most_common()\n",
    "\n",
    "    return most_common_users, most_common_products\n",
    "\n",
    "def process_files(file_paths):\n",
    "    \"\"\"Process multiple files and analyze results.\"\"\"\n",
    "    all_matches = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        matches = extract_perfect_matches(file_path)\n",
    "        all_matches.extend(matches)\n",
    "        for user, product_id, position in matches:\n",
    "            print(f\"User {user}, Product ID: {product_id}, Position: {position}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    # Analyze all matches\n",
    "    print(\"\\nAnalyzing all matches...\")\n",
    "    most_common_users, most_common_products = analyze_matches(all_matches)\n",
    "\n",
    "    # Print analysis results\n",
    "    print(\"\\nMost common users:\")\n",
    "    for user, count in most_common_users:\n",
    "        print(f\"User {user}: {count} times\")\n",
    "\n",
    "    print(\"\\nMost common products:\")\n",
    "    for product, count in most_common_products:\n",
    "        print(f\"Product {product}: {count} times\")\n",
    "\n",
    "# Example usage\n",
    "file_paths = [\n",
    "    'results_None_2024-11-15_with_description_2_samples.txt',\n",
    "    'results_None_2024-11-15_with_description_3_samples.txt',\n",
    "    'results_None_2024-11-14_12pm_samples.txt',\n",
    "    'results_None_2024-11-14_3_samples.txt',\n",
    "    'results_None_2024-11-14_2_samples.txt'\n",
    "]\n",
    "\n",
    "process_files(file_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine train and val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed: All reviews from data2 are correctly merged into data1.\n",
      "Sorted merged data saved to sorted_merged_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Step 1: Load JSON files\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# File paths for the JSON files\n",
    "file1_path = 'new_data/new_train_output.json'  # Replace with the path to your first JSON file\n",
    "file2_path = 'new_data/new_val_output.json'  # Replace with the path to your second JSON file\n",
    "\n",
    "# Load JSON data\n",
    "data1 = load_json(file1_path)\n",
    "data2 = load_json(file2_path)\n",
    "\n",
    "# Step 2: Merge JSON data while retaining all of JSON 1\n",
    "def merge_json(data1, data2):\n",
    "    # Create a dictionary for quick lookup of user_id in data1\n",
    "    user_map = {user['user_id']: user for user in data1}\n",
    "    \n",
    "    for user in data2:\n",
    "        user_id = user['user_id']\n",
    "        if user_id in user_map:\n",
    "            # Add reviews to the existing user in data1\n",
    "            user_map[user_id]['reviews'].extend(user['reviews'])\n",
    "        else:\n",
    "            # Add the new user from data2 to data1\n",
    "            data1.append(user)\n",
    "    return data1\n",
    "\n",
    "# Step 3: Sort reviews by timestamp\n",
    "def sort_reviews_by_timestamp(data):\n",
    "    for user in data:\n",
    "        user['reviews'].sort(key=lambda review: review['timestamp'])\n",
    "    return data\n",
    "\n",
    "# Merge the data\n",
    "merged_data = merge_json(data1, data2)\n",
    "\n",
    "# Sort the reviews for each user by timestamp\n",
    "sorted_data = sort_reviews_by_timestamp(merged_data)\n",
    "\n",
    "# Step 4: Test the merging\n",
    "def test_merge(data1, data2, merged_data):\n",
    "    for user in data2:\n",
    "        user_id = user['user_id']\n",
    "        # Find the user in the merged data\n",
    "        merged_user = next((u for u in merged_data if u['user_id'] == user_id), None)\n",
    "        if not merged_user:\n",
    "            print(f\"Test failed: User {user_id} not found in merged data.\")\n",
    "            return False\n",
    "        # Check if all reviews from data2 are in the merged data\n",
    "        for review in user['reviews']:\n",
    "            if review not in merged_user['reviews']:\n",
    "                print(f\"Test failed: Review {review} not found in merged data for user {user_id}.\")\n",
    "                return False\n",
    "    print(\"Test passed: All reviews from data2 are correctly merged into data1.\")\n",
    "    return True\n",
    "\n",
    "# Run the test\n",
    "test_merge(data1, data2, sorted_data)\n",
    "\n",
    "# Step 5: Save the sorted merged data to a file (optional)\n",
    "output_path = \"sorted_merged_data.json\"\n",
    "with open(output_path, 'w') as outfile:\n",
    "    json.dump(sorted_data, outfile, indent=4)\n",
    "\n",
    "print(f\"Sorted merged data saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
