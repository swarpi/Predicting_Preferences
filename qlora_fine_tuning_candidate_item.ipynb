{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import necessary modules and load configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEAUTY, CHATGPT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Loaded:\n",
      "Tokenizer Path: models/hf-frompretrained-download/meta-llama/Meta-Llama-3-8B-Instruct\n",
      "Model Path: models/hf-frompretrained-downloadmeta-llama/Meta-Llama-3-8B-Instruct\n",
      "Pipeline Parameters: {'max_length': 2048, 'num_return_sequences': 1, 'temperature': 0.7, 'top_k': 50, 'top_p': 0.95, 'repetition_penalty': 1.2}\n",
      "QLoRA Parameters: {'lora_r': 8, 'lora_alpha': 8, 'lora_dropout': 0.01, 'lora_target_modules': ['q_proj', 'v_proj'], 'gradient_accumulation_steps': 2, 'lora_num_epochs': 2, 'lora_val_iterations': 100, 'lora_early_stopping_patience': 10, 'lora_lr': 0.0001, 'lora_micro_batch_size': 1}\n",
      "Prompt Template: {'instruction': \"### Instruction:\\n You are a recommender system specialized. Based on the following user profile text, generate a list of general candidate item categories that align with the user's preferences and interests. Approach this task by treating these categories as a cohesive set, ensuring that they collectively reflect the user’s overall profile and maximize satisfaction. \", 'input': '### Input \\n User Profile: \\n {user_profile}', 'output': '### Response:'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "\n",
    "# Set environment variable for memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Load configuration settings\n",
    "from config import TOKENIZER_PATH, MODEL_PATH, PIPELINE_PARAMS, QLORA_PARAMS, ALPACA_LORA_PROMPTS_CANDIDATE_ITEMS\n",
    "\n",
    "# Verification\n",
    "print(\"Configuration Loaded:\")\n",
    "print(\"Tokenizer Path:\", TOKENIZER_PATH)\n",
    "print(\"Model Path:\", MODEL_PATH)\n",
    "print(\"Pipeline Parameters:\", PIPELINE_PARAMS)\n",
    "print(\"QLoRA Parameters:\", QLORA_PARAMS)\n",
    "print(\"Prompt Template:\", ALPACA_LORA_PROMPTS_CANDIDATE_ITEMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and verify training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Structure Verification:\n",
      "Data verification successful!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the training data\n",
    "data_path = \"QLoRa_finetuning/matching_ids_chatGPT.json\"\n",
    "with open(data_path, \"r\") as file:\n",
    "    training_data = json.load(file)\n",
    "    \n",
    "#torch.cuda.empty_cache()\n",
    "\n",
    "# Verify data structure\n",
    "print(\"Data Structure Verification:\")\n",
    "for i, sample in enumerate(training_data[:2]):\n",
    "    assert \"User_ID\" in sample, f\"User_ID missing in sample {i}\"\n",
    "    assert \"User_Profile\" in sample, f\"User_Profile missing in sample {i}\"\n",
    "    assert \"Candidate_Items\" in sample, f\"Candidate_Items missing in sample {i}\"\n",
    "print(\"Data verification successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize the Tokenizer and Model with Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\quantizers\\auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.08s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS if not already set\n",
    "\n",
    "# Set 4-bit quantization configuration for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',  # Use NormalFloat4 for better memory efficiency\n",
    "    bnb_4bit_use_double_quant=True  # Double quantization for more memory saving\n",
    ")\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"  # Automatically maps layers to available GPU memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Preprocessing Function to Match Reviews with Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Sample: ### Instruction:\n",
      "### Instruction:\n",
      " You are a recommender system specialized. Based on the following user profile text, generate a list of general candidate item categories that align with the user's preferences and interests. Approach this task by treating these categories as a cohesive set, ensuring that they collectively reflect the user’s overall profile and maximize satisfaction. \n",
      "\n",
      "### Input \n",
      " User Profile: \n",
      " \"Short-Term Interests\": The user has recently engaged with beauty products focusing on eye makeup and nail care. Specifically, they have reviewed mascaras that enhance lash length and volume, gel nail polish sets with unique colors, nail lamps, and hair styling tools.\n",
      "\"Long-Term Preferences\": An analysis of the user's reviews reveals consistent themes:\n",
      "* Interest in makeup products that enhance appearance, particularly eyes and nails\n",
      "* Preference for unique and high-quality nail polishes, including indie-style and seasonal colors\n",
      "* Appreciation for effective beauty tools that improve at-home beauty routines, such as nail lamps and curling irons\n",
      "* Desire for products that are efficient and deliver promised results\n",
      "\"User_Profile\": The user appears to be someone who enjoys at-home beauty treatments, with a strong focus on nails and eye makeup. They value high-quality, unique products that enhance their appearance and are interested in tools that make their beauty routines more efficient. They appreciate products that deliver on their promises and improve the overall at-home beauty experience.\n",
      "\n",
      "### Response:\n",
      "Candidate Item Categories:\n",
      "1. Unique Gel Nail Polish Sets\n",
      "2. High-Wattage Nail Lamps\n",
      "3. Volumizing and Lengthening Mascaras\n",
      "4. Advanced Hair Curling Tools\n",
      "5. Nail Art Accessories\n"
     ]
    }
   ],
   "source": [
    "# Preprocess function to format the data for candidate item generation\n",
    "def preprocess_function(profile_sample):\n",
    "    # Use the user profile as input\n",
    "    input_text = ALPACA_LORA_PROMPTS_CANDIDATE_ITEMS['input'].replace(\n",
    "        \"{user_profile}\", profile_sample[\"User_Profile\"]\n",
    "    )\n",
    "    \n",
    "    # Set up the output text (Candidate Items) as the expected response\n",
    "    output_text = \"\\n\".join(\n",
    "        [f\"{i + 1}. {item}\" for i, item in enumerate(profile_sample[\"Candidate_Items\"].values())]\n",
    "    )\n",
    "    \n",
    "    # Format the complete prompt for training\n",
    "    full_text = f\"### Instruction:\\n{ALPACA_LORA_PROMPTS_CANDIDATE_ITEMS['instruction']}\\n\\n{input_text}\\n\\n### Response:\\nCandidate Item Categories:\\n{output_text}\"\n",
    "    return full_text\n",
    "\n",
    "# Verify preprocessing\n",
    "print(\"Preprocessed Sample:\", preprocess_function(training_data[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Tokenize and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(sample):\n",
    "    processed_text = preprocess_function(sample)\n",
    "    tokenized = tokenizer(\n",
    "        processed_text,\n",
    "        truncation=True,\n",
    "        max_length=PIPELINE_PARAMS['max_length'],\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # Set labels identical to input_ids\n",
    "    return tokenized\n",
    "\n",
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=QLORA_PARAMS['lora_r'],\n",
    "    lora_alpha=QLORA_PARAMS['lora_alpha'],\n",
    "    lora_dropout=QLORA_PARAMS['lora_dropout'],\n",
    "    target_modules=QLORA_PARAMS['lora_target_modules'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure LoRA and Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 16 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      " 62%|██████▎   | 10/16 [00:47<00:28,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.9628, 'grad_norm': 9.679896354675293, 'learning_rate': 5.6250000000000005e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:16<00:00,  4.79s/it]\n",
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 76.719, 'train_samples_per_second': 0.417, 'train_steps_per_second': 0.209, 'train_loss': 9.305109024047852, 'epoch': 2.0}\n",
      "Model trained with 16 samples saved to outputs/adapter_test_candidate_items_epoch_2_16_chatGPT_data\n",
      "Starting training with 32 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 10/32 [00:47<01:44,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.3056, 'grad_norm': 15.756006240844727, 'learning_rate': 7.8125e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 20/32 [01:35<00:57,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8001, 'grad_norm': 4.142382621765137, 'learning_rate': 4.6875e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 30/32 [02:22<00:09,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4623, 'grad_norm': 0.32388541102409363, 'learning_rate': 1.5625e-05, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [02:32<00:00,  4.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 152.1008, 'train_samples_per_second': 0.421, 'train_steps_per_second': 0.21, 'train_loss': 2.70556197501719, 'epoch': 2.0}\n",
      "Model trained with 32 samples saved to outputs/adapter_test_candidate_items_epoch_2_32_chatGPT_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 64 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 10/64 [00:47<04:15,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4928, 'grad_norm': 0.2193705141544342, 'learning_rate': 8.4375e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 20/64 [01:34<03:28,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4447, 'grad_norm': 0.1848074346780777, 'learning_rate': 6.875e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 30/64 [02:22<02:41,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4363, 'grad_norm': 0.27242138981819153, 'learning_rate': 5.3125000000000004e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 40/64 [03:09<01:53,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3588, 'grad_norm': 0.2413753867149353, 'learning_rate': 3.7500000000000003e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 50/64 [03:57<01:06,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3682, 'grad_norm': 0.293459951877594, 'learning_rate': 2.1875e-05, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 60/64 [04:44<00:18,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3227, 'grad_norm': 0.2635168731212616, 'learning_rate': 6.25e-06, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [05:03<00:00,  4.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 303.3239, 'train_samples_per_second': 0.422, 'train_steps_per_second': 0.211, 'train_loss': 0.40048264153301716, 'epoch': 2.0}\n",
      "Model trained with 64 samples saved to outputs/adapter_test_candidate_items_epoch_2_64_chatGPT_data\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRA configuration to the model\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.gradient_checkpointing_enable()\n",
    "model = get_peft_model(model, lora_config)\n",
    "# Training sizes\n",
    "training_sizes = [16,32,64]\n",
    "\n",
    "# Loop through different training sizes\n",
    "for train_size in training_sizes:\n",
    "    # Split the dataset\n",
    "    train_data = training_data[:train_size]\n",
    "    eval_data = training_data[train_size:train_size + int(0.2 * train_size)]  # 20% of training data for evaluation\n",
    "\n",
    "    # Tokenize datasets\n",
    "    train_tokenized_data = [tokenize_function(sample) for sample in train_data]\n",
    "    eval_tokenized_data = [tokenize_function(sample) for sample in eval_data]\n",
    "\n",
    "    # Convert tokenized data to Dataset format\n",
    "    train_dataset = Dataset.from_dict({\n",
    "        \"input_ids\": [x[\"input_ids\"][0] for x in train_tokenized_data],\n",
    "        \"attention_mask\": [x[\"attention_mask\"][0] for x in train_tokenized_data],\n",
    "        \"labels\": [x[\"labels\"][0] for x in train_tokenized_data]\n",
    "    })\n",
    "    eval_dataset = Dataset.from_dict({\n",
    "        \"input_ids\": [x[\"input_ids\"][0] for x in eval_tokenized_data],\n",
    "        \"attention_mask\": [x[\"attention_mask\"][0] for x in eval_tokenized_data],\n",
    "        \"labels\": [x[\"labels\"][0] for x in eval_tokenized_data]\n",
    "    })\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"outputs/adapter_test_candidate_items_epoch_{QLORA_PARAMS['lora_num_epochs']}_{train_size}_chatGPT_data\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=QLORA_PARAMS['gradient_accumulation_steps'],\n",
    "        num_train_epochs=QLORA_PARAMS['lora_num_epochs'],\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=QLORA_PARAMS['lora_val_iterations'],\n",
    "        save_steps=QLORA_PARAMS['lora_val_iterations'],\n",
    "        logging_steps=10,\n",
    "        learning_rate=QLORA_PARAMS['lora_lr'],\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=\"none\",\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, padding=True)\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    # Clear GPU cache before training\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Start training\n",
    "    print(f\"Starting training with {train_size} samples.\")\n",
    "    trainer.train()\n",
    "    adapter_name = f\"adapter_test_candidate_items_epoch_{QLORA_PARAMS['lora_num_epochs']}_{train_size}_chatGPT_data\"\n",
    "    # Save the model and tokenizer in separate directories for each training size\n",
    "    model.save_pretrained(f\"outputs/{adapter_name}\")\n",
    "    tokenizer.save_pretrained(f\"outputs/{adapter_name}\")\n",
    "    print(f\"Model trained with {train_size} samples saved to outputs/{adapter_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEAUTY PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Loaded:\n",
      "Tokenizer Path: models/hf-frompretrained-download/meta-llama/Meta-Llama-3-8B-Instruct\n",
      "Model Path: models/hf-frompretrained-downloadmeta-llama/Meta-Llama-3-8B-Instruct\n",
      "Pipeline Parameters: {'max_length': 2048, 'num_return_sequences': 1, 'temperature': 0.7, 'top_k': 50, 'top_p': 0.95, 'repetition_penalty': 1.2}\n",
      "QLoRA Parameters: {'lora_r': 8, 'lora_alpha': 8, 'lora_dropout': 0.01, 'lora_target_modules': ['q_proj', 'v_proj'], 'gradient_accumulation_steps': 2, 'lora_num_epochs': 2, 'lora_val_iterations': 100, 'lora_early_stopping_patience': 10, 'lora_lr': 0.0001, 'lora_micro_batch_size': 1}\n",
      "Prompt Template: {'instruction': \"### Instruction:\\n You are a recommender system specialized. Based on the following user profile text, generate a list of general candidate item categories that align with the user's preferences and interests. Approach this task by treating these categories as a cohesive set, ensuring that they collectively reflect the user’s overall profile and maximize satisfaction. \", 'input': '### Input \\n User Profile: \\n {user_profile}', 'output': '### Response:'}\n",
      "Data Structure Verification:\n",
      "Data verification successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\quantizers\\auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Sample: ### Instruction:\n",
      "### Instruction:\n",
      " You are a recommender system specialized. Based on the following user profile text, generate a list of general candidate item categories that align with the user's preferences and interests. Approach this task by treating these categories as a cohesive set, ensuring that they collectively reflect the user’s overall profile and maximize satisfaction. \n",
      "\n",
      "### Input \n",
      " User Profile: \n",
      " \"Short-Term Interests\": Based on the provided review sequence, we can identify some immediate trends and preferences:\n",
      "* Interest in hydrating products, particularly those focused on addressing dull, tired, dry, and sensitive skin types.\n",
      "* Preference for serums and oils that offer moisturizing benefits without leaving behind residue.\n",
      "* Appreciation for gentle, non-drying textures in cleansing products.\n",
      "* Desire for exfoliating and purifying properties in daily skincare routines.\n",
      "\"Long-Term Preferences\": Analyzing the user's entire history reveals deeper, more enduring interests:\n",
      "* Consistent interest in natural ingredients, evident through repeated mentions of \"natural\" brands and products containing kaolin clay, manuka honey, and ceramides.\n",
      "* History suggests a focus on anti-aging concerns, demonstrated by frequent exploration of retinol-based treatments and collagen-boosting formulas.\n",
      "* Stable preference for premium quality, reflected in willingness to invest in higher-priced products offering unique features and benefits.\n",
      "* Overall emphasis on achieving healthy, glowing, and youthful-looking skin, even at the expense of convenience or affordability.\n",
      "\"User_Profile\": This profile captures the essence of our user's persistent preferences while incorporating the latest interests and desires:\n",
      "Our user is an individual who prioritizes maintaining radiant, healthy skin. They favor natural ingredient-driven products that cater specifically to sensitive skin types and exhibit strong anti-aging concerns. Within their preferred range, they tend towards premium-quality solutions emphasizing hydration, gentle textures, and exfoliative properties. Recent experiences suggest increased enthusiasm for advanced technologies and innovative formulations capable of delivering exceptional results. Expectations remain high regarding their next purchase, which will likely involve exploring cutting-edge options addressing specific needs within their established repertoire. Their commitment to optimal skincare practices drives their decision-making process, ensuring ongoing satisfaction and loyalty toward trusted brands and products aligning with their core values.\n",
      "\n",
      "### Response:\n",
      "Candidate Item Categories:\n",
      "1. Natural Ingredient-Based Skin Treatments\n",
      "2. Premium Hydrating Moisturizers\n",
      "3. Gentle Exfoliants and Purifiers\n",
      "4. Advanced Anti-Aging Formulations\n",
      "5. Sensitive Skin-Friendly Cleansers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 64 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      " 16%|█▌        | 10/64 [00:47<04:16,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.4427, 'grad_norm': nan, 'learning_rate': 8.90625e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 20/64 [01:35<03:28,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.6316, 'grad_norm': 17.751296997070312, 'learning_rate': 7.34375e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 30/64 [02:22<02:41,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0807, 'grad_norm': 5.407251358032227, 'learning_rate': 5.9375e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 40/64 [03:09<01:53,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5756, 'grad_norm': 0.3465035557746887, 'learning_rate': 4.375e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 50/64 [03:57<01:06,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5287, 'grad_norm': 0.24732546508312225, 'learning_rate': 2.8125000000000003e-05, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 60/64 [04:44<00:18,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5075, 'grad_norm': 0.2507157325744629, 'learning_rate': 1.25e-05, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [05:03<00:00,  4.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 303.5524, 'train_samples_per_second': 0.422, 'train_steps_per_second': 0.211, 'train_loss': 2.807116910815239, 'epoch': 2.0}\n",
      "Model trained with 64 samples saved to outputs/adapter_test_candidate_items_epoch_2_64_pipeline_data\n"
     ]
    }
   ],
   "source": [
    "#STEP 1\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "\n",
    "# Set environment variable for memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Load configuration settings\n",
    "from config import TOKENIZER_PATH, MODEL_PATH, PIPELINE_PARAMS, QLORA_PARAMS, ALPACA_LORA_PROMPTS_CANDIDATE_ITEMS\n",
    "\n",
    "# Verification\n",
    "print(\"Configuration Loaded:\")\n",
    "print(\"Tokenizer Path:\", TOKENIZER_PATH)\n",
    "print(\"Model Path:\", MODEL_PATH)\n",
    "print(\"Pipeline Parameters:\", PIPELINE_PARAMS)\n",
    "print(\"QLoRA Parameters:\", QLORA_PARAMS)\n",
    "print(\"Prompt Template:\", ALPACA_LORA_PROMPTS_CANDIDATE_ITEMS)\n",
    "\n",
    "# STEP 2\n",
    "\n",
    "# Load the training data\n",
    "data_path = \"QLoRa_finetuning/updated_matching_ids_pipeline.json\"\n",
    "with open(data_path, \"r\") as file:\n",
    "    training_data = json.load(file)\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Verify data structure\n",
    "print(\"Data Structure Verification:\")\n",
    "for i, sample in enumerate(training_data[:2]):\n",
    "    assert \"User_ID\" in sample, f\"User_ID missing in sample {i}\"\n",
    "    assert \"User_Profile\" in sample, f\"User_Profile missing in sample {i}\"\n",
    "    assert \"Candidate_Items\" in sample, f\"Candidate_Items missing in sample {i}\"\n",
    "print(\"Data verification successful!\")\n",
    "\n",
    "# STEP 3\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS if not already set\n",
    "\n",
    "# Set 4-bit quantization configuration for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',  # Use NormalFloat4 for better memory efficiency\n",
    "    bnb_4bit_use_double_quant=True  # Double quantization for more memory saving\n",
    ")\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"  # Automatically maps layers to available GPU memory\n",
    ")\n",
    "\n",
    "# Step 4\n",
    "# Preprocess function to format the data for candidate item generation\n",
    "def preprocess_function(profile_sample):\n",
    "    # Use the user profile as input\n",
    "    input_text = ALPACA_LORA_PROMPTS_CANDIDATE_ITEMS['input'].replace(\n",
    "        \"{user_profile}\", profile_sample[\"User_Profile\"]\n",
    "    )\n",
    "    \n",
    "    # Set up the output text (Candidate Items) as the expected response\n",
    "    output_text = \"\\n\".join(\n",
    "        [f\"{i + 1}. {item}\" for i, item in enumerate(profile_sample[\"Candidate_Items\"].values())]\n",
    "    )\n",
    "    \n",
    "    # Format the complete prompt for training\n",
    "    full_text = f\"### Instruction:\\n{ALPACA_LORA_PROMPTS_CANDIDATE_ITEMS['instruction']}\\n\\n{input_text}\\n\\n### Response:\\nCandidate Item Categories:\\n{output_text}\"\n",
    "    return full_text\n",
    "\n",
    "# Verify preprocessing\n",
    "print(\"Preprocessed Sample:\", preprocess_function(training_data[10]))\n",
    "# Step 5\n",
    "# Tokenization function\n",
    "def tokenize_function(sample):\n",
    "    processed_text = preprocess_function(sample)\n",
    "    tokenized = tokenizer(\n",
    "        processed_text,\n",
    "        truncation=True,\n",
    "        max_length=PIPELINE_PARAMS['max_length'],\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # Set labels identical to input_ids\n",
    "    return tokenized\n",
    "\n",
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=QLORA_PARAMS['lora_r'],\n",
    "    lora_alpha=QLORA_PARAMS['lora_alpha'],\n",
    "    lora_dropout=QLORA_PARAMS['lora_dropout'],\n",
    "    target_modules=QLORA_PARAMS['lora_target_modules'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Step 6\n",
    "\n",
    "# Apply LoRA configuration to the model\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.gradient_checkpointing_enable()\n",
    "model = get_peft_model(model, lora_config)\n",
    "# Training sizes\n",
    "training_sizes = [64]\n",
    "\n",
    "# Loop through different training sizes\n",
    "for train_size in training_sizes:\n",
    "    # Split the dataset\n",
    "    train_data = training_data[:train_size]\n",
    "    eval_data = training_data[train_size:train_size + int(0.2 * train_size)]  # 20% of training data for evaluation\n",
    "\n",
    "    # Tokenize datasets\n",
    "    train_tokenized_data = [tokenize_function(sample) for sample in train_data]\n",
    "    eval_tokenized_data = [tokenize_function(sample) for sample in eval_data]\n",
    "\n",
    "    # Convert tokenized data to Dataset format\n",
    "    train_dataset = Dataset.from_dict({\n",
    "        \"input_ids\": [x[\"input_ids\"][0] for x in train_tokenized_data],\n",
    "        \"attention_mask\": [x[\"attention_mask\"][0] for x in train_tokenized_data],\n",
    "        \"labels\": [x[\"labels\"][0] for x in train_tokenized_data]\n",
    "    })\n",
    "    eval_dataset = Dataset.from_dict({\n",
    "        \"input_ids\": [x[\"input_ids\"][0] for x in eval_tokenized_data],\n",
    "        \"attention_mask\": [x[\"attention_mask\"][0] for x in eval_tokenized_data],\n",
    "        \"labels\": [x[\"labels\"][0] for x in eval_tokenized_data]\n",
    "    })\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"outputs/adapter_test_candidate_items_epoch_{QLORA_PARAMS['lora_num_epochs']}_{train_size}_pipeline_data\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=QLORA_PARAMS['gradient_accumulation_steps'],\n",
    "        num_train_epochs=QLORA_PARAMS['lora_num_epochs'],\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=QLORA_PARAMS['lora_val_iterations'],\n",
    "        save_steps=QLORA_PARAMS['lora_val_iterations'],\n",
    "        logging_steps=10,\n",
    "        learning_rate=QLORA_PARAMS['lora_lr'],\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=\"none\",\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, padding=True)\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    # Clear GPU cache before training\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Start training\n",
    "    print(f\"Starting training with {train_size} samples.\")\n",
    "    trainer.train()\n",
    "    adapter_name = f\"adapter_test_candidate_items_epoch_{QLORA_PARAMS['lora_num_epochs']}_{train_size}_pipeline_data\"\n",
    "    # Save the model and tokenizer in separate directories for each training size\n",
    "    model.save_pretrained(f\"outputs/{adapter_name}\")\n",
    "    tokenizer.save_pretrained(f\"outputs/{adapter_name}\")\n",
    "    print(f\"Model trained with {train_size} samples saved to outputs/{adapter_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIDEO GAMES CHATGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Loaded:\n",
      "Tokenizer Path: models/hf-frompretrained-download/meta-llama/Meta-Llama-3-8B-Instruct\n",
      "Model Path: models/hf-frompretrained-downloadmeta-llama/Meta-Llama-3-8B-Instruct\n",
      "Pipeline Parameters: {'max_length': 2048, 'num_return_sequences': 1, 'temperature': 0.7, 'top_k': 50, 'top_p': 0.95, 'repetition_penalty': 1.2}\n",
      "QLoRA Parameters: {'lora_r': 8, 'lora_alpha': 8, 'lora_dropout': 0.01, 'lora_target_modules': ['q_proj', 'v_proj'], 'gradient_accumulation_steps': 2, 'lora_num_epochs': 2, 'lora_val_iterations': 100, 'lora_early_stopping_patience': 10, 'lora_lr': 0.0001, 'lora_micro_batch_size': 1}\n",
      "Prompt Template: {'instruction': \"### Instruction:\\n You are a recommender system specialized. Based on the following user profile text, generate a list of general candidate item categories that align with the user's preferences and interests. Approach this task by treating these categories as a cohesive set, ensuring that they collectively reflect the user’s overall profile and maximize satisfaction. \", 'input': '### Input \\n User Profile: \\n {user_profile}', 'output': '### Response:'}\n",
      "Data Structure Verification:\n",
      "Data verification successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\quantizers\\auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Sample: ### Instruction:\n",
      "### Instruction:\n",
      " You are a recommender system specialized. Based on the following user profile text, generate a list of general candidate item categories that align with the user's preferences and interests. Approach this task by treating these categories as a cohesive set, ensuring that they collectively reflect the user’s overall profile and maximize satisfaction. \n",
      "\n",
      "### Input \n",
      " User Profile: \n",
      " \"Short-Term Interests\": The user has recently engaged with a variety of gaming peripherals—most notably keyboards, clicky switches, headsets, and word-based board games. They pay close attention to how products ‘feel’ (key switches, comfort levels) and whether instructions or compatibility details are clear.\n",
      "\"Long-Term Preferences\": An analysis of the user’s reviews reveals consistent themes:\n",
      "* Highly values how a keyboard or headset feels and performs during longer sessions\n",
      "* Looks for clarity in instructions and easy-to-use function keys or hotkeys\n",
      "* Appreciates adaptability or ‘hackability’ in games for different age ranges\n",
      "* Prefers well-constructed products, even at budget price points\n",
      "\"User_Profile\": The user is a moderately tech-savvy gamer who’s mindful of comfort, product build quality, and clear instructions. They also enjoy family-friendly or flexible tabletop/board-style games that can be tailored to various skill levels. They seek PC peripherals with good build quality, comfort, and thorough documentation for quick set-up or custom macro functionality.\n",
      "\n",
      "### Response:\n",
      "Candidate Item Categories:\n",
      "1. Compact Hotswap 65% Mechanical Keyboard with Rotary Knob\n",
      "2. Modular Gaming Headset with Swappable Ear Cushions\n",
      "3. Magnetic Keyboard Wrist Rest with Adjustable Height\n",
      "4. LED Wireless Mouse with Multi-Device Pairing\n",
      "5. Modular Board Game with Word and Trivia Expansion Packs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 16 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      " 62%|██████▎   | 10/16 [00:47<00:28,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.0916, 'grad_norm': 9.3617525100708, 'learning_rate': 5.6250000000000005e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:16<00:00,  4.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 76.2063, 'train_samples_per_second': 0.42, 'train_steps_per_second': 0.21, 'train_loss': 9.397164583206177, 'epoch': 2.0}\n",
      "Model trained with 16 samples saved to outputs/adapter_test_candidate_items_epoch_2_16_chatgpt_data_video_games\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 32 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 10/32 [00:47<01:44,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.0998, 'grad_norm': 15.630152702331543, 'learning_rate': 7.8125e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 20/32 [01:35<00:57,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8654, 'grad_norm': 4.487438201904297, 'learning_rate': 4.6875e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 30/32 [02:22<00:09,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5734, 'grad_norm': 0.3956744372844696, 'learning_rate': 1.5625e-05, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [02:32<00:00,  4.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 152.1393, 'train_samples_per_second': 0.421, 'train_steps_per_second': 0.21, 'train_loss': 2.7007004395127296, 'epoch': 2.0}\n",
      "Model trained with 32 samples saved to outputs/adapter_test_candidate_items_epoch_2_32_chatgpt_data_video_games\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 64 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 10/64 [00:47<04:16,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5299, 'grad_norm': 0.20251959562301636, 'learning_rate': 8.4375e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 20/64 [01:35<03:29,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.481, 'grad_norm': 0.1976896971464157, 'learning_rate': 6.875e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 30/64 [02:22<02:41,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4299, 'grad_norm': 0.24011029303073883, 'learning_rate': 5.3125000000000004e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 40/64 [03:09<01:53,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.389, 'grad_norm': 0.27251380681991577, 'learning_rate': 3.7500000000000003e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 50/64 [03:57<01:06,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3725, 'grad_norm': 0.33729416131973267, 'learning_rate': 2.1875e-05, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 60/64 [04:44<00:19,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3438, 'grad_norm': 0.30028724670410156, 'learning_rate': 6.25e-06, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [05:04<00:00,  4.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 304.2866, 'train_samples_per_second': 0.421, 'train_steps_per_second': 0.21, 'train_loss': 0.4190325606614351, 'epoch': 2.0}\n",
      "Model trained with 64 samples saved to outputs/adapter_test_candidate_items_epoch_2_64_chatgpt_data_video_games\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#STEP 1\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "\n",
    "# Set environment variable for memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Load configuration settings\n",
    "from config import TOKENIZER_PATH, MODEL_PATH, PIPELINE_PARAMS, QLORA_PARAMS, ALPACA_LORA_PROMPTS_CANDIDATE_ITEMS\n",
    "\n",
    "# Verification\n",
    "print(\"Configuration Loaded:\")\n",
    "print(\"Tokenizer Path:\", TOKENIZER_PATH)\n",
    "print(\"Model Path:\", MODEL_PATH)\n",
    "print(\"Pipeline Parameters:\", PIPELINE_PARAMS)\n",
    "print(\"QLoRA Parameters:\", QLORA_PARAMS)\n",
    "print(\"Prompt Template:\", ALPACA_LORA_PROMPTS_CANDIDATE_ITEMS)\n",
    "\n",
    "# STEP 2\n",
    "\n",
    "# Load the training data\n",
    "data_path = \"QLoRa_finetuning/chatGPT_UP_output_video_games.json\"\n",
    "with open(data_path, \"r\") as file:\n",
    "    training_data = json.load(file)\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Verify data structure\n",
    "print(\"Data Structure Verification:\")\n",
    "for i, sample in enumerate(training_data[:2]):\n",
    "    assert \"User_ID\" in sample, f\"User_ID missing in sample {i}\"\n",
    "    assert \"User_Profile\" in sample, f\"User_Profile missing in sample {i}\"\n",
    "    assert \"Candidate_Items\" in sample, f\"Candidate_Items missing in sample {i}\"\n",
    "print(\"Data verification successful!\")\n",
    "\n",
    "# STEP 3\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS if not already set\n",
    "\n",
    "# Set 4-bit quantization configuration for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',  # Use NormalFloat4 for better memory efficiency\n",
    "    bnb_4bit_use_double_quant=True  # Double quantization for more memory saving\n",
    ")\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"  # Automatically maps layers to available GPU memory\n",
    ")\n",
    "\n",
    "# Step 4\n",
    "# Preprocess function to format the data for candidate item generation\n",
    "def preprocess_function(profile_sample):\n",
    "    # Use the user profile as input\n",
    "    input_text = ALPACA_LORA_PROMPTS_CANDIDATE_ITEMS['input'].replace(\n",
    "        \"{user_profile}\", profile_sample[\"User_Profile\"]\n",
    "    )\n",
    "    \n",
    "    # Set up the output text (Candidate Items) as the expected response\n",
    "    output_text = \"\\n\".join(\n",
    "        [f\"{i + 1}. {item}\" for i, item in enumerate(profile_sample[\"Candidate_Items\"].values())]\n",
    "    )\n",
    "    \n",
    "    # Format the complete prompt for training\n",
    "    full_text = f\"### Instruction:\\n{ALPACA_LORA_PROMPTS_CANDIDATE_ITEMS['instruction']}\\n\\n{input_text}\\n\\n### Response:\\nCandidate Item Categories:\\n{output_text}\"\n",
    "    return full_text\n",
    "\n",
    "# Verify preprocessing\n",
    "print(\"Preprocessed Sample:\", preprocess_function(training_data[10]))\n",
    "# Step 5\n",
    "# Tokenization function\n",
    "def tokenize_function(sample):\n",
    "    processed_text = preprocess_function(sample)\n",
    "    tokenized = tokenizer(\n",
    "        processed_text,\n",
    "        truncation=True,\n",
    "        max_length=PIPELINE_PARAMS['max_length'],\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # Set labels identical to input_ids\n",
    "    return tokenized\n",
    "\n",
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=QLORA_PARAMS['lora_r'],\n",
    "    lora_alpha=QLORA_PARAMS['lora_alpha'],\n",
    "    lora_dropout=QLORA_PARAMS['lora_dropout'],\n",
    "    target_modules=QLORA_PARAMS['lora_target_modules'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Step 6\n",
    "\n",
    "# Apply LoRA configuration to the model\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.gradient_checkpointing_enable()\n",
    "model = get_peft_model(model, lora_config)\n",
    "# Training sizes\n",
    "training_sizes = [16,32,64]\n",
    "\n",
    "# Loop through different training sizes\n",
    "for train_size in training_sizes:\n",
    "    # Split the dataset\n",
    "    train_data = training_data[:train_size]\n",
    "    eval_data = training_data[train_size:train_size + int(0.2 * train_size)]  # 20% of training data for evaluation\n",
    "\n",
    "    # Tokenize datasets\n",
    "    train_tokenized_data = [tokenize_function(sample) for sample in train_data]\n",
    "    eval_tokenized_data = [tokenize_function(sample) for sample in eval_data]\n",
    "\n",
    "    # Convert tokenized data to Dataset format\n",
    "    train_dataset = Dataset.from_dict({\n",
    "        \"input_ids\": [x[\"input_ids\"][0] for x in train_tokenized_data],\n",
    "        \"attention_mask\": [x[\"attention_mask\"][0] for x in train_tokenized_data],\n",
    "        \"labels\": [x[\"labels\"][0] for x in train_tokenized_data]\n",
    "    })\n",
    "    eval_dataset = Dataset.from_dict({\n",
    "        \"input_ids\": [x[\"input_ids\"][0] for x in eval_tokenized_data],\n",
    "        \"attention_mask\": [x[\"attention_mask\"][0] for x in eval_tokenized_data],\n",
    "        \"labels\": [x[\"labels\"][0] for x in eval_tokenized_data]\n",
    "    })\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"outputs/adapter_test_candidate_items_epoch_{QLORA_PARAMS['lora_num_epochs']}_{train_size}_chatgpt_data_video_games\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=QLORA_PARAMS['gradient_accumulation_steps'],\n",
    "        num_train_epochs=QLORA_PARAMS['lora_num_epochs'],\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=QLORA_PARAMS['lora_val_iterations'],\n",
    "        save_steps=QLORA_PARAMS['lora_val_iterations'],\n",
    "        logging_steps=10,\n",
    "        learning_rate=QLORA_PARAMS['lora_lr'],\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=\"none\",\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, padding=True)\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    # Clear GPU cache before training\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Start training\n",
    "    print(f\"Starting training with {train_size} samples.\")\n",
    "    trainer.train()\n",
    "    adapter_name = f\"adapter_test_candidate_items_epoch_{QLORA_PARAMS['lora_num_epochs']}_{train_size}_chatgpt_data_video_games\"\n",
    "    # Save the model and tokenizer in separate directories for each training size\n",
    "    model.save_pretrained(f\"outputs/{adapter_name}\")\n",
    "    tokenizer.save_pretrained(f\"outputs/{adapter_name}\")\n",
    "    print(f\"Model trained with {train_size} samples saved to outputs/{adapter_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]\n",
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\peft\\utils\\save_and_load.py:198: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adapters_weights = torch.load(filename, map_location=torch.device(device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Candidate Items:\n",
      "### Instruction:\n",
      " You are a recommender system specialized. Based on the following user profile text, generate a list of general candidate item categories that align with the user's preferences and interests. Approach this task by treating these categories as a cohesive set, ensuring that they collectively reflect the user’s overall profile and maximize satisfaction. \n",
      "\n",
      "### Input \n",
      " User Profile: \n",
      " \n",
      "- Short-term Intentions: Looking for high-quality tech accessories.\n",
      "- Long-term Preferences: Prefers durable, high-performance gadgets.\n",
      "- User Profile: The users likes technical stuff with preferences for windows laptops\n",
      "\n",
      "### Response:\n",
      "Technical Accessories (Headphones, Tablet Stylus)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from config import *\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "TOKENIZER_PATH = \"models/hf-frompretrained-download/meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "MODEL_PATH = \"models/hf-frompretrained-downloadmeta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# Load the base model and tokenizer\n",
    "base_model_path =\"models/hf-frompretrained-downloadmeta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=False)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    #quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    #torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Load the adapter\n",
    "adapter_path = f\"outputs/adapter_test_candidate_items_epoch_{QLORA_PARAMS['lora_num_epochs']}_{train_size}_chatGPT_data\"\n",
    "adapter_name = \"candidate_items\"\n",
    "model = PeftModel.from_pretrained(model, adapter_path, adapter_name=adapter_name)\n",
    "\n",
    "# Set the active adapter\n",
    "model.set_adapter(adapter_name)\n",
    "model.eval()\n",
    "\n",
    "# Define the text generation function\n",
    "def generate_text(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=PIPELINE_PARAMS['max_length'],\n",
    "        do_sample=True,\n",
    "        temperature=PIPELINE_PARAMS['temperature'],\n",
    "        top_k=PIPELINE_PARAMS['top_k'],\n",
    "        top_p=PIPELINE_PARAMS['top_p'],\n",
    "        repetition_penalty=PIPELINE_PARAMS['repetition_penalty'],\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Define a sample user profile input for testing\n",
    "user_profile = \"\"\"\n",
    "- Short-term Intentions: Looking for high-quality tech accessories.\n",
    "- Long-term Preferences: Prefers durable, high-performance gadgets.\n",
    "- User Profile: The users likes technical stuff with preferences for windows laptops\n",
    "\"\"\"\n",
    "\n",
    "# Format the prompt\n",
    "prompt = (\n",
    "    ALPACA_LORA_PROMPTS_CANDIDATE_ITEMS['instruction'] + \"\\n\\n\" +\n",
    "    ALPACA_LORA_PROMPTS_CANDIDATE_ITEMS['input'].replace(\"{user_profile}\", user_profile)+ \"\\n\" + \"### Response\"\n",
    ")\n",
    "\n",
    "# Generate candidate items using the model\n",
    "generated_text = generate_text(prompt)\n",
    "\n",
    "# Display the output\n",
    "print(\"Generated Candidate Items:\")\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
