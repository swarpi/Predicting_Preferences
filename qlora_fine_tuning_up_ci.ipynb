{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import necessary modules and load configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Loaded:\n",
      "Tokenizer Path: models/hf-frompretrained-download/meta-llama/Meta-Llama-3-8B-Instruct\n",
      "Model Path: models/hf-frompretrained-downloadmeta-llama/Meta-Llama-3-8B-Instruct\n",
      "Pipeline Parameters: {'max_length': 2048, 'num_return_sequences': 1, 'temperature': 0.7, 'top_k': 50, 'top_p': 0.95, 'repetition_penalty': 1.2}\n",
      "QLoRA Parameters: {'lora_r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'lora_target_modules': ['q_proj', 'v_proj'], 'gradient_accumulation_steps': 2, 'lora_num_epochs': 2, 'lora_val_iterations': 100, 'lora_early_stopping_patience': 10, 'lora_lr': 0.0001, 'lora_micro_batch_size': 1}\n",
      "Prompt Template: {'instruction': \"### Instruction:\\n You are a recommender system specialized in creating user profiles. Your task is to create a comprehensive user profile and a list of candidate items based on a user's review. The user Profile consists of: 1. Short-term Interests: Examine the user's most recent items along with their personalized descriptions 2. Long-term Preferences: Analyze all reviews and items from the user’s entire history to capture deeper, stable interests that define the user’s lasting preferences. Use this comprehensive historical data to outline consistent themes and inclinations that have remained steady over time. 3. User Profile Summary: Synthesize these findings into a concise profile (max 200 words) that combines insights from both short-term interests and long-term preferences. Use the short-term interests as a query to refine or highlight relevant themes from the long-term history, and provide a cohesive picture of the user’s tastes, typical habits, and potential future interests. Afterwards generate five Candidate Items that are general product categories that align with the user's preferences and interests. Approach this task by treating these categories as a cohesive set, ensuring that they collectively reflect the user’s overall profile and maximize satisfaction. \", 'input': 'User Reviews: {user_review}', 'output': '### Response:'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# Load configuration settings\n",
    "from config import TOKENIZER_PATH, MODEL_PATH, PIPELINE_PARAMS, QLORA_PARAMS, ALPACA_LORA_PROMPTS_USER_PROFILE_AND_CANDIDATE_ITEMS\n",
    "from utils import *\n",
    "\n",
    "# Verification\n",
    "print(\"Configuration Loaded:\")\n",
    "print(\"Tokenizer Path:\", TOKENIZER_PATH)\n",
    "print(\"Model Path:\", MODEL_PATH)\n",
    "print(\"Pipeline Parameters:\", PIPELINE_PARAMS)\n",
    "print(\"QLoRA Parameters:\", QLORA_PARAMS)\n",
    "print(\"Prompt Template:\", ALPACA_LORA_PROMPTS_USER_PROFILE_AND_CANDIDATE_ITEMS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and verify training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Sample: [{'User_ID': 'AFQQQ5LGNSQUEBGDCYBAZZE5T3DA', 'User_Profile': '\"Short-Term Interests\": The user recently reviewed products focused on facial skincare, specifically creams and serums targeting anti-aging concerns such as fine lines, wrinkles, and dehydration. They showed interest in natural and organic ingredients, vegan-friendly options, and cruelty-free practices. Their preference leans towards face washes and moisturizers that cater to specific skin concerns like acne-prone, sensitive, and dry skin.\\n\"Long-Term Preferences\": Based on the entirety of their reviews, some common denominators emerge:\\n* Focus on natural ingredients, particularly plant-based extracts, essential oils, and herbal remedies\\n* Concern for anti-aging and rejuvenation, seeking effective solutions for maintaining healthy-looking skin\\n* Interest in sheet masks, especially those with unique features like eye patches and targeted application areas\\n* Appreciation for moisturized and hydrated skin, often mentioning specific requirements like deep hydration and non-sticky textures\\n* Willingness to explore various brands and products, demonstrating adaptability and open-mindedness\\n\"User_Profile\": Our user appears to be someone who prioritizes natural and organic approaches to skincare while focusing on addressing specific skin concerns. They exhibit a willingness to experiment with diverse products and techniques to achieve optimal results. As they continue exploring the world of skincare, they may gravitate toward more niche markets, such as bespoke formulas tailored to individual skin types or advanced technology-driven treatments. For now, our recommendation would focus on recommending products featuring natural, sustainable, and innovative formulations catering to their varied skin needs, including hydration, anti-aging, and sensitivity management. A suggested next step could involve introducing them to emerging trends in customized skincare regimens and cutting-edge technologies for enhanced customer experience.', 'Candidate_Items': {'1': 'Nourishing Skin Serenity', '2': 'Botanical Beauty Discovery', '3': 'Hydrating Harmony Regimen', '4': 'Natural Radiance Revival', '5': 'Advanced Anti-Aging Adaptations'}}, {'User_ID': 'AEE4M36AZAKURLEYGV23TM3BE7OQ', 'User_Profile': '\"Short-Term Interests\": The user has recently engaged with hair care products focused on moisturizing and detangling, such as shampoos and conditioners with natural oils, hair towels, and convenient towelettes.\\n\"Long-Term Preferences\": An analysis of the user\\'s reviews reveals consistent themes:\\n* Preference for moisturizing hair products that leave fine, shoulder-length hair shiny and bouncy without greasiness\\n* Interest in products that help with detangling, reducing frizz, and minimizing static electricity\\n* Appreciation for high-quality hair accessories like twist towels and soft hair towels\\n* Values convenience items for on-the-go use, such as individually packaged towelettes\\n* Preference for natural ingredients like coconut oil and avocado oil in hair care\\n\"User_Profile\": The user appears to prioritize hair care and values products that enhance the health and appearance of their fine, shoulder-length hair. They seek moisturizing shampoos and conditioners that provide shine and manageability without weighing hair down. The user appreciates effective detangling solutions and products that combat frizz and static. They value quality hair accessories and convenient personal care items, showing a preference for natural ingredients in their hair care routine.', 'Candidate_Items': {'1': 'Anti-Frizz Hair Serums', '2': 'Leave-In Conditioners with Natural Oils', '3': 'Detangling Hair Brushes', '4': 'Travel-Friendly Dry Shampoos', '5': 'Silk Pillowcases for Hair Care'}}]\n",
      "Data Structure Verification:\n",
      "Data verification successful!\n"
     ]
    }
   ],
   "source": [
    "data_path = \"QLoRa_finetuning/matching_ids_chatGPT.json\"\n",
    "\n",
    "# Load the training data\n",
    "with open(data_path, \"r\") as file:\n",
    "    training_data = json.load(file)\n",
    "\n",
    "# Sample a couple of data points to verify format\n",
    "print(\"Training Data Sample:\", training_data[:2])  # Display first two entries\n",
    "# Clear cache before loading model\n",
    "#torch.cuda.empty_cache()\n",
    "# Verify data structure\n",
    "print(\"Data Structure Verification:\")\n",
    "for i, sample in enumerate(training_data[:2]):\n",
    "    assert \"User_ID\" in sample, f\"User_ID missing in sample {i}\"\n",
    "    assert \"User_Profile\" in sample, f\"User_Profile missing in sample {i}\"\n",
    "    assert \"Candidate_Items\" in sample, f\"Candidate_Items missing in sample {i}\"\n",
    "print(\"Data verification successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Data Sample: [{'User_ID': 'AFQQQ5LGNSQUEBGDCYBAZZE5T3DA', 'User_Profile': '\"Short-Term Interests\": The user recently reviewed products focused on facial skincare, specifically creams and serums targeting anti-aging concerns such as fine lines, wrinkles, and dehydration. They showed interest in natural and organic ingredients, vegan-friendly options, and cruelty-free practices. Their preference leans towards face washes and moisturizers that cater to specific skin concerns like acne-prone, sensitive, and dry skin.\\n\"Long-Term Preferences\": Based on the entirety of their reviews, some common denominators emerge:\\n* Focus on natural ingredients, particularly plant-based extracts, essential oils, and herbal remedies\\n* Concern for anti-aging and rejuvenation, seeking effective solutions for maintaining healthy-looking skin\\n* Interest in sheet masks, especially those with unique features like eye patches and targeted application areas\\n* Appreciation for moisturized and hydrated skin, often mentioning specific requirements like deep hydration and non-sticky textures\\n* Willingness to explore various brands and products, demonstrating adaptability and open-mindedness\\n\"User_Profile\": Our user appears to be someone who prioritizes natural and organic approaches to skincare while focusing on addressing specific skin concerns. They exhibit a willingness to experiment with diverse products and techniques to achieve optimal results. As they continue exploring the world of skincare, they may gravitate toward more niche markets, such as bespoke formulas tailored to individual skin types or advanced technology-driven treatments. For now, our recommendation would focus on recommending products featuring natural, sustainable, and innovative formulations catering to their varied skin needs, including hydration, anti-aging, and sensitivity management. A suggested next step could involve introducing them to emerging trends in customized skincare regimens and cutting-edge technologies for enhanced customer experience.', 'Candidate_Items': {'1': 'Nourishing Skin Serenity', '2': 'Botanical Beauty Discovery', '3': 'Hydrating Harmony Regimen', '4': 'Natural Radiance Revival', '5': 'Advanced Anti-Aging Adaptations'}}, {'User_ID': 'AFSHXT5PTGDSFW2725SDXIE6ZVEA', 'User_Profile': '\"Short-Term Interests\": Based on the latest review, we see that the user recently engaged with skincare products, specifically vitamin C foam cleansers. Their experience suggests that they value effectiveness in cleansing without strong scents, possibly indicating a preference for subtle fragrances or unscented products. Additionally, their interest in facial care might imply a concern for skin health and wellness.\\n\"Long-Term Preferences\": Our analysis reveals that this user tends towards trying new products with unique characteristics. For instance:\\n* In their early engagement with AG Care Natural Remedy Apple Cider Vinegar Leave-On Mist, they appreciated its distinct, fruity fragrance and connection to nature-based ingredients.\\n* When exploring perfumes through PINROSE Perfumes Bold Soul, they demonstrated curiosity about unusual aromas and willingness to experiment with different scents.\\n* While reviewing microfiber hair towels, their focus lay on functionality, comfort, and ease of use, suggesting practicality plays a significant role in their purchasing decisions.\\nFrom this historical context, we can infer that our user is drawn to unconventional products, enjoys discovering novel sensations, and values functional design. They likely prioritize quality materials, innovative features, and gentle, non-overpowering fragrances.\\n\"User_Profile\": This user exhibits a curious and adventurous approach to consumerism, often seeking experiences beyond mainstream choices. With a growing emphasis on self-care and personal grooming, they gravitate toward products offering effective solutions and attention-grabbing attributes. Our analysis indicates that they tend to appreciate subtlety in scents, preferring understated or unscented options. As they continue navigating diverse product offerings, we predict that their enthusiasm for exploration and discovery will remain a defining characteristic, driving them towards the next exciting find.', 'Candidate_Items': {'1': 'Experiential Wellness Treats', '2': 'Niche Fragrance Discoveries', '3': 'Functional Grooming Tools', '4': 'Unconventional Home Remedies', '5': 'Artisanal Skin & Body Care Sets'}}]\n",
    "Data Structure Verification:\n",
    "Data verification successful!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize the Tokenizer and Model with Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\quantizers\\auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS if not already set\n",
    "\n",
    "# Set 4-bit quantization configuration for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',  # Use NormalFloat4 for better memory efficiency\n",
    "    bnb_4bit_use_double_quant=True  # Double quantization for more memory saving\n",
    ")\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"  # Automatically maps layers to available GPU memory\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Preprocessing Function to Match Reviews with Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Sample: \n",
      "### Instruction:\n",
      " You are a recommender system specialized in creating user profiles. Your task is to create a comprehensive user profile and a list of candidate items based on a user's review. The user Profile consists of: 1. Short-term Interests: Examine the user's most recent items along with their personalized descriptions 2. Long-term Preferences: Analyze all reviews and items from the user’s entire history to capture deeper, stable interests that define the user’s lasting preferences. Use this comprehensive historical data to outline consistent themes and inclinations that have remained steady over time. 3. User Profile Summary: Synthesize these findings into a concise profile (max 200 words) that combines insights from both short-term interests and long-term preferences. Use the short-term interests as a query to refine or highlight relevant themes from the long-term history, and provide a cohesive picture of the user’s tastes, typical habits, and potential future interests. Afterwards generate five Candidate Items that are general product categories that align with the user's preferences and interests. Approach this task by treating these categories as a cohesive set, ensuring that they collectively reflect the user’s overall profile and maximize satisfaction. \n",
      "\n",
      "User reviews:\n",
      "Product: 13.5 fl. Oz. Goat Milk Facial Cleanser, Moisturizing Face Wash for Women, Hydrating Natural Face Wash, Anti-aging Face Wash, Face Wash for Aging Ski\n",
      "Rating: 5.0\n",
      "Title: Gentle & Natural\n",
      "Review: I'm trying to do a better job with paying as much attention to the ingredients in my skincare as those in my food. This is a great natural cleanser. It's very gentle and is good for sensitive skin. The lactic acid hydrates and gently exfoliates over time. You won't need a science degree to understand the majority of the ingredients. This is a great clean beauty cleanser that I highly recommend.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "Product: ABOUT ME MediAnswer Collagen Firming Up Mask 4ea (1box) 25g - 77% Pure Collagen Extract Anti-Aging Total Care Facial Mask Sheet, Powerful Hydrating and Anti-Wrinkle Night Skin Care\n",
      "Rating: 5.0\n",
      "Title: Effective Mask\n",
      "Review: If you're reading this review, you probably already know the anti-aging benefits of collagen. This mask has a high-potency form of collagen along with a special delivery system to help firm your skin. The mask is in two parts, so you can get a better fit than a regular sheet mask. I can't say it performs miracles, but it's definitely worth a try.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "Product: Queendom Unlashed Mascara | Volumizing and Lengthening | Boosts Lash Length | Vegan, Cruelty Free, Paraben Free\n",
      "Rating: 5.0\n",
      "Title: New Favorite!!\n",
      "Review: Every mascara claims to be new and different. This one actually lives up to the hype. The fibers have a novel shape that really makes your lashes POP. If you're not a fan of falsies, this is a great alternative. The formula isn't waterproof, so keep that in mind. I HIGHLY recommend this product.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "Product: Higher Education Skincare: Goal Digger - Silky Moisturizing Cucumber Creme; formulated for dry and sensitive skin; cucumber extract; natural extract: melon, kale, cabbage, ginger, turmeric - 1.7 fl oz\n",
      "Rating: 5.0\n",
      "Title: Lightweight and Creamy\n",
      "Review: I really like this moisturizer. It's lightweight and creamy. It has a light cucumber scent that I find refreshing. It's a powerhouse at deeply hydrating parched skin. It also locks in moisture to keep skin looking fresh. It contains botanicals that soothe irritated or inflamed skin. It's ideal for those with acne, rosacea or eczema. You can use it as both a day and night cream. I highly recommend this product.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "Product: Easydew 2-Step Face Contour Sheet Mask - Tightening & Anti-Aging & Hydrating Korean Face Mask with DW-EGF, Hyaluronic Acid for Anti-Winkle, Reducing Fine Line & Producing Collagen (Qty 5)\n",
      "Rating: 5.0\n",
      "Title: Special Care Mask\n",
      "Review: I really like that this mask includes special patches for the eyes and smile lines. If you're over 30, you'll appreciate the extra help. You apply the patches first, the the sheet mask. I keep all of my sheet masks in the refrigerator. You'll appreciate the cooling and anti-inflammatory effect if you do so with this mask. I highly recommend this product.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "Product: [Abib] Creme coating mask Tone-up solution 17g (5pcs)\n",
      "Rating: 5.0\n",
      "Title: Doesn't Leave Skin Sticky\n",
      "Review: I use lots of sheet masks. Most leave your skin a little sticky when the serum dries. This mask works differently. The sheet locks the moisture in and doesn't leave a sticky residue. It's a great hydration and brightening mask. I highly recommend it.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "Product: Dr. Denese SkinScience Essential Lipid Anti Aging Power Infusion Dry Oil - Skin Nutrients 97% Organic 100% Natural - Rejuvinating Blend with Amaranth & Resveratrol - Paraben-Free, Cruelty-Free - 2oz\n",
      "Rating: 5.0\n",
      "Title: Youth Serum\n",
      "Review: One of the reasons skin starts to wrinkle as we age is that we make less and less of the natural oil that keeps skin looking youthful. This serum is a good way to supplement what your skin lacks. The oils in this blend are all high quality. The texture is lightweight and easily absorbed. Dr. Denese is a trusted brand when it comes to care for aging skin. Whether you're trying to slow the clock or reverse it a little, this serum should be part of your daily skincare routine. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "Product: Moisturizing Facial Emulsion for Restoring Hydrating Smoothing Skin from Manilla Natural Skincare\n",
      "Rating: 5.0\n",
      "Title: Intense Hydration\n",
      "Review: With winter approaching, it's time to start thinking about cold-weather skincare. This moisturizer is a good choice because it's a blend of several skin-loving oils like argan, coconut and macadamia. It's fairly lightweight but it protects and hydrates like a heavier cream. It can be used both day and night. The packaging is pretty enough for display. It can be used by all skin types. I highly recommend this product and the entire line from this brand.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "\n",
      "\n",
      "### Response:\n",
      "User Profile:\n",
      "\"Short-Term Interests\": The user recently reviewed products focused on facial skincare, specifically creams and serums targeting anti-aging concerns such as fine lines, wrinkles, and dehydration. They showed interest in natural and organic ingredients, vegan-friendly options, and cruelty-free practices. Their preference leans towards face washes and moisturizers that cater to specific skin concerns like acne-prone, sensitive, and dry skin.\n",
      "\"Long-Term Preferences\": Based on the entirety of their reviews, some common denominators emerge:\n",
      "* Focus on natural ingredients, particularly plant-based extracts, essential oils, and herbal remedies\n",
      "* Concern for anti-aging and rejuvenation, seeking effective solutions for maintaining healthy-looking skin\n",
      "* Interest in sheet masks, especially those with unique features like eye patches and targeted application areas\n",
      "* Appreciation for moisturized and hydrated skin, often mentioning specific requirements like deep hydration and non-sticky textures\n",
      "* Willingness to explore various brands and products, demonstrating adaptability and open-mindedness\n",
      "\"User_Profile\": Our user appears to be someone who prioritizes natural and organic approaches to skincare while focusing on addressing specific skin concerns. They exhibit a willingness to experiment with diverse products and techniques to achieve optimal results. As they continue exploring the world of skincare, they may gravitate toward more niche markets, such as bespoke formulas tailored to individual skin types or advanced technology-driven treatments. For now, our recommendation would focus on recommending products featuring natural, sustainable, and innovative formulations catering to their varied skin needs, including hydration, anti-aging, and sensitivity management. A suggested next step could involve introducing them to emerging trends in customized skincare regimens and cutting-edge technologies for enhanced customer experience. \n",
      "Candidate Items:\n",
      "1. Nourishing Skin Serenity\n",
      "2. Botanical Beauty Discovery\n",
      "3. Hydrating Harmony Regimen\n",
      "4. Natural Radiance Revival\n",
      "5. Advanced Anti-Aging Adaptations \n"
     ]
    }
   ],
   "source": [
    "# Load user reviews\n",
    "reviews_path = \"new_data/new_train_output.json\"\n",
    "with open(reviews_path, \"r\") as file:\n",
    "    reviews_data = json.load(file)\n",
    "\n",
    "# Index reviews by user_id for easy matching\n",
    "reviews_by_user = {entry[\"user_id\"]: entry[\"reviews\"] for entry in reviews_data}\n",
    "\n",
    "# Preprocess function to format the data for fine-tuning\n",
    "def preprocess_function(profile_sample):\n",
    "    user_id = profile_sample[\"User_ID\"]\n",
    "    \n",
    "    # Retrieve and format user reviews as input\n",
    "    reviews = reviews_by_user.get(user_id, [])\n",
    "    sorted_reviews = sorted(reviews, key=lambda x: x['timestamp'])\n",
    "# Get the last 10 reviews\n",
    "    latest_reviews = sorted_reviews[-10:]\n",
    "    # Format the latest reviews\n",
    "    review_texts = [f\"Product: {review['product_name']}\\nRating: {review['rating']}\\nTitle: {review['title']}\\nReview: {review['text']}\\n\" \n",
    "                    for review in latest_reviews]\n",
    "\n",
    "    review_texts = review_texts[-10:]\n",
    "    formatted_reviews = \"\\n\".join(review_texts)\n",
    "    \n",
    "    # Instruction prompt\n",
    "    instruction = ALPACA_LORA_PROMPTS_USER_PROFILE_AND_CANDIDATE_ITEMS['instruction']\n",
    "    input_text = f\"User reviews:\\n{formatted_reviews}\" if formatted_reviews else \"No reviews available for this user.\"\n",
    "    output_alpaca = ALPACA_LORA_PROMPTS_USER_PROFILE_AND_CANDIDATE_ITEMS['output']\n",
    "    # Combine user profile and candidate items as the response\n",
    "    user_profile_text = profile_sample[\"User_Profile\"]\n",
    "    candidate_items_text = \"\\n\".join(\n",
    "        [f\"{i + 1}. {item}\" for i, item in enumerate(profile_sample[\"Candidate_Items\"].values())]\n",
    "    )\n",
    "    \n",
    "    output_text = f\"User Profile:\\n{user_profile_text} \\nCandidate Items:\\n{candidate_items_text} \"\n",
    "    \n",
    "    # Complete prompt\n",
    "    full_text = f\"\\n{instruction}\\n\\n{input_text}\\n\\n\\n{output_alpaca}\\n{output_text}\"\n",
    "    return full_text\n",
    "\n",
    "print(\"Preprocessed Sample:\", preprocess_function(training_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Tokenize and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(sample):\n",
    "    processed_text = preprocess_function(sample)\n",
    "    tokenized = tokenizer(\n",
    "        processed_text,\n",
    "        truncation=True,\n",
    "        max_length=PIPELINE_PARAMS['max_length'],\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # Set labels identical to input_ids\n",
    "    return tokenized\n",
    "\n",
    "# Set up two different training sizes: 16 and 32 samples\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=QLORA_PARAMS['lora_r'],\n",
    "    lora_alpha=QLORA_PARAMS['lora_alpha'],\n",
    "    lora_dropout=QLORA_PARAMS['lora_dropout'],\n",
    "    target_modules=QLORA_PARAMS['lora_target_modules'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "batch_size = 16\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure LoRA and Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 16 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 10/16 [09:01<05:33, 55.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.9944, 'grad_norm': 13.984397888183594, 'learning_rate': 5.6250000000000005e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [14:34<00:00, 54.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 874.9106, 'train_samples_per_second': 0.037, 'train_steps_per_second': 0.018, 'train_loss': 4.519654154777527, 'epoch': 2.0}\n",
      "Model trained with 16 samples saved to outputs/best_model_up_ci_16_samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 32 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 10/32 [10:23<22:26, 61.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3691, 'grad_norm': 1.4578264951705933, 'learning_rate': 7.8125e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 20/32 [21:36<13:28, 67.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4208, 'grad_norm': 0.3514408469200134, 'learning_rate': 4.6875e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 30/32 [32:58<02:15, 67.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4351, 'grad_norm': 0.5033930540084839, 'learning_rate': 1.5625e-05, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [35:13<00:00, 66.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2113.4589, 'train_samples_per_second': 0.03, 'train_steps_per_second': 0.015, 'train_loss': 1.7429602965712547, 'epoch': 2.0}\n",
      "Model trained with 32 samples saved to outputs/best_model_up_ci_32_samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 64 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 10/64 [09:06<49:04, 54.54s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5959, 'grad_norm': 0.500237226486206, 'learning_rate': 8.4375e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 20/64 [18:18<41:07, 56.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4197, 'grad_norm': 0.6927663087844849, 'learning_rate': 6.875e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 30/64 [27:38<31:16, 55.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4272, 'grad_norm': 0.7542613744735718, 'learning_rate': 5.3125000000000004e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 40/64 [37:21<23:23, 58.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2994, 'grad_norm': 0.8727654218673706, 'learning_rate': 3.7500000000000003e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 50/64 [47:05<13:31, 57.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1895, 'grad_norm': 0.6733031868934631, 'learning_rate': 2.1875e-05, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 60/64 [57:23<04:04, 61.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2437, 'grad_norm': 0.7231120467185974, 'learning_rate': 6.25e-06, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [1:01:27<00:00, 57.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 3687.1073, 'train_samples_per_second': 0.035, 'train_steps_per_second': 0.017, 'train_loss': 1.3637834638357162, 'epoch': 2.0}\n",
      "Model trained with 64 samples saved to outputs/best_model_up_ci_64_samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up two different training sizes: 16 and 32 samples\n",
    "training_sizes = [16,32,64]\n",
    "\n",
    "for train_size in training_sizes:\n",
    "    # Split the dataset\n",
    "    train_data = training_data[:train_size]\n",
    "    eval_data = training_data[train_size:train_size + int(0.2 * train_size)]  # 20% of training data as eval\n",
    "\n",
    "    # Tokenize datasets\n",
    "    train_tokenized_data = [tokenize_function(sample) for sample in train_data]\n",
    "    eval_tokenized_data = [tokenize_function(sample) for sample in eval_data]\n",
    "\n",
    "    # Convert to Dataset format\n",
    "    train_dataset = Dataset.from_dict({\n",
    "        \"input_ids\": [x[\"input_ids\"][0] for x in train_tokenized_data],\n",
    "        \"attention_mask\": [x[\"attention_mask\"][0] for x in train_tokenized_data],\n",
    "        \"labels\": [x[\"labels\"][0] for x in train_tokenized_data]\n",
    "    })\n",
    "    eval_dataset = Dataset.from_dict({\n",
    "        \"input_ids\": [x[\"input_ids\"][0] for x in eval_tokenized_data],\n",
    "        \"attention_mask\": [x[\"attention_mask\"][0] for x in eval_tokenized_data],\n",
    "        \"labels\": [x[\"labels\"][0] for x in eval_tokenized_data]\n",
    "    })\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"outputs/best_model_up_ci_{train_size}_samples\",  # Distinct directory for each model\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=QLORA_PARAMS['gradient_accumulation_steps'],\n",
    "        num_train_epochs=QLORA_PARAMS['lora_num_epochs'],\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=QLORA_PARAMS['lora_val_iterations'],\n",
    "        save_steps=QLORA_PARAMS['lora_val_iterations'],\n",
    "        logging_steps=10,\n",
    "        learning_rate=QLORA_PARAMS['lora_lr'],\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=\"none\",\n",
    "        fp16=True  # Enable FP16 mixed precision\n",
    "    )\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, padding=True)\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    # Clear GPU cache before training\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Start training\n",
    "    print(f\"Starting training with {train_size} samples.\")\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model and tokenizer in separate directories for each training size\n",
    "    model.save_pretrained(f\"outputs/best_model_up_ci_{train_size}_samples\")\n",
    "    tokenizer.save_pretrained(f\"outputs/best_model_up_ci_{train_size}_samples\")\n",
    "    print(f\"Model trained with {train_size} samples saved to outputs/best_model_up_ci_{train_size}_samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
